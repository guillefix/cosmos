created: 20160406164853743
creator: guillefix
modified: 20180722000020083
modifier: cosmos
tags: [[Artificial intelligence innovation]]
title: New advances in deep learning
tmap.id: e02512fa-e9cf-4761-abd6-5be637f3ea17
type: text/vnd.tiddlywiki

review/position paper on giving neural nets the right biases -- https://arxiv.org/abs/1806.01261

2017 updates: http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/

posenet https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5 tensorflowjs

DeepMind publications: https://deepmind.com/research/publications/

Conferences: [[ICLR|http://iclr.cc/doku.php?id=iclr2017:schedule]], NIPS

Transformer networks. https://research.googleblog.com/2017/08/transformer-novel-neural-network.html -- New: Universal Transformers networks

[[Attention is all you need]]..

[[Generative adversarial network]]s, etc.

Generating video!

[[Variational autoencoder]]

WaveNet, PixelRNN, SampleRNN

!!![[Augmented RNN]]s ([[Memory]]-augmented)

!!!Neural architecture search

[[Neural Architecture Search with Reinforcement Learning|https://arxiv.org/abs/1611.01578]]

!!![[Meta-learning]]

Avoiding catastrophic forgetting..

Capsules. [[Dynamic Routing Between Capsules|https://nips.cc/Conferences/2017/Schedule?showEvent=9167]]

---------------

[[Multisensory integration]]

[[Deep learning theory]]

[[Deep learning with Elastic Averaging SGD|https://arxiv.org/abs/1412.6651]]

We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master).

---------------------

Solve vanishing gradients problem, allow be computations to depend on all/any of the existing computations, thus creating a DAG structure instead of a layered one: http://www.jonolick.com/home/dagnn-a-deeper-fully-connected-network

!!![[Batch normalization]]

[[ Deep Networks with Stochastic Depth|http://arxiv.org/abs/1603.09382v1]] [[Stochastic Depth Networks will Become the New Normal|http://deliprao.com/archives/134]]

[[ Highway Networks|http://arxiv.org/abs/1505.00387]]

!!![[Dropout]]

https://en.m.wikipedia.org/wiki/Modular_neural_network STN?

DCGAN http://arxiv.org/abs/1511.06434

DRAW http://arxiv.org/abs/1502.04623

Soft/hard attention https://www.google.es/url?sa=t&source=web&rct=j&url=http://arxiv.org/pdf/1502.03044&ved=0ahUKEwi4yof-jPjLAhVC5xoKHcTjDM4QFgggMAA&usg=AFQjCNEs1Yw8fZF9oaqo73cwbHJqKwQHTw

CharCNN https://www.google.es/url?sa=t&source=web&rct=j&url=http://arxiv.org/pdf/1508.06615&ved=0ahUKEwjZqaXnk_jLAhWCsxQKHZsuApUQFgglMAM&usg=AFQjCNHk8JQpI98eUtyiluv7d2G9aWRtyA

NeuralStyle https://github.com/jcjohnson/neural-style

"Take a look at @karpathy's Tweet: https://twitter.com/karpathy/status/709465955223543808?s=09"

http://arxiv.org/abs/1604.00790 bidirectional LSTM

http://www.computervisionblog.com/2016/06/deep-learning-trends-iclr-2016.html

Adversarial networks

Neural Turing machines, neural programmers-interpreters

http://www.kdnuggets.com/2016/09/9-key-deep-learning-papers-explained.html

---------------------

A couple of cool recent neural network developments!
* Lip reading
* Neural enhance! Super-resolution of images (like in movies but real ;) )

LipNet. Lip reading NN: http://prostheticknowledge.tumblr.com/post/152735696866/lipnet-deep-learning-research-from-the-university

image super-resolution using deep convolutional networks! try here: http://waifu2x.udp.jp/
NeuralEnhance lets you apply 4x super-resolution to your photos CSI-style in only 340 lines of code!
https://github.com/alexjc/neural-enhance

https://twitter.com/karpathy

https://twitter.com/alexjc

https://twitter.com/NandoDF
