created: 20160124143800337
creator: guillefix
modified: 20161022125350235
modifier: cosmos
tags: [[Neuronal network]] [[Artificial intelligence]]
title: Artificial neural network
type: text/vnd.tiddlywiki

<small>Aka artificial neural network..</small>

A particularly useful way of representing nonlinear functions, for problems in [[Machine learning]]. It is a very good model for many problems, and learning algorithms produce very good results with them. In particular __deep learning__ (which uses ANNs with many layers). It is a nonlinear [[classifier|Classifier]], and [[Regression analysis]] model.

[[Hugo Larochelle class videos|https://www.youtube.com/watch?v=apPiZd-qnZ8&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&index=4]] ([[website|http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html]]). [[Andrew Ng intro|https://www.youtube.com/watch?v=qyyJKd-zXRE&index=6&list=PLA89DCFA6ADACE599#t=26m]]. [[NN|https://www.youtube.com/watch?v=qyyJKd-zXRE&index=6&list=PLA89DCFA6ADACE599#t=29m]]. Learning parameters in a NN is generally a [[non-convex optimization problem|Convex optimization]], which makes it very hard to reach global optima.

!!__Definition__

Neuron has:

1) ''inputs''

2) ''weight vectors'', that multiplies the input vector or activation vector of hidden layers.

3) ''bias'', that is added to result

4) ''activation function'' takes as argument the result of the above (called pre-activation or input activation)

5) The result (called ''activation'') may be the input of other neurons in the next ''layer'', in a ''multilayer feedforward neural network''.

6) The activation of the last layer, is the ''output''

Overall... we are multiplying by matrices and applying simple nonlinear function

!!!See [[Neural network theory]], [[Mathematical modelling of neural networks]], for more on the theory

!!__Learning by [[optimization|Optimization]]__

Learning by minimizing cost function (see [[Learning theory]])

[[Training neural networks - optimization|https://www.youtube.com/watch?v=Bver7Ttgb9M&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&index=17]]. There are several global optima, and plateaus. Uses [[Gradient descent]], in particular [[SGD|Stochastic gradient descent]].

An efficient algorithm to compute the gradients of the loss function for (SGD) w.r.t. the ANN's parameters is [[Backpropagation]].

see more at [[Learning theory]]

!!!__Parameter initialization for NNs__

[[Neural networks [2.9] : Training neural networks - parameter initialization|https://www.youtube.com/watch?v=sLfogkzFNfc&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&index=15]]

!!!__[[Model selection]] for neural networks__

[[Neural networks [2.10] : Training neural networks - model selection|https://www.youtube.com/watch?v=Fs-raHUnF2M&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&index=16]]. How to set the hyperparameters. Can use [[Cross-validation]].

!!__Types of neural networks__

* Feedforward (basic)

* [[Convolutional neural network]]. Good for image recognition for e.g.

* [[Recurrent|Recurrent neural network]]. Good for sequences in time

* Long-Short term memory NN.

* [[Spiking neural network]]

* [[Residual neural network]]

* [[Boltzmann machine]]

* [[Autoencoder]]

* [[Deep belief network]]

* [[Generative adversarial network]]

* [[Hopfield network]]

Many models in [[Machine learning]] can be seen as neural networks

[img[http://www.asimovinstitute.org/wp-content/uploads/2016/09/neuralnetworks.png]]

--------

[[Early video that created about TTS|https://www.youtube.com/watch?v=qyyJKd-zXRE&list=PLA89DCFA6ADACE599&index=6#t=41m23s]] using ANNs (NetTalk), see [[Speech synthesis]]

[[A Neural Network in 11 Lines of Python|http://fossbytes.com/a-neural-network-in-11-lines-of-python/]]

__More models, and generalizations__

//Backpropagation//, temporal networks, etc..

[[Visualizing and Understanding Deep Neural Networks by Matt Zeiler|https://www.youtube.com/watch?v=ghEmQSxT6tw]]

[[Two Minute Papers - Estimating Matrix Rank With Neural Networks|https://www.youtube.com/watch?v=bLFISzfQCDQ]]

--------

Physical implementations:

[[Chemical implementations of neural networks and Turing machines|http://www.dna.caltech.edu/courses/cs191/paperscs191/PNAS(88)10983.pdf]]

http://knowmtech.com/

--------

//More//

Layerless neural networks? See Chico Calmagro's work with Ard Louis.


On the complex backpropagation algorithm

[[Neural networks for control systemsâ€”A survey|http://www.sciencedirect.com/science/article/pii/000510989290053I]]

[[Genetic deep neural networks using different activation functions for financial data mining|http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7364099&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7364099]]

[[Structure Discovery of Deep Neural Network Based on Evolutionary Algorithms|http://www.merl.com/publications/docs/TR2015-032.pdf]]

[[Genetic algorithms for evolving deep neural networks|http://dl.acm.org/citation.cfm?id=2602287]]

[[Busqueda de la estructura optima de redes neurales con Algoritmos Geneticos y Simulated Annealing. Verificacion con el benchmark PROBEN1|http://polar.lsi.uned.es/revista/index.php/ia/article/viewFile/532/516]]

[[Implementation of Evolutionary Algorithms for Deep Architectures|http://ceur-ws.org/Vol-1315/paper15.pdf]]

See ideas here: Idea for neural network for chemical synethesis and manufacturing etc. Facebook post: [ext[https://www.facebook.com/guillermovalleperez/posts/10153853693416223?]]

!!!__Statistical mechanics of neural networks__

[[Neural networks and physical systems with emergent collective computational abilities|http://www.pnas.org/content/79/8/2554.short]]

[[Spin-glass models of neural networks|http://journals.aps.org/pra/abstract/10.1103/PhysRevA.32.1007]]

[[Learning and pattern recognition in spin glass models|http://link.springer.com/article/10.1007/BF01304440]]

[[Neural nets : classical results and current problems|http://ir.library.oregonstate.edu/xmlui/handle/1957/28802]]