created: 20170521163814024
creator: cosmos
modified: 20170525213606185
modifier: cosmos
tags: [[Learning theory]] [[Information geometry]]
title: Singular learning theory
tmap.id: 8f4f7ba4-b9eb-4225-abe3-00556c8d9826
type: text/vnd.tiddlywiki

A statistical model or a learning machine is called regular if the map taking a parameter to a probability distribution is one-to-one and if its Fisher information matrix is always positive definite. If otherwise, it is called singular. In regular statistical models, the Bayes free energy, which is defined by the minus logarithm of Bayes marginal likelihood, can be asymptotically approximated by the Schwarz Bayes information criterion (BIC), whereas in singular models such approximation does not hold. Uses tools of [[Algebraic geometry]]

[[A Widely Applicable Bayesian Information Criterion|http://www.jmlr.org/papers/v14/watanabe13a.html]]

[[Algebraic Analysis for Nonidentifiable Learning Machines|http://www.mitpressjournals.org/doi/pdf/10.1162/089976601300014402]], [[Algebraic Analysis for Singular Statistical Estimation|https://link.springer.com/chapter/10.1007/3-540-46769-6_4]]

[[Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory|http://www.jmlr.org/papers/v11/watanabe10a.html]]

[[Singularities in mixture models and upper bounds of stochastic complexity|http://www.sciencedirect.com/science/article/pii/S0893608003000054?np=y&npKey=142c3bf066ad1c365c9aa78437002039085af147ca0a88722344eda3e73f0bc4]]

Algorithm for singular models: http://sci-hub.cc/10.1007/s11063-013-9283-z

[[Singularities Affect Dynamics of Learning in Neuromanifolds|http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.2006.18.5.1007]]

[[Information-based inference in sloppy and singular models|https://arxiv.org/abs/1506.05855]]

[[Likelihood ratio of unidentifiable models and multilayer neural networks|http://projecteuclid.org/euclid.aos/1056562464]]

------------------

[[Dynamics of Learning Near Singularities in Layered Networks|http://www.mitpressjournals.org/doi/abs/10.1162/neco.2007.12-06-414]]

[[A Regularity Condition of the Information Matrix of a Multilayer Perceptron Network|http://www.sciencedirect.com/science/article/pii/0893608095001190]]. The Fisher information matrix of a multi-layer perceptron network can be singular at certain parameters, and in such cases many statistical techniques based on asymptotic theory cannot be applied properly. In this paper, we prove rigorously that the Fisher information matrix of a three-layer perceptron network is positive definite if and only if the network is irreducible; that is, if there is no hidden unit that makes no contribution to the output and there is no pair of hidden units that could be collapsed to a single unit without altering the input-output map. This implies that a network that has a singular Fisher information matrix can be reduced to a network with a positive definite Fisher information matrix by eliminating redundant hidden units. 

[[Singularities Affect Dynamics of Learning in Neuromanifolds|http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.2006.18.5.1007]]

[[Application of the error function in analyzing the learning dynamics near singularities of the multilayer perceptrons|http://ieeexplore.ieee.org/abstract/document/6390480/]]

[[Resolution of Singularities Introduced by Hierarchical Structure in Deep Neural Networks|http://ieeexplore.ieee.org/abstract/document/7502092/]]

[[On the Singularity in Deep Neural Networks|https://link.springer.com/chapter/10.1007/978-3-319-46681-1_47]]

[[On the Geometry of Feedforward Neural Network Error Surfaces|http://www.mitpressjournals.org/doi/abs/10.1162/neco.1993.5.6.910]]