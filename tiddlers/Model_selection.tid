created: 20160914085745536
creator: cosmos
modified: 20170516120502731
modifier: cosmos
tags: [[Statistical inference]]
title: Model selection
tmap.id: 4c4ef4b0-6e57-4dad-b2a4-cb3ebf8a438a
type: text/vnd.tiddlywiki

This includes [[Model evaluation]], which is the way models are selected...

[[Introduction|https://www.youtube.com/watch?v=0kWZoyNRxTY&index=10&list=PLA89DCFA6ADACE599#t=36m40s]], see overfitting and underfitting below. Model selection algorithms provide methods to automatically choose optimal bias/variance tradeoffs. [[Explanation|https://www.youtube.com/watch?v=0kWZoyNRxTY&index=10&list=PLA89DCFA6ADACE599#t=40m10s]]

!!__[[Predictive posterior]]__

Predictive posterior checks. Likelihood of data, mostly on test data (see [[Cross-validation]]).

Check distribution of extreme values

!!__Information criteria__

* [[Akaike information criterion]] (AIC)

* [[Bayesian information criterion]] (BIC)

* [[Widely applicable information criterion]] (WAIC)

* [[Approximate leave-one-out cross-validation]] (LOO) using Pareto smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. ([[R package|https://cran.r-project.org/web/packages/loo/index.html]])

[ext[Paper|http://www.stat.columbia.edu/~gelman/research/unpublished/loo_stan.pdf]]

!!__[[Cross-validation]]__


!!__[[Feature selection]]__

!!__[[Structural risk minimization]]__

----------------------

A lot of these methods are very much related to [[Regularization]] methods, as both try to make our model better. Often we want the model to be better at generalizing, and this is done by reducing model complexity.

Using cross-validation for regularization can be done using [[Early stopping]] using the validation set


----------

__Model selection for [[Artificial neural network]]s__: [[Neural networks [2.10] : Training neural networks - model selection|https://www.youtube.com/watch?v=Fs-raHUnF2M&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&index=16]]

,,//Old comment//: One can show (maybe technical details I don't know..) that given the real distribution of the data, and a sample used for training, one is likely to underestimate the error. So I think cross-validation can be shown rigorously to be good for assessing a model's predictive power (i.e. probability of predicting rightly). See Elements of Statistical Learning book for all details..,,

It is a way to find out if you are overfitting

Related: https://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data