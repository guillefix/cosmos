created: 20180427161738016
creator: cosmos
draft.of: Exploding gradients problem
draft.title: Exploding gradients problem
modified: 20180528170844533
modifier: cosmos
tags: [[Deep learning theory]]
title: Draft of 'Exploding gradients problem'
tmap.id: c6075ff4-bc7f-4e2b-927a-5d8249895b46
type: text/vnd.tiddlywiki

[[video|https://www.youtube.com/watch?v=5v41xjKauY0]] -- [[The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs, and solutions|https://arxiv.org/abs/1712.05577]] ([[openreview|https://openreview.net/forum?id=HkpYwMZRb]])

# For some types of nonlinearities/layers, the gradients w.r.t. to parameters at different layers increase with the number of layers (it increases as we go from output to input). ([[See visualization of decision surfaces on parameter space for fixed input|https://youtu.be/5v41xjKauY0?t=21m51s]]

# To avoid pseudorandom walk due to the decision surface from lower layers, we have to choose a smaller learning rate, as the we add more layers (see [[here|https://youtu.be/5v41xjKauY0?t=28m8s]])

# For any network, we can do the [[residual trick|https://youtu.be/5v41xjKauY0?t=31m59s]] to convert to a residual network. [[The size of the residuals is then found to be small when the learning rate is small|https://youtu.be/5v41xjKauY0?t=34m18s]]

# By using the [[residual trick|https://youtu.be/5v41xjKauY0?t=31m59s]], and then doing [[a Taylor expansion|https://youtu.be/5v41xjKauY0?t=37m42s]] (where we assume residual terms are small), we can reduce the number of layers, while still approximating well, giving a notion of ''effective depth'' (note: depending on for how many layers we do the Taylor expansion, we get different number of layers in the approximate network). [[The effective depth decreases if the residuals are smaller|https://youtu.be/5v41xjKauY0?t=38m42s]].

# [[Therefore error is large|https://youtu.be/5v41xjKauY0?t=39m5s]]. Although for the nets with exploding gradients (and thus small residuals), the depth of the approximate nets is small, they have a lot of layers of nonliearities (with fixed parameters, that's why they are not counted for in the effective depth). These __needlessly randomize the features__, they argue

See [[summary|https://youtu.be/5v41xjKauY0?t=48m16s]]

[[gingkotree|https://gingkoapp.com/app#72f84d34b9dce3b61f00003d]]

__Other problems discussed in the video__

Apart from exploding gradients, networks can fail by 

* pseudo-linearity (which means that as the variance of activations decreases, for certain nonlinearities, the nonlinearity effectively looks more and more linear). This causes ineffective layers, and low effective depth (and then potentially high error). For some nonlinearities, the pseudolinearity effect only happens if domain bias also occurs. [[ReLU]] is one example.

They then present a theoretical model to argue that nonlinearity must lead to either gradient growth or domain bias. But they present waits to overcome these problems