created: 20160226112409331
creator: guillefix
modified: 20160726015535667
modifier: guillefix
tags: [[Artificial and machine intelligence]]
title: Machine learning

//aka statistical learning//, see [[Statistics]]

See [[Artificial and machine intelligence]] and [[Artificial intelligence]], [[Deep learning]]

--[[Book recommendations|https://www.quora.com/Machine-Learning/How-do-I-learn-machine-learning-1]]

[[Building Machine Learning Systems with Python|https://www.packtpub.com/big-data-and-business-intelligence/building-machine-learning-systems-python]]
-- [[Machine learning in Matlab|http://uk.mathworks.com/videos/machine-learning-with-matlab-87051.html?s_eid=PSM_12825]]
 --[[Lecture list of Andrew's course:|https://www.quora.com/What-are-the-differences-between-the-Andrew-Ngs-Machine-Learning-courses-offered-on-Coursera-and-iTunes-U]]
-- [[lecture notes|http://cs229.stanford.edu/materials.html]]
-- [[Andrew Ng machine learning course|https://www.coursera.org/learn/machine-learning]] https://www.youtube.com/watch?v=UzxYlbK2c7E . On [[lecture 2|https://www.youtube.com/watch?v=5u4G23_OohI]]
 -- [[Machine Learning - mathematicalmonk|https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA]]
 -- [[Machine Learning: A Probabilistic Perspective|http://www.amazon.co.uk/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020]] [[and here|https://www.cs.ubc.ca/~murphyk/MLbook/]]
 -- [[Machine Learning: Discriminative and Generative (The Springer International Series in Engineering and Computer Science)|http://www.amazon.co.uk/Machine-Learning-Discriminative-International-Engineering/dp/1402076479]] https://en.wikipedia.org/wiki/Generative_model

!!__[[Supervised learning]]__

Training data consisting on inputs and outputs. Want to find function relating inputs to outputs, to then be able to predict new outputs from new inputs. Need ''a way to represent the function approximation'', with some parameters (the ''model''): 

* Linear functions
* Kernel (basis) functions, polynomials, Gaussians, etc.
* [[Artificial neural network]]s
* ...

and a ''learning algorithm'' to find best parameters for the data. 

Two main types:

* __Regression__. Output value is continuous

* __Classification__. Output value is discrete

New paradigm: [[Deep learning]]

!!__[[Unsupervised learning]]__

[[Intro by Andrew Ng|https://www.youtube.com/watch?v=UzxYlbK2c7E#t=50m40s]]

[[Self-organizing map|https://en.wikipedia.org/wiki/Self-organizing_map]]

Cocktail party problem. Independent component analysis

K-means

//Clustering//

Community clustering in networks

!!__Variations on supervised and unsupervised__

[[Variations on supervised and unsupervised|https://www.youtube.com/watch?v=pytUuJPOnVI&list=PLD0F06AA0D2E8FFBA&index=4]]

!!!__[[Semi-supervised learning|https://www.youtube.com/watch?v=pytUuJPOnVI&list=PLD0F06AA0D2E8FFBA&index=4#t=27s]]__

You are given a set of inputs $$x$$, but you only have the corresponding outputs $$y$$ for some. You have to predict the $$y$$ for the rest (by learning the function $$y(x)$$ for instance, like in [[Supervised learning]].

!!!__[[Active learning|https://www.youtube.com/watch?v=pytUuJPOnVI&list=PLD0F06AA0D2E8FFBA&index=4#t=4m07s]]__

Like semi-supervised learning but the algorithm can //ask// for extra data, which it deems to be the most useful data to ask for.

!!!__Decision-theoretic learning__

Basically loss-functions/costs used by the learning agent are based on [[Decision theory]]. See example [[here|https://www.youtube.com/watch?v=pytUuJPOnVI&list=PLD0F06AA0D2E8FFBA&index=4#t=5m32s]].

!!__[[Reinforcement learning]]__

To me it seems like the difference with supervised learning, is that you //don't specify input, output pairs, but just outputs//. You specify desired outputs, and undesired outputs. There is no input, but still the problem is not just trivial (i.e. it only ever produces one output), because the model is probabilistic.

Sequence of decisions

Reward function

Used often in robotics.

!!__[[Learning theory]] and [[Learning algorithm]]s__



!!__[[Deep learning]]__

<small>Go deep into the rabbit hole</small>

!!__[[Bayesian inferential statistics|Bayesian inference]]__

__[[Graphical models|https://en.wikipedia.org/wiki/Graphical_model]]__

Good framework: [[Stan|http://mc-stan.org/]]

------

[img[https://lh3.googleusercontent.com/1vNipHvUofw7v2XurbRcoKWdrha4XwW2Wg6iv_CjnPJ7yaJeLuOGPHEXP5r0bHiXDa5jXmi3gXWzRs-rnOmoWT2qdrpDlhqoPaINOW1e8wCnkcMmsfjL5I7MAnuysZNkA0ZS-AduSU6My_vj8QjrLwgU7PtqeOxEmOHYOzJMm1COtI55peywxXwYc4Ot0XMg3WSk4ctE620Fg-kQuA8Zw86ejVU0wPx4C6f-yYJYDol4KmH_zV43EJREoK0ZJaU0v4j54Luq0_enrS9FA4oPcWX5v4h6hTCXJq3aubFRI-HBAP0Az3Js3cA9ZxPQV0U-1MZBCEdfI-0b87bEVSEAvZ7vsWTfyadsG43bfwc8ZGr4XRhXWYVlGj48WxrpQyTPFhPQMXNoiRURzx5bm4ZHukhomdEE98JJ4c5XqhybUHdIk6qJbUS7BXjcYaBlm3z8bGiBlPtDSdt61a59mbotPi7DS3N-LdHrHUd3PXtG59t_5fHfKi3WpqNS_dJOefgRukPJ0OAK4fE579XHNw_8l0Fi2mAqsP7Y8WNm1lg8yXQI2c6hrlGzWt2jO_4it_Zef_2r=w1269-h675-no]]

[[Deep Learning Lecture 5: Regularization, model complexity and data complexity (part 2)|https://www.youtube.com/watch?v=qz9bKfOqd0Y&index=5&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=25m43s]]

So the simplest model that works seems to work best most of the time. Seems like an example of Occam's razor, and thus related to Solomonoff's ideas on inference (see [[Algorithmic information theory]]). Epicurus principle also related to Bayesian inference, because we give a distribution over models, but we keep all of them.

Hmm, also your error can't be smaller than the fundamental noise in the data. Well it can, but your model will at best be wasteful then.

--------

<mark>Try [[Torch|Torch (Deep learning framework)]]:</mark>

See https://www.youtube.com/watch?v=DHspIG64CVM#t=45m40s