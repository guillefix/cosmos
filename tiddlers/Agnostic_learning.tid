created: 20170306114139469
creator: cosmos
modified: 20170426225928502
modifier: cosmos
tags: [[Computational learning theory]]
title: Agnostic learning
tmap.id: d10f44c4-8d64-4b95-abe9-c938ed7174fb
type: text/vnd.tiddlywiki

So far in all the learning frameworks we’ve studied, we’ve made an assumption that there is some “ground truth” target function that we attempt to learn. Our goal has been to identify a hypothesis that is close to this target, with respect to the target distribution. Learning algorithms are given access to the target function in the form of labelled observations, which in some cases may be noisy. In this lecture, we’ll drop the assumption of a ground-truth target completely; it is for this reason that the framework is called agnostic learning. As there is no longer a well-defined notion of target, ''our goal will be to identify a hypothesis that is competitive with respect to the best concept from a particular class''

!!!-->[[notes|http://www.cs.ox.ac.uk/people/varun.kanade/teaching/AML-HT2017/lectures/lecture09.pdf]]

[img[agnostic_learning_definition.png]]

[[video|https://www.youtube.com/watch?v=aILazXK059Y&t=9m40s]] -- [[weakness of the PAC definition which motivates agnostic learning|https://www.youtube.com/watch?v=PflkE9JmNLc&t=55m55s]] -- [[to approach this we redefine succesful learning to have only a relative error guarantee|https://www.youtube.com/watch?v=PflkE9JmNLc&t=1h8m50s]] --> [[Definition of Agnostic PAC learnability|https://www.youtube.com/watch?v=PflkE9JmNLc&t=1h10m22s]]

[[explaining agnostic learning|https://www.youtube.com/watch?v=iknI2iga9ps]] -- [[repeating the definition|https://www.youtube.com/watch?v=iknI2iga9ps#t=17m]]

!!!__Proving agnostic learnability of finite classes with [[ERM|Empirical risk minimization]]__

[[video|https://www.youtube.com/watch?v=Lyz4ewLefpE]]

[[Epsilon-representative classes|https://www.youtube.com/watch?v=Lyz4ewLefpE#t=16m30s]]

[[Defining sample complexity of uniform convergence|https://www.youtube.com/watch?v=Lyz4ewLefpE#t=26m15s]]

-- [[if we have uniform convergence, we can prove agnostic learninability|https://www.youtube.com/watch?v=Lyz4ewLefpE&t=32m]]

[[Step 1: bounding non-representativity of individual hypothesis|https://www.youtube.com/watch?v=Lyz4ewLefpE&t=40m]] -- [[doing it|https://www.youtube.com/watch?v=Lyz4ewLefpE&t=49m24s]], using [[Heoffding's inequality]] (a [[Concentration inequality]]) -- [[result|https://www.youtube.com/watch?v=Lyz4ewLefpE&t=1h45s]]

[[Step 2: union bound|https://www.youtube.com/watch?v=Lyz4ewLefpE&t=42m]] -- [[doing it|https://www.youtube.com/watch?v=Lyz4ewLefpE&t=1h1m40s]] -- [[RESULT|https://www.youtube.com/watch?v=Lyz4ewLefpE&t=1h6m15s]]

[[result for sample complexity of uniform convergence|https://www.youtube.com/watch?v=Lyz4ewLefpE&t=1h11m40s]]

We want to bound the sample complexity of uniform convergence

See more at [[Computational learning theory]]