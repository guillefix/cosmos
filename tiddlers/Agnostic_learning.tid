created: 20170306114139469
creator: cosmos
modified: 20190516223735860
modifier: cosmos
tags: [[Computational learning theory]]
title: Agnostic learning
tmap.id: d10f44c4-8d64-4b95-abe9-c938ed7174fb
type: text/vnd.tiddlywiki

So far in all the learning frameworks we’ve studied, we’ve made an assumption that there is some “ground truth” target function that we attempt to learn. Our goal has been to identify a hypothesis that is close to this target, with respect to the target distribution. Learning algorithms are given access to the target function in the form of labelled observations, which in some cases may be noisy. In this lecture, we’ll drop the assumption of a ground-truth target completely; it is for this reason that the framework is called agnostic learning. As there is no longer a well-defined notion of target, ''our goal will be to identify a hypothesis that is competitive with respect to the best concept from a particular class''

See Wolpert's "The relationship between PAC, the Statistical Physics Framework, and Bayesian Framework, and the VC Framework" (by VC framework, they essentially refer to what is now called agnostic PAC learning). He points out something, I also realized after thinking about it: "In general, VC results say it is unlikely for empirical error and true error to differ greatly. By themsleves, without information about P(true error), they do not say that it is unlikely that true error is large in those cases where empirical error is small".

!!!-->[[notes|http://www.cs.ox.ac.uk/people/varun.kanade/teaching/AML-HT2017/lectures/lecture09.pdf]]

[img[agnostic_learning_definition.png]]

[[video|https://www.youtube.com/watch?v=aILazXK059Y&t=9m40s]] -- [[weakness of the PAC definition which motivates agnostic learning|https://www.youtube.com/watch?v=PflkE9JmNLc&t=55m55s]] -- [[to approach this we redefine succesful learning to have only a relative error guarantee|https://www.youtube.com/watch?v=PflkE9JmNLc&t=1h8m50s]] --> [[Definition of Agnostic PAC learnability|https://www.youtube.com/watch?v=PflkE9JmNLc&t=1h10m22s]]

[[explaining agnostic learning|https://www.youtube.com/watch?v=iknI2iga9ps]] -- [[repeating the definition|https://www.youtube.com/watch?v=iknI2iga9ps#t=17m]]

!!!__Proving agnostic learnability of finite classes with [[ERM|Empirical risk minimization]]__

[[video|https://www.youtube.com/watch?v=Lyz4ewLefpE]]

[[Epsilon-representative classes|https://www.youtube.com/watch?v=Lyz4ewLefpE#t=16m30s]]

[[Defining sample complexity of uniform convergence|https://www.youtube.com/watch?v=Lyz4ewLefpE#t=26m15s]]

-- [[if we have uniform convergence, we can prove agnostic learninability|https://www.youtube.com/watch?v=Lyz4ewLefpE&t=32m]]

[[Step 1: bounding non-representativity of individual hypothesis|https://www.youtube.com/watch?v=Lyz4ewLefpE&t=40m]] -- [[doing it|https://www.youtube.com/watch?v=Lyz4ewLefpE&t=49m24s]], using [[Heoffding's inequality]] (a [[Concentration inequality]]) -- [[result|https://www.youtube.com/watch?v=Lyz4ewLefpE&t=1h45s]]

[[Step 2: union bound|https://www.youtube.com/watch?v=Lyz4ewLefpE&t=42m]] -- [[doing it|https://www.youtube.com/watch?v=Lyz4ewLefpE&t=1h1m40s]] -- [[RESULT|https://www.youtube.com/watch?v=Lyz4ewLefpE&t=1h6m15s]]

[[result for sample complexity of uniform convergence|https://www.youtube.com/watch?v=Lyz4ewLefpE&t=1h11m40s]]

We want to bound the sample complexity of uniform convergence

See more at [[Computational learning theory]]

!!__Tighter agnostic bounds__

One can obtain bounds which are tight both when the training error is high, or when it is low, by using tighter versions of the Chernoff bound on the Bernoulli probability. One can numerically compute optimal bounds in some cases. 

See [[here|http://www.jmlr.org/papers/volume6/langford05a/langford05a.pdf]].

If one wants more analytic expressions, the [[Relative entropy Chernoff bound]] is pretty tight for all the range of training errors.

!!__More general training error-conditional bounds__

Using the [[Weighted union bound]], as in [[Structural risk minimization]], one can give a non-uniform bound over training errors, which can allow to get better bounds if one has a prior belief over which training errors are more likely. <mark>Hmm, interesting. So does this mean that the relative entropy chernoff bound encodes a prior that favors smaler training error?</mark>

[img[SRM_bound_with_prior_over_training_errors.png]] See [[here|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=705570&tag=1#page=3]] for proof. See [[Growth function generalization error bound]] for result that is needed to prove this.

-------------

''Relevance of purely agnostic learning?''

Given that we really care in practice, mostly, about when we will have small training error, so we are close to [[PAC learning|Probably approximately correct]].

It can be useful as a way to prove general bounds (that are still applicable in PAC learning), and also to guarantee that if we have small training error, our hypothesis class is probably not appropriate. One can however, obtain better bounds that interpolate between the standard Chernoff agnostic bounds, and the realizable ones. See seciton above.
