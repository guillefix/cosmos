created: 20160702131047142
creator: guillefix
modified: 20171010155302346
modifier: cosmos
tags: [[Random variable]]
title: Independence of random variables
tmap.id: 791f44bb-fa8b-4580-9835-8e9cdf1dfc3a
type: text/vnd.tiddlywiki

Given a [[Probability space]], two events $$A,B \in \mathbb{F}$$ are called ''independent'' if $$P(A\cap b) = P(A)P(B)$$

For random variables, they are called independent if for all $$A \subset \mathcal{X}$$, $$B \subset \mathcal{Y}$$, $$\{X \in A\}$$ and $$\{Y \in B\}$$ are independent in the above sense.

Equivalently, $$E[f(X),g(Y)] = E[f(X)]E[g(Y)]$$ for all functions $$f,g$$

See here: [[Chapter 2 Information Measures - Section 2.1 A Independence and Markov Chains|https://www.youtube.com/watch?v=nuJ3jTL5qqM&list=PLJfu_xpF92pvTfcJAILr5Kg1ptMvHUnft&index=4]] -- [[independence in graphical models|https://www.coursera.org/learn/probabilistic-graphical-models/lecture/PTXfn/conditional-independence]]

!!!__Independence of two random variables__

!!!__Mutual independence__

!!!__Pairwise independence__

!!!__Conditional independence__

https://en.wikipedia.org/wiki/Conditional_independence

See [[here|https://www.youtube.com/watch?v=nuJ3jTL5qqM&list=PLJfu_xpF92pvTfcJAILr5Kg1ptMvHUnft&index=4#t=3m52s]]. Note that his definition is the same as in wiki. Just divide by $$p(y)$$ to see this. His example at the end is rather illustrative too.