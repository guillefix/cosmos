created: 20160709004653144
creator: guillefix
modified: 20181004163443472
modifier: cosmos
tags: [[Statistical inference]] Learning [[Artificial intelligence]] [[Machine learning]]
title: Learning theory
tmap.id: fde229eb-00fc-4002-98bb-ad7a1dcce373
type: text/vnd.tiddlywiki

//the general field of computational learning theory, statistical learning theory, etc; note that these have slightly different connotations/focus//

Mathematical theory of learning, and learning algorithms. See [[Machine learning]], [[Deep learning theory]], [[wiki|https://www.wikiwand.com/en/Computational_learning_theory]].

See [[these notes|https://github.com/damaru2/optimization17/]]. See book and lectures "Understanding machine learning" by Shia (found [ext[here|http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/]] for free)


[[Statistical learning theory]] -- [[Computational learning theory]]

''Learning problem'': Design a system that improves on its ability to perform task T, as measured by performance measure P, by going through experience E.

!!__Types of learning problem__

Some important types:

* [[Supervised learning]]
* [[Unsupervised learning]]
* [[Reinforcement learning]]
* [[Statistical inference]]
** [[Causal inference]]
* [[Inverse problem]]
** [[Linear inverse problem]]

See more at [[Machine learning]], and [[Computational learning theory]]

----------------

<b>Learning theory focuses mostly on [[Supervised learning]], which is one of the most common learning problems in practice.</b>, and provides a general foundation for many other learning problems. 

[[Good course on statistical learning theory|https://www.youtube.com/watch?v=Q5itLKscYTA]]! ([[course webpage|http://www.mit.edu/~9.520/fall17/]])

!__Supervised learning theory__

See [[Supervised learning]] for more details on practical algorithms and applications.

!!__Problem definition: risk minimization__

[[Definition and introduction of the problem of learning a function given a loss|https://www.youtube.com/watch?v=SFxypsvhhMQ#t=7m15s]]

If one defines a [[Loss function]] (or ''risk'', or ''cost'', see [[Decision theory]]), $$R_f (\hat{y}, y)$$, one can ask what is the prediction scheme (for instance, the function $$\hat{y} = f(x)$$ that minimizes the expected risk. This is called risk minimization.

Given the previous abstract definition, the solution to expected risk minimization is well defined given the distribution of data. However, what is a good solution when only given training data (real learning problem)? [[The answer to this is what defines a good learning algorithm|https://www.youtube.com/watch?v=9-oxo_k69qs#t=8m]]

Some answers: 
[[Consistency of a learning algorithm|https://www.youtube.com/watch?v=9-oxo_k69qs#t=16m50s]] and other notions of goodness (one common one is also [[Probably approximately correct]] learning. See also Walport's chapter on boon "Mathematics of generalization") . A learning algorithm that has low expected risk with high probability (or related notions) is said to predict well, and if we do this even when we were given a training set much smaller than the test set, we say the algorithm is able to generalize.

!!__[[Generalization]]/prediction__

[[Making good generalizations is the goal of predictive/supervised learning|https://www.youtube.com/watch?v=tojaGtMPo5U&list=PLA89DCFA6ADACE599&index=9#t=17m45s]], where ''generalization'' means 
making predictions about unseen data from seen data, and good generalization means these predictions agree with the actual results, as often as possible. See also [[Induction]].

For the case of 0-1 loss functions, the expected risk is also known as ''generalization error'', $$\epsilon$$, [[defined as|https://www.youtube.com/watch?v=tojaGtMPo5U&list=PLA89DCFA6ADACE599&index=9#t=18m10s]] the probability that if I sample a new sample $$(x,y)$$ from the same distribution producing the data, my hypothesis misslabels that example.

[[No free lunch theorem]] -- [[Wolpert article on No Free Lunch and the different learning theory frameworks|http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=1A58309E5BB673000B0A0AA9E69BD559?doi=10.1.1.49.1549&rep=rep1&type=pdf]]

!!__[[Empirical risk minimization]]__

Risk minimization, requires knowing the joint probability distribution $$P(x,y)$$, so one often uses the sample mean of the risk as an estimator for the expected value of the risk. Minimizing this empirical quantity is called empirical risk minimization.

The empirical risk, for a 0-1 loss function is also known as ''training error'' $$\hat{\epsilon}$$, which is just the fraction of training points that your hypothesis missclassifies. See [[Training error (vid)|https://www.youtube.com/watch?v=tojaGtMPo5U&list=PLA89DCFA6ADACE599&index=9#t=11m]].

Sometimes, the difference between the empirical risk and the true risk is called the ''generalization gap''. Many learning algorithms are about doing approximate ERM while ensuring low generalization gap (and thus ensuring subsequent generalization/good prediction). This is approached with [[Regularization]]. This is a tradeoff whose extremes are called overfitting and underfitting.

!!!__[[Overfitting and underfitting]]__

Overfitting and underfitting refer to ways of misstraining a model, i.e., making it have poor generalization error, compared to the optimal model. [[Bias-variance tradeoff|https://www.youtube.com/watch?v=tojaGtMPo5U&list=PLA89DCFA6ADACE599&index=9#t=4m1.5s]]

!!__[[Regularization]]__

A way to avoid overfitting, while keeping all parameters in your model.
[[Intro vid|https://www.youtube.com/watch?v=sQ8T9b-uGVE&list=PLA89DCFA6ADACE599&index=11#t=1m]]

We use the [[Prior distribution]] from [[Bayesian statistics]], to make [[simple|Simplicity]] hypothesis more likely. See [[Simplicity and learning]].

[[Intuition|https://www.youtube.com/watch?v=sQ8T9b-uGVE&list=PLA89DCFA6ADACE599&index=11#t=12m]]

!!__[[Model selection/assessment|Model selection]]__

Model selection algorithms provide methods to automatically choose optimal bias/variance tradeoffs. 

* [[Cross-validation]]

* [[Feature selection]]

!!__Learning theory__

Mostly studies ways of ensuring good generalization for learning algorithms. See [[Probably approximately correct]]

See more at [[Computational learning theory]]!

!!!__[[Convergence and performance results in learning theory]]__

Mostly about general bounds, that often are pessimistic because they have to include worst cases, but give good intuition of general relations, for instance, between training set size and model complexity.

-->[[Theorem|https://www.youtube.com/watch?v=0kWZoyNRxTY&index=10&list=PLA89DCFA6ADACE599#t=17m15s]]. Let a hypothesis class $$H$$ be given, and let the [[VC dimension]] VC(H) = d. Then w.p. at leat$$1-\delta$$, we have that for all $$h \in H$$

$$|\epsilon(h)-\hat{\epsilon}(h)|\leq O\left(\sqrt{\frac{d}{m}\log{\frac{m}{d}}+\frac{1}{m}\log{\frac{1}{d}}}\right)$$

where $$m$$ is the size of the training set. A corollary is that for ERM, the number of training samples needed for a particular performance is roughly linear on the [[VC dimension]] of the hypothesis class (see [[video|https://www.youtube.com/watch?v=0kWZoyNRxTY&index=10&list=PLA89DCFA6ADACE599#t=22m]]). ''Sample complexity is upper bounded by VC dimension''.

!!__Design factors in machine learning__

[img[machine_learning_design_factors.png]]

!!__[[Deep learning theory]]__

[[Neural network theory]], [[Generalization in deep learning]]

--------------------

!__[[Bayesian statistics]]__

Applicable to problems in supervised learning, but also beyond

!!!__[[Maximum likelihood]]__

Minimize the negative log [[likelihood|Likelihood function]] (similar to entropy. More precisely, cross-entropy, or relative entropy), which corresponds to ''maximizing likelihood''.

!!!__[[Maximum a posteriori]] (MAP)__

!!!__[[Posterior mean]]__

!__[[Reinforcement learning]]__

--------------------

!__[[Optimization for learning]]__

!__[[Statistical physics and inference]]__

-------------------

[[Learning theory and neural networks gingko tree|https://gingkoapp.com/vehvff]]

------------------

!!!__General/worst-case analysis vs specific case analysis of learning__

[img[specific_case_learning_theory.jpg]]

quite relevant for [[Deep learning theory]]
