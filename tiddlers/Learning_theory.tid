created: 20160709004653144
creator: guillefix
modified: 20160808160902036
modifier: guillefix
title: Learning theory
type: text/vnd.tiddlywiki

Mathematical theory of learning.

See [[Machine learning]], [[Deep learning]]. 

''Learning problem'': Design a system that improves on its ability to perform task T, as measured by performance measure P, by going through experience E.

__Types of learning problem__

Some important types:

* [[Supervised learning]]
* [[Unsupervised learning]]
* [[Reinforcement learning]]
* [[Statistical inference]]
** [[Causal inference]]
* [[Inverse problem]]
** [[Linear inverse problem]]

__Empirical risk minimization__

''Minimize a cost function'', which often is the negative log likelihood (similar to entropy. More precisely, cross-entropy, or relative entropy), which corresponds to ''maximizing likelihood''. Likelihood is the probability of getting the right $$y$$ given $$x$$ and $$\theta$$, i.e. the probability that a given model predicts the right outputs. This is equivalent to finding the most likely $$\theta$$ in the Bayesian posterior, given a flat prior (but if we add a ''regularizer'', we can tweak the prior, by just adding a term to the log likelihood). If our model uses a Gaussian distribution to predict the data (where the $$\theta$$s are the means), maximizing likelihood is equivalent to minimizing spring energy for springs vertically placed between fit curve and data.

The maximum likelihood is found by [[Optimization]], often by [[Stochastic gradient descent]].

If we want the whole distribution of likelihoods over $$\theta$$s, we need to use Bayesian statistics, which involves doing complicated integrals, often done numerically using [[Montecarlo methods]]

--------------------

__Simplicity and learning__

See [[Order]] and [[Simplicity]]. The simplicity and structure in signals in the real-world is often seized to make the learning problem easier to solve.

Applications in [[Inverse problem]]s. For instance, see [[Convex optimization heuristics for linear inverse problems]] and [[Linear inverse problem]]

Applications in [[Compressed sensing]]

-------------------

[[Adaptive resonance theory|https://en.wikipedia.org/wiki/Adaptive_resonance_theory]]
The primary intuition behind the ART model is that object identification and recognition generally occur as a result of the interaction of 'top-down' observer expectations with 'bottom-up' sensory information. The model postulates that <b>'top-down' expectations take the form of a memory template or prototype that is then compared with the actual features of an object as detected by the senses</b>. This comparison gives rise to a measure of category belongingness. As long as this difference between sensation and expectation does not exceed a set threshold called the 'vigilance parameter', the sensed object will be considered a member of the expected class. The system thus offers a solution to the 'plasticity/stability' problem, i.e. the problem of acquiring new knowledge without disrupting existing knowledge.