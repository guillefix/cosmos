created: 20160701024258791
creator: guillefix
modified: 20170416094159771
modifier: cosmos
title: Entropy rate
tmap.id: dab00cd9-3d81-440c-ae01-1a80d8295331
type: text/vnd.tiddlywiki

The entropy rate of an information source (see [[Data transmission]]) is the average entropy of a letter of the source.

An information source is often modelled as a discrete-time stochastic process $$\{X_k\}$$, where each $$X_k$$ is called a "letter". The entropy rate is then defined as:

$$H_X = \lim_{n\rightarrow \infty} \frac{1}{n} H(X_1, X_2, \cdots, X_n)$$

when the limit exits (see also Shannon-McMillan-Breiman theorem).

[[Chapter 2 Information Measures - Section 2.10 Entropy Rate of a Stationary Source|https://www.youtube.com/watch?v=1-gH2ecuewk]]

One can define a related measure, $$H_X$$, by using conditional entropies. It can be shown that, for an stationary [[Information source]], the entropy rate exists and is equal to $$H_X$$.

[img[http://i.imgur.com/EdV8plQ.png]]

https://www.wikiwand.com/en/Entropy_rate

[[Entropy rate of a finite state process]]

!!!__Relations with [[Kolmogorov complexity]]__

[[A note on Kolmogorov complexity and entropy|http://www.sciencedirect.com/science/article/pii/S0893965903901054]], for stationary ergodic sources. See also Elements of Information theory by Thomas and Clover