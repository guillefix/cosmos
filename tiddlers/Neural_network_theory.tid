created: 20160913110136500
creator: cosmos
modified: 20171128215515260
modifier: cosmos
tags: [[Artificial neural network]]
title: Neural network theory
tmap.id: 71a60d67-6e95-45b8-84b3-6da49b6235f0
type: text/vnd.tiddlywiki

The theory (mainly [[Learning theory]]) of [[Artificial neural network]]s. See also [[Deep learning theory]], [[Mathematical modelling of neural networks]], [[Statistical mechanics of neural networks]]

[[Neural networks class - Universit√© de Sherbrooke (Hugo Larochelle)|https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH]]

<small>Note that the notation in [[this paper|http://arxiv.org/abs/1608.08225]] is opposite to that standard in [[Machine learning]]. __In the paper, $$y$$ is the input, and $$x$$ is the output.__
</small>

!!__[[Expressibility of neural networks]]__

!!__Learning of neural networks__

See [[Learning theory]], [[Artificial neural network]]s, [[Deep learning theory]], [[Non-convex optimization]]...

!!![[Learning theory and neural networks gingko tree|https://gingkoapp.com/vehvff]]

See talk at ICLR2017 by [[Riccardo Zecchina|http://staff.polito.it/riccardo.zecchina/]]

[[Domains of Solutions and Replica Symmetry Breaking in Multilayer Neural Networks|http://iopscience.iop.org/article/10.1209/0295-5075/27/2/002/meta]]

[[Weight Space Structure and Internal Representations: A Direct Approach to Learning and Generalization in Multilayer Neural Networks|https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.75.2432]]

[[Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses|https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.115.128101]]

Computational efficiency and optimization: [[Unreasonable effectiveness of learning neural nets: Accessible states and robust ensembles|https://pdfs.semanticscholar.org/a13e/ab6052cc9f85054d70d3ba395b0d77652172.pdf]]

!!__Generalization__

[[Theory of Deep Learning III: Generalization Properties of SGD|https://dspace.mit.edu/handle/1721.1/107841]]

See [[Generalization]], [[Generalization in deep learning]], [[Statistical physics and inference]]

<small>[[Statistical mechanics of learning|https://www.youtube.com/watch?v=hfBHELbk2Yw]] </small>

-----------------

[[Uniqueness of the weights for minimal feedforward nets with a given input-output map|http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.29.6931&rep=rep1&type=pdf]],. [[more here|https://www.researchgate.net/profile/Vera_Kurkova/publication/240374504_UNIQUENESS_OF_NETWORK_PARAMETERIZATIONS_AND_FASTER_LEARNING/links/547024480cf216f8cfa9e99e.pdf]]

[[For neural networks, function determines form|http://ieeexplore.ieee.org/abstract/document/371799/]], for 0-hidden layer, neural nets...

[[Functionally Equivalent Feedforward Neural Networks|http://www.mitpressjournals.org/doi/abs/10.1162/neco.1994.6.3.543?journalCode=neco]]

See more at [[Singular learning theory]]

----------------------

Neural network dynamics, see [[Dynamical system]]s

[[Dynamical Systems Theory for Transparent Symbolic Computation in Neuronal Networks|https://pearl.plymouth.ac.uk/handle/10026.1/8647]] We show that a correspondence can be found between these networks and [[Finite-state transducer]]s, and use the derived abstraction to investigate how noise affects computation in this class of systems, unveiling a surprising facilitatory effect on information transmission.

http://ieeexplore.ieee.org.sci-hub.cc/document/58339/
