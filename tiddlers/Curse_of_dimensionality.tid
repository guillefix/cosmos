created: 20161015101744053
creator: cosmos
modified: 20170516185104731
modifier: cosmos
title: Curse of dimensionality
tmap.id: 003c34ef-050a-429c-b8bf-36b92f6732ce
type: text/vnd.tiddlywiki

[[video|https://www.youtube.com/watch?v=LKdFTsM3hl4&list=PLA89DCFA6ADACE599&index=17#t=27m06s]] -- discretization scales poorly to high dimensional state spaces.

See Elements of Statistical learning


As can be seen by applying [[Nearest-neighbour classification]], the size of the neighbourhood to consider a certain fraction of the total populations for the choice, grows with dimension (linear size needs to be larger to have same fraction of total volume). The problem then is that we are making the choice based on points which are quite far in terms of linear distance, and thus need not be good predictors any more. This is the ''curse of dimensionality''.

See page 18 in Murphy's book

The main way to combat the curse of dimensionality is to make some assumptions about
the nature of the data distribution (either p(y|x) for a supervised problem or p(x) for an
unsupervised problem). These assumptions, known as inductive bias, are often embodied in
the form of a parametric model, which is a statistical model with a fixed number of parameters.

[ext[http://www.dbs.ifi.lmu.de/~zimek/publications/SSDBM2010/SNN-SSDBM2010-preprint.pdf]]. See page 5 for some interesting comments (c.f. [[Sloppy systems]])

In many cases (for many distance measures) distances between pairs of points (distributed in some way) tend to their mean (i.e. their variance decreases) as we increase dimension. These well-known studies generally
assumed that the full data set followed a single data distribution, subject to
certain restrictions. In fact, when the data follows a mixture of distributions,
the concentration effect is not always observed; in such cases, distances between
members of different distributions may not necessarily tend to the global mean
as the dimensionality increases.