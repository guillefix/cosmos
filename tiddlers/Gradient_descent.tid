created: 20160810174239855
creator: cosmos
modified: 20170524165413407
modifier: cosmos
tags: [[Local optimization]]
title: Gradient descent
tmap.id: b5d1a094-7a95-4550-b130-55a793cacbb3
type: text/vnd.tiddlywiki

A [[Local optimization]] technique based on following gradients to decrease a function.

Take steps of a size given by the learning rate along the [[Gradient]].

Can prove convergence for [[Convex function]]s

!!![[Newton's method]]

* [[Quasi-Newton method]]

!!![[Stochastic gradient descent]]

!!![[Convergence conditions for gradient descent|https://www.youtube.com/watch?v=Bver7Ttgb9M&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&index=17#t=8m]], to determine ''learning rate''

Nesterov's accelerated gradient

line-search to find step-size

__Constrained optimization with projection based methods__

Gradient step and project on the surface of the constraint area..

!!!__Natural gradients__

[[Original NIPS paper|https://papers.nips.cc/paper/1248-neural-learning-in-structured-parameter-spaces-natural-riemannian-gradient.pdf]]

[[Natural gradients for deep learning|https://arxiv.org/pdf/1301.3584v7.pdf]]
