created: 20160810174239855
creator: cosmos
modified: 20161104134328667
modifier: cosmos
tags: [[Local optimization]]
title: Gradient descent
tmap.id: b5d1a094-7a95-4550-b130-55a793cacbb3
type: text/vnd.tiddlywiki

A [[Local optimization]] technique based on following gradients to decrease a function.

Take steps of a size given by the learning rate along the [[Gradient]].

Can prove convergence for [[Convex function]]s

!!![[Newton's method]]

!!![[Stochastic gradient descent]]

!!![[Convergence conditions for gradient descent|https://www.youtube.com/watch?v=Bver7Ttgb9M&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&index=17#t=8m]], to determine ''learning rate''

Nesterov's accelerated gradient

line-search to find step-size

__Second order/Newton/Quasinewton methods__

Conjugate gradient method

BGFS and L-GBFS

__Constrained optimization with projection based methods__

Gradient step and project on the surface of the constraint area..
