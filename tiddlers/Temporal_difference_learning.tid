created: 20161019173818091
creator: cosmos
modified: 20171208012607347
modifier: cosmos
tags: [[Reinforcement learning]]
title: Temporal difference learning
tmap.id: f82b8412-412c-4fa1-9c06-fe989dbd81b2
type: text/vnd.tiddlywiki

An approach to [[Reinforcement learning]] (particularly [[Model-free reinforcement learning]]) -- [[video|https://www.youtube.com/watch?v=PnHCvfgC_ZA&index=4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=34m10s]]

TD learning is a combination of [[Monte Carlo]] ideas and [[Dynamic programming]] (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment's dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap).

Here we discuss the basic [[On-policy learning]]s. See [[Off-policy learning]] for their extensions to off-policy learning.

[[Sutton -- TD learning|http://videolectures.net/deeplearning2017_sutton_td_learning/]]

!__TD [[Policy evaluation]]__

[[intuition for why we update the current value function assuming the value function at the state after one step, instead of updating it the other way|https://www.youtube.com/watch?v=PnHCvfgC_ZA&index=4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h13m]]

!!__TD0__

[[intro vid|https://www.youtube.com/watch?v=PnHCvfgC_ZA&index=4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=36m10s]]

[[vid|https://www.youtube.com/watch?v=dV80NAlEins&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&index=16#t=19m30s]]

A kind of [[Gradient descent]] to converge to solution to V(s) that satisfies [[Bellman equation]]

[ext[https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node60.html]]

Proven to work (converge to true value function, in the case of table-lookup representation. But in the case of representing value function in some other ways (parametric function approximation), [[there are subtleties.|https://www.youtube.com/watch?v=PnHCvfgC_ZA&index=4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=53m10s]]

!!![[Simple example comparing monte carlo vs TD0|https://www.youtube.com/watch?v=PnHCvfgC_ZA&index=4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=59m]].

If you let TD0 converge on a limited sample (a limited set of episodes from an MDP), it will converge to the [[Maximum likelihood]] estimate [[MRP|Markov reward process]] for that data. [[TD makes use of the Markov property|https://www.youtube.com/watch?v=PnHCvfgC_ZA&index=4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h3m30s]]

!!!Optimality of TD(0)

See section 6.3 of [[Sutton-Barto]]

__n-step look-ahead__

[[intro vid|https://www.youtube.com/watch?v=PnHCvfgC_ZA&index=4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h12m5s]]

[[video|https://www.youtube.com/watch?v=PnHCvfgC_ZA&index=4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h16m]]

We can take $$n$$ steps of the (unknown) MDP, instead of 1. Monte Carlo [[Model-free reinforcement learning]] is when $$n \rightarrow \infty$$

!!__[[TD(lambda)]]__



!__TD control__

[[introduction to TD learning for control|https://www.youtube.com/watch?v=0g4j2k_Ggc4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=5#t=38m45s]]

TD prediction + policy improvement ([[GPI|Generalized policy iteration]])

[[Sarsa]]

-------------------------

Related with [[Actor-critic method]]s