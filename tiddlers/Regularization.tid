created: 20160916173127900
creator: cosmos
modified: 20171218141909559
modifier: cosmos
tags: [[Machine learning]]
title: Regularization
tmap.id: ef001054-6462-4281-87bc-86e4553cbd8a
type: text/vnd.tiddlywiki

Often we want the model to be better at generalizing, and this is done by reducing model complexity. This encourages "encourages fitting signal rather than just noise". A lot of these methods are very much related to [[Model selection]] methods, as both try to make our model better, often by chosing "simpler" models.

[[9.520 - 09/14/2015 - Class 02 - Prof. Tomaso Poggio: The Learning Problem and Regularization|https://www.youtube.com/watch?v=iO919hIhO-w&list=PLyGKBDfnk-iDj3FBd0Avr_dLbrU8VG73O&index=2]] -- [[Class 02 - The Learning Problem and Regularization|https://www.youtube.com/watch?v=SFxypsvhhMQ#t=7m15s]]

We can penalize functions with high complexity, or limiting/penalizing other properties of the function. An example of this is penalizing the size of the weights or the norm of the parameter vector. We can do this by modifying certain parts of our learning algorithm for this purpose:

* by modifying our hypothesis class (like limiting the number of features in [[Dictionary learning]], what is called [[Feature selection]]). Another example is limiting the norm of the function in a [[Reproducing kernel Hilbert space]], which can correspond, for instance, to [[Band-limited function]]s, with no high frequency features (or few).

* See [[here|https://www.youtube.com/watch?v=bBRX3OqNC9c#t=1h]]. by modifying the [[Loss function]], which is perhaps the most common/basic form of regularization, explained [[here|https://www.youtube.com/watch?v=sQ8T9b-uGVE&list=PLA89DCFA6ADACE599&index=11#t=1m]]. This one is what is often meant by regularization.

* we can modify other aspects of the learning algorithm, which do other forms of more [[implicit regularization|Implicit bias]]. 

--> [[See comment here|https://www.youtube.com/watch?v=0UOaS-8E7Po]]: whenever we project our data, or we optimize, we may be able to think of these as forms of regularization!

[[Iterative regularization via early stopping|https://youtu.be/WwxbvHkuEpY?t=32m7s]] -- [[derivation for squared loss|https://youtu.be/WwxbvHkuEpY?t=43m29s]] -- [[result for finite time|https://youtu.be/WwxbvHkuEpY?t=54m52s]]

Using cross-validation for regularization can be done using [[Early stopping]] using the validation set

We can use a [[Prior distribution]] from [[Bayesian statistics]], to make [[simple|Simplicity]] hypothesis more likely. See [[Simplicity and learning]].
 [[Intuition|https://www.youtube.com/watch?v=sQ8T9b-uGVE&list=PLA89DCFA6ADACE599&index=11#t=12m]]

[[Neural networks [2.8] : Training neural networks - regularization|https://www.youtube.com/watch?v=JfkbyODyujw&index=14&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH]]

!!!__[[Dropout]]__

!!!__[[Batch normalization]]__

!!!__[[Tikhonov regularization]]__

----------------------------

[[Amount of distance travelled is what regularizes in iterative regularization|https://youtu.be/TwH_-MzhQ1I?t=22m1s]] 

[[Spectral filtering perspective|https://youtu.be/TwH_-MzhQ1I?t=38m28s]], many regularization algorithms can be seen as filtering out the high frequencies of the covariance matrix of the input data (i.e. make the prediciton not depend on small variations, as these can be easily affected by noise!)

