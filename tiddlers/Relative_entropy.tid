created: 20160919220735827
creator: cosmos
modified: 20161104134332015
modifier: cosmos
tags: [[Information measures]]
title: Relative entropy
tmap.id: 4b28e036-d8eb-400e-9bcc-32f9a512593d
type: text/vnd.tiddlywiki

//aka Kullbackâ€“Leibler divergence, KL divergence//

[[video|https://www.youtube.com/watch?v=fc5FyE41zeo#t=3m15s]]

[[Mutual information]] is a special case. Defines a measure of "distance" between probabiliy distributions. Applications in estimating hypothesis testing errors and in large deviation theory.

$$D_{\mathrm{KL}}(P\|Q) = \sum_i P(i) \, \log\frac{P(i)}{Q(i)}.$$


https://www.wikiwand.com/en/Kullback%E2%80%93Leibler_divergence

https://www.youtube.com/watch?v=QPkb5VcgXAM#t=20m55