created: 20170104121347964
creator: cosmos
modified: 20170905223010159
modifier: cosmos
tags: [[Unsupervised learning]]
title: Dimensionality reduction
tmap.id: 38eb7430-940d-430e-930f-16fb6c90023f
type: text/vnd.tiddlywiki

A type of [[Unsupervised learning]] where we describe the data using less features (called latent factors) than the data was initially described with.

[[Graph embedding and extensions: A general framework for dimensionality reduction|http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.453.8815]]. Basically minimize $$\sum_{i \neq j} ||y_i -y_j||^{2} W_{ij}$$

See also [[Feature learning]], which is very similar.

!!!__[[Factor analysis model]]__

!!!__[[Linear discriminant analysis]]__

!!!__[[Principal component analysis]]__

!!!__[[Multidimensional scaling]]__

https://en.wikipedia.org/wiki/Multidimensional_scaling

!!!__[[Locality preserving projection]]__


!!!__[[Manifold learning]]__

https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Laplacian_eigenmaps

----------------------

[[Non-parametric models|Nonparametric statistics]] are suitable especially for a scenario that all the data points in the source space are known or available and the embedding task needs to be undertaken on a given data set without the need of extension to unseen data points during learning. This is a salient characteristic that distinguishes between parametric and non-parametric subspace learning. As a typical non-parametric subspace learning framework, multi-dimensional scaling (MDS) (Cox and Cox 2000) refers to a family of algorithms that learn embedding a set of given high-dimensional data points into a low-dimensional subspace by preserving the distance information between data points in the high-dimensional space. Sammon mapping (Sammon 1969) is an effective non-linear MDS algorithm.

The fact that it works is related to the [[Sloppy systems]] and the [[Manifold hypothesis]], and [[Simplicity bias]]

--------------

!!!__Incremental algorithms ([[Online learning]])__

[[Incremental Laplacian eigenmaps by preserving adjacent information between data points|http://www.sciencedirect.com/science/article/pii/S016786550900213X?np=y&npKey=a7379a552798ed3cc17a1cfb6ef118a83b9304ec725b827b3449a6898cc8f8bc]]

[[Incremental manifold learning by spectral embedding methods|http://www.sciencedirect.com/science/article/pii/S0167865511001048?np=y&npKey=a7379a552798ed3cd01dc0579657be210f27d523e4ac61e53b81eb388b94a047]]

[[Embedding new observations via sparse-coding for non-linear manifold learning|http://www.sciencedirect.com/science/article/pii/S0031320313002732]]

[[Incremental Construction of Low-Dimensional Data Representations|https://link.springer.com/chapter/10.1007/978-3-319-46182-3_5]]

A New Manifold Learning Algorithm Based on Incremental Spectral Decomposition

-------------------

[[Learning to detect concepts with Approximate Laplacian Eigenmaps in large-scale and online settings|https://link.springer.com/article/10.1007/s13735-015-0079-y]]