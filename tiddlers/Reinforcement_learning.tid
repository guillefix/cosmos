created: 20160428203926307
creator: guillefix
modified: 20180529231119885
modifier: cosmos
tags: [[Machine learning]] Optimization
title: Reinforcement learning
tmap.id: 408808c2-3e36-4109-a712-607087200589
type: text/vnd.tiddlywiki

See [[Machine learning]], [[Decision theory]]

[[Andrew Ng intro lecture|https://www.youtube.com/watch?v=RtxI449ZjSc&index=16&list=PLA89DCFA6ADACE599#t=20s]] -- [ext[book|https://sites.ualberta.ca/~szepesva/RLBook.html]] [[which proves several important theorems|https://www.youtube.com/watch?v=dV80NAlEins&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&index=16#t=10m30]]. -- [[good github repo with sources|https://github.com/dennybritz/reinforcement-learning]] -- [[Sutton&Barto book|http://incompleteideas.net/book/bookdraft2017nov5.pdf]]

https://www.alexirpan.com/2018/02/14/rl-hard.html

-- [[Credit assignment problem|https://www.youtube.com/watch?v=RtxI449ZjSc&list=PLA89DCFA6ADACE599&index=16#t=6m30s]]

[[Intro lecture by David Silver|https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT]] -- [[slides|http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf]] 

!!!__[[Types of RL|https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h11m]] __

* [[Prediction vs control|https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h24m40s]].
** Prediction = [[Policy evaluation]]
**control = find optimal policy
* Planning vs learning+planning
** [[Planning]] = solve RL (prediction/control) problem with full knowledge of MDP
** Learning + planning = RL (prediction/control) in not fully-known environment. need to learn to model environment too!. [[Model-free reinforcement learning]]

[[Free energy principle]]

[img width=300 [RL_taxonomy.png]]
[img width=300 [reinforcement_learning_venn_diagram.png]]

__[[Software for reinforcement learning]]__

!!!__Environment, observation, and [[State|https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=36m30s]]__:

[[The next action could depend on the whole previous history|https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=35m]], however, it is more efficient to just take an action according to a summary of the history (often sufficient statistic of the future) which is called the //agent state//. In fact, there are two types of states in RL.

* ''Enviroment state''. Real state of the environment which determines its future behaviour. Generally, unobservable.
* ''Agent state'' of the model. In general, a function of the agent's history

The agent state is often chosen to be the [[Information state|https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=42m]] or Markov state. An information that contains all useful information of the history, in order to probabilistically predict the next state (a [[Sufficient statistic]] of the future). This Markov state is used as part of the ''model'' the agent uses, which is often formalized as a [[Markov decision process]] ([[vid|https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h06m36s]])

[[Example|https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=49m30s]]

[[Fully observable environment|https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=51m15]]. agent state = environment state = observation. Model = real environment.

[[Partially-observable environment|https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=52m25s]]. Agent needs to learn an "agent state", and an MDP. Now environment state $$\neq$$ agent state $$\neq$$ observation.

!!__[[Markov decision process]]__

(Fully observable) Reinforcement leaning models the world as a Markov decision process. [[Andrew Ng intro to MDPs|https://www.youtube.com/watch?v=RtxI449ZjSc&list=PLA89DCFA6ADACE599&index=16#t=9m]] -- [[operational definition|https://www.youtube.com/watch?v=RtxI449ZjSc&list=PLA89DCFA6ADACE599&index=16#t=19m]] -- [[David Silver lecture|https://www.youtube.com/watch?v=lfHX2hHRMVQ&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=2&spfreload=1]]

[[Planning -> policy evaluation (prediction)-> control|https://www.youtube.com/watch?v=Nd1-UUMVfz4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=3#t=8m10s]]

!!![[Methods for policy evaluation (prediction)|https://www.youtube.com/watch?v=PnHCvfgC_ZA&index=4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h7m]]

!!__Optimal policy problem ([[Optimal control]])__

[[David Silver video|https://www.youtube.com/watch?v=lfHX2hHRMVQ&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=2&spfreload=1#t=1h16m]] -- [[Optimal policy theorem|https://www.youtube.com/watch?v=lfHX2hHRMVQ&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=2&spfreload=1#t=1h17m25s]]!

The core problem of MDPs is to find a ''policy'' for the decision maker: a function $$\pi$$ that specifies the action $$\pi(s)$$ that the decision maker will choose when in state $$s$$ ([[vid|https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=58m44s]])
 . [[vid|https://www.youtube.com/watch?v=lfHX2hHRMVQ&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=2&spfreload=1#t=1h8m9s]]

The __[[goal|https://www.youtube.com/watch?v=RtxI449ZjSc&list=PLA89DCFA6ADACE599&index=16#t=23m]] is to choose a policy $$\pi$$ that will maximize__ some cumulative function of the random __rewards__, typically the __expected__ discounted sum over a potentially infinite horizon (known as the state-[[Value function]]):

|$$V_\pi (s) := \sum^{\infty}_{t=0} {\gamma^t R_{a_t} (s_t, s_{t+1})} $$|''Value function'' ([[vid|https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=59m54s]])|

where we choose $$a_t = \pi(s_t)$$,  $$\ \gamma \ $$ is the [[discount factor|https://www.youtube.com/watch?v=RtxI449ZjSc&list=PLA89DCFA6ADACE599&index=16#t=22m]] and satisfies $$0 \le\ \gamma\ < 1$$. (For example, $$ \gamma = 1/(1+r) $$ when the discount rate is r.)  $$ \gamma $$ is typically close to 1. [[This is known as the total payoff|https://www.youtube.com/watch?v=RtxI449ZjSc&list=PLA89DCFA6ADACE599&index=16#t=20m20s]], or [[value function|https://www.youtube.com/watch?v=RtxI449ZjSc&list=PLA89DCFA6ADACE599&index=16#t=29m]], for policy $$\pi$$.

__Undiscounted reward__

One can have RL with no discount factor $$\gamma$$. Approach: average reward MPD.

One can and often does apply plain undiscounted rewards to [[Episodic MDP]]s

!!__[[Policy evaluation]]__

Computing the value function.
 [[video|https://www.youtube.com/watch?v=Nd1-UUMVfz4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=3#t=12m40s]]

Use [[Bellman expectation equation|Bellman equation]] [[gives a set of linear constraints|https://www.youtube.com/watch?v=RtxI449ZjSc&list=PLA89DCFA6ADACE599&index=16#t=40m20s]] to the value function $$V_\pi (s)$$ that can be solved as a linear system, to obtain the value of the value function, for a given policy. Most effective way to __solve it exactly is iteratively__:

* [[Synchronous update|https://www.youtube.com/watch?v=Nd1-UUMVfz4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=3#t=14m45s]]

Policy evaluation is most often used as a substep of a reinforcement learning algorithm (see next Learning algorithms section)

!__Learning algorithms__

[[video|https://www.youtube.com/watch?v=RtxI449ZjSc&list=PLA89DCFA6ADACE599&index=16#t=28m20s]]

Algorithms to solve the prediction and control problems, i.e. for [[Policy evaluation]] and optimal control (finding [[optimal policies|Optimal policy]])

[[Generalized policy iteration]]

They can be seen in an unified manner as applied to [[Planning]] (see [[Sutton-Barto]] chapter 8). See hybrid methods below. Basically, as in [[Generalized policy iteration]], they revolve in improving value functions by [[Value function backup]]

!!__[[Model-based methods|Model-based reinforcement learning]] (//planning//)__

Use a [[Model]]

!!!__Linear programming approach__


!!!__[[Dynamic programming]] for optimal value function__

[[idea|https://www.youtube.com/watch?v=dV80NAlEins&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&index=16#t=13m05]]
 -- [[Tradeoffs|https://www.youtube.com/watch?v=RtxI449ZjSc&list=PLA89DCFA6ADACE599&index=16#t=1h01m45s]]. The idea is to solve consistency equations (derived by a [[look ahead tree|https://www.youtube.com/watch?v=Nd1-UUMVfz4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=3#t=1h4m25s]] and [[principle of optimality|https://www.youtube.com/watch?v=Nd1-UUMVfz4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=3#t=1h1m48s]]) iteratively (see [[Fixed-point iteration]]). -- [[Summary of methods|https://www.youtube.com/watch?v=Nd1-UUMVfz4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=3#t=1h23m55s]]

!!![[Neuro-dynamic programming|https://www.youtube.com/watch?v=dV80NAlEins&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&index=16#t=3m]]

!!!__[[Policy iteration]]__

!!!__[[Value iteration]]__


!!__[[Model-free reinforcement learning]]__

<small>(learning proper)</small>

[[Unlike model-based RL (planning)|https://www.youtube.com/watch?v=Nd1-UUMVfz4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=3#t=1h36m30s]], we don't know the dynamics. This is also useful even if we know the dynamics, but the state/action space is too big to be computationally tractable (with dynamic programming approaches, we do a short lookahead which has complexity proportional to the number of actions and states). So instead we [[sample|Monte Carlo]] the dynamics, which is more efficient, and can be done even when we don't know the environment.

[[Summary|https://www.youtube.com/watch?v=RtxI449ZjSc&list=PLA89DCFA6ADACE599&index=16#t=1h9m20s]]

[[Model-free prediction|https://www.youtube.com/watch?v=PnHCvfgC_ZA&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=4]] -- [[Model-free control|https://www.youtube.com/watch?v=0g4j2k_Ggc4&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&index=5]]

!!!__Monte Carlo learning__

full look ahead, but only samples

!!!__[[Temporal differences|Temporal difference learning]] (TD)__

one step look ahead, and estimate return.. There are also n-step look ahead versions, and other extensions

//TD0//
. [[vid|https://www.youtube.com/watch?v=dV80NAlEins&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&index=16#t=19m30s]]. 
A kind of [[Gradient descent]] to converge to solution to V(s) that satisfies [[Bellman equation]]

!!!__[[Unkown state transition probabilites|https://www.youtube.com/watch?v=RtxI449ZjSc&list=PLA89DCFA6ADACE599&index=16#t=1h4m48s]]__

* Estimate from data.

!!__Hybrid methods__

Integrate learning and [[Planning]] processes. The most straightforward method is simply by allowing both to update the same estimated [[Value function]].

[img[learning_planning_acting.png]] 

[[Dyna-Q]]

[[Prioritized sweeping]] (see chapter 8.4 of [[Sutton-Barto]])


!!__[[Value function approximation]]__

Methods which attempt to compute the exact value function are called [[Tabular method]]. The other alternative, which can be applied to larger and more difficult problems is to approximate the value function, for instance using a [[neural network|Artificial neural network]]

!!__[[Policy search]]__

[[Policy gradient method]]s --> [[Actor-critic method]]

[[Stochastic gradient descent]] on the parameters determining (stochastic) policy, to maximize expected payoff.

[[Application to POMDP|https://www.youtube.com/watch?v=yCqPMD6coO8&index=20&list=PLA89DCFA6ADACE599#t=44m25s]]

!!![[Optimal value function vs policy search approaches|https://www.youtube.com/watch?v=yCqPMD6coO8&index=20&list=PLA89DCFA6ADACE599#t=42m]]

Policy search is usually best when the policy is a simple function of the state features (like a 'reflex').  Optimal value function approaches are better when the policy is more complicated, maybe needing some multistep reasoning, as in chess.

Policy search often works well, but is very slow, and is stochastic. Also, because one needs to simulate the MDP, it is trained most often using simulation.

See [[here|https://www.youtube.com/watch?v=yCqPMD6coO8&index=20&list=PLA89DCFA6ADACE599#t=48m]] for ''Pegasus policy search'', using "scenarios", which look like [[Quenched disorder]]

!!__[[Evolutionary methods|Evolutionary computing]]__

[[Neuroevolution]]

See papers from Uber AI and OpenAI, [[Deep GA]], [[Evolutionary strategies]]

[[Evolutionary Reinforcement Learning|https://arxiv.org/abs/1805.07917]]

---------------------------------

!!__[[Decision-time planning]]__

A type of planning which is used like a way of encoding a policy, where the action the agent takes is done by solving a [[Planning]] problem focused on the current state.

------------------------------------

!!__[[Reinforcement learning in continuous state space]]__

!!!__[[Fitted value iteration]]__

!!!__[[Linear quadratic regulation]] (LQR)__

!!__[[Partially-observable MDP]] (POMDP)__

!!!__[[Kalman filter]]s and [[LQG control]]__

!!!__[[Spectral methods in reinforcement learning]]__


------------------

!!__[[Debugging RL algorithms|https://www.youtube.com/watch?v=UFH5ibWnA7g&index=19&list=PLA89DCFA6ADACE599#t=1m]]__

!!__Applications__

Many applications to [[Robotics]]

[[More applications]]

-------------------

[[RL Course by David Silver|https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa]] [[course page|http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html]]

__Deep reinforcement learning__

See Nando's lectures

__OpenAI Gym__

https://gym.openai.com/docs

https://github.com/openai/gym

Example: https://github.com/joschu/modular_rl

[[Pavlov.js - Reinforcement learning using Markov Decision Processes|https://github.com/NathanEpstein/Pavlov.js]]

!!![[https://openai.com/blog/universe/]]

!!![[DeepMind Lab|https://deepmind.com/blog/open-sourcing-deepmind-lab/]]

See also [[Decision theory]], [[Game theory]]

!![[Deep reinforcement learning]]

!![[Multi-agent system]]s