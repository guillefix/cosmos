created: 20160221181117472
creator: guillefix
modified: 20161207211940485
modifier: cosmos
tags: Engineering [[Scientific computing]] [[Operations research]] [[Mathematical methods]]
title: Optimization
tmap.id: 000eeacd-eb46-40ca-a38b-1d78a2c89866
type: text/vnd.tiddlywiki

https://en.wikipedia.org/wiki/Mathematical_optimization

https://en.wikipedia.org/wiki/Optimization_%28disambiguation%29

,,[ext[oxford course|http://mpawankumar.info/teaching/cdt-optimization.html]],,

http://www.benfrederickson.com/numerical-optimization/

-----------

$$\min\limits_{x \in X} f(x)$$

* ''Continuous otpimization''. The space $$X$$ over which you are optimizing is continuous
* ''Discrete optimization''. The space $$X$$ over which you are optimizing is discrete. [[Good book|http://www.designofapproxalgs.com/]]

!__Continuous optimization__

Good learning resource: [ext[book + lectures|http://stanford.edu/~boyd/cvxbook/]].

!!!__Unconstrained and constrained optimization__

$$\min\limits_{x \in X} f(x)$$

See [[intro slides|http://mpawankumar.info/teaching/cdt-optimization/lecture1_1.pdf]]. Often $$X = R^n$$ an n-dimensional real number space.

''Constrained optimization'' refers to optimization where the optimization space is some subspace of an "underlying space". This is a bit arbitrary, but the underlying space is most often assumed to be $$R^n$$ and the constraints then define a subset of $$R^n$$ through inequalities and equalities.

!!__[[Constrained optimization]]__

//All of these are mostly refering to continuous optimization//

[[wiki|https://www.wikiwand.com/en/Constrained_optimization]] -- [ext[notes|http://www1.maths.leeds.ac.uk/~cajones/math2640/notes4.pdf]] -- [ext[notes2|http://www.stat.cmu.edu/~cshalizi/statcomp/14/lectures/18/lecture-18.pdf]]

!!!--__Optimization problem with inequality constraints__

A //mathematical optimization problem//, or ''optimization problem''.

[img[http://i.imgur.com/BwndXEY.jpg]]

One can also have equality constraints, apart from inequalities.

!!!__Primal and dual problem and KKT conditions__

[[Andrew Ng video|https://www.youtube.com/watch?v=s8B4A5ubw6c&index=7&list=PLA89DCFA6ADACE599#t=23m30s]] -- [[General optimization problem|https://www.youtube.com/watch?v=s8B4A5ubw6c&index=7&list=PLA89DCFA6ADACE599#t=27m]] -- [[Dual problem|https://www.youtube.com/watch?v=s8B4A5ubw6c&index=7&list=PLA89DCFA6ADACE599#t=34m30s]], uses [[Max–min inequality]]. [[Conditions for the primal and dual problems being equivalent|https://www.youtube.com/watch?v=s8B4A5ubw6c&index=7&list=PLA89DCFA6ADACE599#t=39m]] (includes [[Karush–Kuhn–Tucker conditions|https://www.wikiwand.com/en/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions]])

Has applications in [[Support vector machine]]s. The reasons we use the dual problem is that it has many useful properties, I think it is convex?

!!!__[[Linear programming]]__: [[Linear|Linearity]] objective and constraint functions. Used in [[Operations research]]

!!!__''Nonlinear programming''__

Objective and constraint functins are non-linear. Some special cases:

* [[Convex optimization]]. Generalizes linear programming.
* [[Quadratic programming]]
** [[Least-squares]]

Often for global nonlinear optimization, we need to use brute force methods, with [[Computational complexity]] exponential in the dimension of th optimzation space (space of optimization variables). For approximate but faster solutions, we can use local optimizal optimization, or heuristics.

!!!__[[Lagrange multiplier]] method__

http://mat.gsia.cmu.edu/classes/QUANT/NOTES/chap4/node6.html

Good explanation of inequality constraints using an extension of Lagrange multipliers. Think of planes in 3D as constraint surfaces for intuition on equation! Delta f should be perpendicular to hypersurface defined by constraints.

!!--__[[Local optimization]]__

{{Local optimization}}

[[More things on optimization|https://www.youtube.com/watch?v=0qUAb94CpOw&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&index=6#t=56m20s]]

!__Discrete optimization__

!!!__Discrete optimizatino using energy minimization__

Can formulate discrete optimization as an [[Energy minimization|http://mpawankumar.info/teaching/cdt-optimization/lecture2_2.pdf]] problem on a set of random variables which can take values in a discrete set. This can be formulated as a [[Graphical model]]. [[Energy-based model]], [[Ising model]]..

In simple cases it can be solved iteratively using [[Dynamic programming]]. However, in general, the problem [[NP-hard]] (non-deterministic polynomial).

We can solve it (approximately?) using [[Max-flow min-cut theorem]]. Partition of the graph will correspond to the labelling. We will make terms in the energy correspond to capacities so that the minimum-capacity cut will correspond to minimum energy. For the energy, we will use:

* ''Unary potential''. Links of a particular random variable / node in the [[Graphical model]] to a source, and to a sink, with capacities corresponding to the unary potential (cost of one label or the other).

* ''Pairwise potential''. Links between non-source/sink nodes. Corresponds to cost of two labels being the same (no cut through edge), or different (cut passes through edge). They may be negative, in which case we can't apply this method, and it's NP-hard! This is case is called super-modular. To apply min-cut algo, we want the weights of the link to be postivie, and this is called sub-modular. Sometimes, we can avoid supermodularity, by renaming labels, but in some cases, even this can't avoid negative links, these cases correspond to [[Frustration]]. [[see here|http://mpawankumar.info/teaching/cdt-optimization/lecture2_2.pdf]] (...remember you substracted P from all link weights, on slide..)

Multiple labels: move-making algorithm. 

* Expansion algorithm. $$\alpha$$-expansion.
** Binary choice: Keep your label, or change to a new label $$\alpha$$.
** If the distance function is a [[Metric]], the [[Triangle inequality]] ensures sub-modularity, and can use min cut!

-------------

!!--__Heuristic optimization__

[[Evolutionary computing]]

[[Artificial intelligence]]?

-----------

!!!__Hyperoptimization__

[[Gradient-based Hyperparameter Optimization through Reversible Learning|http://arxiv.org/abs/1502.03492]]

----------------


[[Probabilistic programming|http://probabilistic-programming.org/wiki/Home]]

See links [[here|https://en.wikipedia.org/wiki/Multidisciplinary_design_optimization#Problem_solution]]

[[Memetic algorithm|https://en.wikipedia.org/wiki/Memetic_algorithm]]

[[Evolutionary computing]]

!!![[Simulated annealing|https://en.wikipedia.org/wiki/Simulated_annealing]] ! [ext[http://www.mit.edu/~dbertsim/papers/Optimization/Simulated%20annealing.pdf]]

https://en.wikipedia.org/wiki/Inferential_programming

------------

!!__Applications__

Many applications in [[Science]], [[Engineering]], [[Statistics]]... 

* [[Portfolio theory]]

* [[Electronic design]]

* [[Machine learning]] (data fitting, in particular)

* [[Computational geometry]]


