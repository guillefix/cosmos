created: 20160221181117472
creator: guillefix
modified: 20160808000523685
modifier: guillefix
tags: Engineering [[Scientific computing]] [[Operations research]] [[Mathematical methods]]
title: Optimization
type: text/vnd.tiddlywiki

https://en.wikipedia.org/wiki/Mathematical_optimization

https://en.wikipedia.org/wiki/Optimization_%28disambiguation%29

-----------

!!--__Optimization problem__

A //mathematical optimization problem//, or ''optimization problem''

[img[http://i.imgur.com/BwndXEY.jpg]]

One can also have equality constraints, apart from inequalities.

!!!''[[Linear programming]]'': [[Linear|Linearity]] objective and constraint functions. Used in [[Operations research]]

!!!''Nonlinear programming''

Objective and constraint functins are non-linear. Some special cases:

* [[Convex optimization]]. Generalizes linear programming.
* [[Quadratic programming]]
** [[Least-squares]]

Often for global nonlinear optimization, we need to use brute force methods, with [[Computational complexity]] exponential in the dimension of th optimzation space (space of optimization variables). For approximate but faster solutions, we can use local optimizal optimization, or heuristics.

!!--__Local optimization__

Find local optima, as a method to approximately solve hard nonlinear optimization problems. The main method is based around ''gradient descent''.

!!!''Newton's method''. 

(//Offline algorithm//, you process all the data at each step)

Taylor expand to second order (in multi-variate way) and minimize that (i.e. take derivative (gradient)) and set to 0. [[It performs upper bound minimization|https://www.youtube.com/watch?v=0qUAb94CpOw&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&index=6#t=26m33s]]. 

Newton CG (conjugate gradient) algorithms. 

Expensive thing is computing Hessian. Approximate methods like BFGS, LBFGS.

Line search

!!!''Stochastic gradient descent''

[[Vid|https://www.youtube.com/watch?v=0qUAb94CpOw&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&index=6#t=35m05s]]

(//Online algorithm//, you process the data sequentially, by chunks. You need this if you do not access to all of it at the same time, or you have so much data that not all of it fits on your RAM..)

You only use a mini-batch (a small sample) of input data at a time, in practice

There're theorems that show that this converges well.

Downpour -- Asynchronous SGD

[[Polyak averaging|https://www.youtube.com/watch?v=0qUAb94CpOw&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&index=6#t=49m30s]]. Running average over the parameter values at all time steps performed up to now.

[[Momentum|https://www.youtube.com/watch?v=0qUAb94CpOw&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&index=6#t=50m40s]]. You add inertia to the particle so that the gradient descent is not just velocity = gradient (as it'd be in viscous fluid), but it is acceleration = (viscosity) + gradient.

[[Adagrad|https://www.youtube.com/watch?v=0qUAb94CpOw&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&index=6#t=52m40s]]: Put more weight on rare features [Duchi et al]. <b> Very useful </b> Rare features (i.e. value along a dimension for example) tend to have more information, i.e., they are able to tell you more about what the output $$y$$ should be. This seems maybe related to [[AIT|Algorithmic information theory]].

[[More things on optimization|https://www.youtube.com/watch?v=0qUAb94CpOw&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&index=6#t=56m20s]]



!!--__Heuristic optimization__

[[Evolutionary computing]]

[[Artificial and machine intelligence]]?

-----------

!!!__Hyperoptimization__

[[Gradient-based Hyperparameter Optimization through Reversible Learning|http://arxiv.org/abs/1502.03492]]

----------------


[[Probabilistic programming|http://probabilistic-programming.org/wiki/Home]]

See links [[here|https://en.wikipedia.org/wiki/Multidisciplinary_design_optimization#Problem_solution]]

[[Memetic algorithm|https://en.wikipedia.org/wiki/Memetic_algorithm]]

[[Evolutionary computing]]

!!![[Simulated annealing|https://en.wikipedia.org/wiki/Simulated_annealing]] ! [[http://www.mit.edu/~dbertsim/papers/Optimization/Simulated%20annealing.pdf]]

https://en.wikipedia.org/wiki/Inferential_programming

------------

!!__Applications__

Many applications in [[Science]], [[Engineering]], [[Statistics]]... 

* [[Portfolio theory]]

* [[Electronic design]]

* [[Machine learning]] (data fitting, in particular)

* [[Computational geometry]]


