created: 20181108173901591
creator: cosmos
modified: 20181129103601476
modifier: cosmos
tags: [[Rademacher complexity]] Generalization [[Structural risk minimization]]
title: Margin-based generalization bound
tmap.id: 918cd741-ce85-4e46-83aa-73d9a9e624df
type: text/vnd.tiddlywiki

//aka Margin theory//

See {{UML}} for [[SVM|Support vector machine]] bounds; they arell all based on [[Structural risk minimization]] and bounding [[Rademacher complexity]] by bounding norms of the parameters/

[[Theory of classification: A survey of some recent advances.|https://www.esaim-ps.org/articles/ps/pdf/2005/01/ps0420.pdf]]

[[Video about margin theory|https://youtu.be/8wFMkMI7J1U?t=1331]]

[[Estimation of Dependences Based on Empirical Data|https://www.springer.com/gb/book/9780387308654]] -- [[a training algorithm for optimal margin classifiers|http://www.svms.org/training/BOGV92.pdf]] -- [[Support-vector networks.|https://link.springer.com/article/10.1007/BF00994018]] --  [[The Nature of Statistical Learning Theory|https://www.springer.com/gb/book/9780387987804]]

The algorithmic idea of large margin classifiers was introduced in the linear case by [[Vapnik (1982)|https://www.springer.com/gb/book/9780387308654]] (see also ([[Boser et al., 1992|http://www.svms.org/training/BOGV92.pdf]]; [[Cortes and Vapnik, 1995|https://link.springer.com/article/10.1007/BF00994018]])). [[Vapnik (1995)|https://www.springer.com/gb/book/9780387987804]] gave an intuitive explanation ofthe performance of these methods based on a sample-dependent VC-dimension calculation, but withoutgeneralization bounds. The first rigorous generalization bounds for large margin linear classifiers ([[Shawe-Taylor et al., 1998|https://pdfs.semanticscholar.org/db5f/533d9f06d8d86e4e003478b3dc4bba15b848.pdf]]) required a scale-sensitive complexity analysis of real-valued function classes. At thesame time, a large margin analysis was developed for two-layer networks ([[Bartlett, 1996|https://papers.nips.cc/paper/1204-for-valid-generalization-the-size-of-the-weights-is-more-important-than-the-size-of-the-network.pdf]]), indeed with a proof technique that inspired the layer-wise induction used to prove Theorem 1.1 in the [[present work|https://arxiv.org/abs/1706.08498]] .Margin theory was quickly extended to many other settings (see for instance the survey by [[Boucheron et al.(2005)|https://www.esaim-ps.org/articles/ps/pdf/2005/01/ps0420.pdf]]), one major success being an explanation of the generalization ability of boosting methods, whichexhibit an explicit growth in the size of the function class over time, but a stable excess risk (Schapireet al., 1997)

[[Structural risk minimization over data-dependent hierarchies|https://pdfs.semanticscholar.org/db5f/533d9f06d8d86e4e003478b3dc4bba15b848.pdf]] -- [[For valid generalization the size of the weights is more important than the size of thenetwork|https://papers.nips.cc/paper/1204-for-valid-generalization-the-size-of-the-weights-is-more-important-than-the-size-of-the-network.pdf]] -- [[The sample complexity of pattern classification with neural networks: the size ofthe weights is more important than the size of the network.|https://pdfs.semanticscholar.org/9f1e/b4445219fbc994eb3e47e76cf1428d99815c.pdf]]