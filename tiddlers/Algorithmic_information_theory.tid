created: 20160308171001447
creator: guillefix
modified: 20170421113804109
modifier: cosmos
tags: [[Information theory]] Algorithms [[Theoretical computer science]]
title: Algorithmic information theory
tmap.id: 96374c54-a7b2-4466-a803-a3171997d584
type: text/vnd.tiddlywiki

See [[Descriptional complexity]] and [[MMathPhys oral presentation]]. See also [[Information theory]], [[Theory of computation]], [[Complexity theory]], and [[Computational complexity]]

<mark>Good lecture notes for AIT</mark>: http://www.cse.iitk.ac.in/users/satyadev/a10/a10.html

New book: http://bookstore.ams.org/surv-220?utm_content=buffer87b56&utm_medium=social&utm_source=facebook.com&utm_campaign=buffer

http://algoinfo4u.com/index.html

[[book|http://algoinfo4u.com/AlgoInfo4u.pdf]]

Introduction to Kolmogorov complexity and its applicatoins

!!__[[Kolmogorov complexity]]__

''[[Plain Kolmogorov Complexity|http://www.cse.iitk.ac.in/users/satyadev/a10/kolm_2.pdf]]''

See //Elements of information theory// by Cover and Thomas (chap 14)

[img width=500 class=img-centered [Kolmogorov_complexity_definition.png]]

__Conditional Kolmogorov complexity__

[img[conditional_kolmogorov.png]]

$$<x, y>$$ is the pairing function (see [[Computability theory]]). The conditional Kolmogorov complexity is often defined as in Def. 2.0.1, but with $$y$$ is $$l(x)$$, the length of $$x$$.

__''Universality of Kolmogorov complexity''__

,,aka Invariance theorem,,

[img class=img-centered [kolmogorov_universality.png]]

<b>For sufficiently long x</b>, the length of this
simulation program can be neglected, and we can discuss Kolmogorov
complexity without talking about the constants.

<small>Note in the book on info theory, they use the [[ceiling function|https://en.wikipedia.org/wiki/Floor_and_ceiling_functions]] for the {number of bits in a binary representation of a number}; however, as mentioned [[here|http://www.exploringbinary.com/number-of-bits-in-a-decimal-integer/]] that fails for multiples of $$2$$, so we need to use $$\lfloor log(n) \rfloor +1$$</small>

!!!''Bounds''

__Upper bound on Kolmogorov complexity__

[img[Selection_162.png]]

where $$log^*(x) = \log(x) + \log(\log(x)) + \log(\log(\log(x))) + ...$$

__Lower bounds on Kolmogorov complexity__

::<big><big><i class="fa fa-lightbulb-o" aria-hidden="true"></i></big> There are very few sequences with low complexity</big>

[img width = 500 [Selection_163.png]]

[img[Selection_164.png]]

!!__Relations to entropy__

__Kraft inequality__

[img[Selection_165.png]]

__Relation to entropy__

[img[Selection_166.png]]

as $$n \rightarrow \infty$$. See proof in the book (uses Kraft's inequality, Jensen's inequality, and the concavity of the entropy) Therefore the average Kolmogorov complexity of the string approaches the entropy of the random variable from which the letters of the string are sampled. The compressibility achieved by the computer goes to the entropy limit.

__Theorem__ 14.4.3 //There are an infinite number of integers $$n$$ such that $$K(n) > \log{n}$$//.

!!__Algorithmic randomness and incompressible sequences__

__Theorem__ 14.5.1 //Let $$X_1, X_2, ..., X_n$$ be drawn according to a Bernoulli $$(\frac{1}{2})$$ process. Then//

$$P(K(X_1 X_2 ... X_n | n) < n-k) < 2^{-k}$$

For example, the fraction of sequences of length n that have complexity less than n âˆ’ 5 is less than 1/32. This motivates the following definition.

__Definitions__ of ''algorithmic randomness'', ''incompressibility''. 

//Strong law of large numbers for incompressible sequences//

[img[Selection_167.png]]

In general, we
can show that if a sequence is incompressible, it will satisfy all computable
statistical tests for randomness. (Otherwise, identification of the test that x
fails will reduce the descriptive complexity of x, yielding a contradiction.)
''In this sense, the algorithmic test for randomness is the ultimate test,
including within it all other computable tests for randomness.''

We now remove the //expectation// from Theorem 14.3.1

[img[Selection_168.png]]

!!__''[[Universal probability]]''__

Probability distribution of outputs of a [[Turing machine]], when fed random inputs. Apparently, the distribution is rather robust to the probability distribution of inputs, thus it being called "universal"

!!__The //halting problem// <i class="fa fa-long-arrow-right" aria-hidden="true"></i> noncomputability of Kolmogorov complexity__

//Epimenides liar pradox// <i class="fa fa-long-arrow-right" aria-hidden="true"></i> Godels incompleteness theorem <i class="fa fa-long-arrow-right" aria-hidden="true"></i> Halting problem

Related: //Berry's paradox// and //Bechenbach's paradox//

!!__Chaitin's $$\Omega$$__

__Definition__

[img[Selection_172.png]]

__Properties__:

1. $$\Omega$$ //is noncomputable//

2. $$\Omega$$ //is a "philosopher's stone"//, or an oracle. Knowledge of $$\Omega$$ to $$n$$ bits can be used to prove any theorem for which {a proof expressible in less than $$n$$ bits exists}.

3. $$\Omega$$ //is algorithmically random//.

__Theorem__ 14.8.1. $$\Omega$$ //cannot be compressed by more than a constant; that is, there exits a constant $$c$$ such that//

$$K(\omega_1\omega_2...\omega_n) \geq n-c$$     //for all $$n$$//

!!__Universal gambling__

the universal gambling scheme on a
random sequence does asymptotically as well as a scheme that uses prior
knowledge of the true distribution!

!!![[Universal prediction|http://cbio.ensmp.fr/~jvert/svn/bibli/local/Merhav1998Universal.pdf]]

!!__Occam's razor__

......

!!__''Coding theorem''__

[img[Coding_theorem.png]]

Proof involves an extension of the {//tree construction// used for ''Shannon-Fano-Elias code''s for computable probability distributions} to the uncomputable universal probability distributions.

As stated in the proof in the InfoTheory book, "However, there is no effective procedure to find the lowest depth node corresponding to x". This means that the coding they use in the proof is incomputable. However, they show it exist, and that it can be decoded in finite time, giving a description of the string.

---------------

See also [[Sequence space]]s

---------

http://www.scholarpedia.org/article/Algorithmic_information_theory

<mark>[ext[The discovery of algorithmic probability|http://world.std.com/~rjs/barc97.pdf]]</mark> Seems like vary nice read. [[Solomonoff's theory of inductive inference|https://en.wikipedia.org/wiki/Solomonoff's_theory_of_inductive_inference]]

[ext[An Introduction to Kolmogorov Complexity and Its Applications (1 cr)|http://users.ics.aalto.fi/pkaski/kca/]]

[ext[Algorithmic Learning Theory (ALT) 2016|http://www.comp.nus.edu.sg/~fstephan/alt/alt2016/id.html]]

[[Expanded and improved proof of the relationbetween description complexity and algorithmicprobability|http://www.cs.bu.edu/faculty/gacs/papers/day.pdf]]

[ext[http://www-igm.univ-mlv.fr/~berstel/Articles/2010HandbookCodes.pdf]]

[ext[ALGORITHMS OF INFORMATICS|http://compalg.inf.elte.hu/~tony/Oktatas/AlgofInf-Vol1-HTML/Volume1.html]]