created: 20171023143453623
creator: cosmos
modified: 20181130012228851
modifier: cosmos
tags: [[Generative supervised learning]] [[Probabilistic model]]
title: Gaussian process
tmap.id: 7fb2e34a-cd15-4583-ac08-44d368808763
type: text/vnd.tiddlywiki

[ext[Good quick intro|https://www.robots.ox.ac.uk/~mebden/reports/GPtutorial.pdf]]. See [[Gaussian Processes for Machine Learning|http://www.gaussianprocess.org/gpml/chapters/]]

Gaussian processes are [[Probabilistic model]]s, which define a probability distribution over a set of random variables (i.e. a [[Stochastic process]]); this set of random variables usually corresponds to the values of a function over a set of inputs.

<small>Gaussian processes are usually used in [[Generative supervised learning]]. In brief, generative supervised learning works as follows: Basically assume a certain model $$p(\mathbf{y}|\mathbf{x})$$ where the $$y$$s correspond to the $$x$$s in these vectors. For prediction, we do the following: given output $$y$$s for some inputs $$x$$s as data, we can have a [[Predictive distribution]] for the outputs $$y$$s corresponding to unobserved inputs $$x$$s.</small>

A ''Gaussian process'' models $$p(\mathbf{y}|\mathbf{x})$$ as a [[Multivariate Gaussian distribution]] with a covariance matrix that is given by a [[Kernel]] function (ensuring consistency via what's called the marginaliation property).  This can be interpreted as ''a Gaussian prior on the space of functions'' (i.e. a [[Gaussian random field]])

> Formally we define a Gaussian process as a [[Stochastic process]] where any finite set of [[Random variable]]s in the process is [[jointly|Joint probability distribution]] [[Gaussian]] distributed.

In terms of equations, the values of the function at any finite set of  $$n$$ inputs $$(x_1,...,x_n)$$, are jointly distributed with a Gaussian distribution:,

$$
   P_{\mathbf{\theta}\sim Q} \left(f_\mathbf{\theta}(x_1)=\tilde{y}_1,...,f_\mathbf{\theta}(x_n)=\tilde{y}_n\right) \propto \exp{\left(-\frac{1}{2}\mathbf{\tilde{y}}^T \mathbf{K}^{-1}\mathbf{\tilde{y}}\right)},
$$

where $$\mathbf{\tilde{y}}=(\tilde{y}_1,...,\tilde{y}_n)$$. The entries of the covariance matrix $\mathbf{K}$ are given by the [[Kernel]] function $$k$$ as $$K_{ij}=k(x_i,x_j)$$.

Kernels encode how "similar" two points $$x_i$$ and $$x_j$$  in the input [[Domain]] of the distribution over functions are. What this means precisely is that the kernel at these two points, $$k(x_i,x_j)$$ is high, the the function is more likely to have similar values at these two points. This allows to encode a wide variety of prior knowledge/assumptions about the functions one is trying to learn, like [[Invariance]]s/[[symmetries|Symmetry]]. Often, one chooses kernels that prefers smoothness, so that that $$y$$s which are close $$x$$s under some [[Metric]] (often [[Euclidean metric]]) are more likely to be similar... To see more on choice of kernels, see discussion in the page of [[Reproducing kernel Hilbert space]]s.

Using Gaussian processes for can be efficiently done up to datasets of about 100,000 data points, with current techniques and computers.

-------------

They are equivalent to Bayesian [[Kernel ridge regression]]! (what they call the "weight-space view" in [[here|http://www.gaussianprocess.org/gpml/chapters/RW2.pdf]])

See section 4.3 in Murphy's book (Machine learning - a probabilistic perspective) to see the derivation of the fact that the marginal distribution of a subset of variables from a larger set of random variables which have a [[Gaussian joint distribution|Multivariate Gaussian]]. This is why the Gaussian process property (that the values at any set of points have joint Gaussian distribution) corresponds to a Gaussian prior over functions ([[Gaussian random field]]; [[field|Physical field]] with quadratic energy functional..; see [[Path integral]] ).

----------------

[[Relationships between Gaussian processes, Support Vector machines and Smoothing Splines|https://pdfs.semanticscholar.org/3e38/092b962bcb430fdcebf1407d1299adb1a10b.pdf]] -- [[Support vector machine]] --[[Spline]]s

[[Deep Neural Networks as Gaussian Processes|https://arxiv.org/abs/1711.00165]] -- Extensions for [[CNN|Convolutional neural network]]s

[[Gaussian Process Behaviour in Wide Deep Neural Networks|https://arxiv.org/abs/1804.11271]]

https://en.wikipedia.org/wiki/Gaussian_process

---------------

!!__Gaussian processes with non-Gaussian likelihood__

Usually the observed labels / $$y$$ are assumed to be either equal to the function modelled by the GP, or have a Gaussian distribution around it (what's called a [[Gaussian]] [[likelihood|Likelihood function]] -- note that here the function $$f$$ works like the parameters in [[Bayesian inference]]).

If one assumes a non-Gaussian likelihood, then the problem is not [[Analytically tractable]] any more..

There are several approximations which are used then 

* [[Laplacian approximation|Laplace method]]
* [[Expectation-propagation]]
* [[Variational inference]]

The most common case is in [[Gaussian process classification]]. See [[here|http://www.gaussianprocess.org/gpml/chapters/RW3.pdf]]

