created: 20160704164449084
creator: guillefix
modified: 20170824131752012
modifier: cosmos
tags: [[Machine learning]] [[Probabilistic model]]
title: Graphical model
tmap.id: b32995bc-055f-468e-b4ce-1fdf793c6a5e
type: text/vnd.tiddlywiki

A ''probabilistic graphical model'' is a [[Model]] to represent a [[Joint probability distribution]] (joint PD) of a set of [[Random variable]]s, which takes into account [[causal|Causality]] relations, and dependencies. The models are called graphical, because these dependencies are represented using [[Graph]]s, which allow for building the sparsely-parametrized representations of the joint PDs, and for many useful [[Algorithms]] for inference and learning to be used.

__Factors__ are functions of the random variables, which are used to build the joint PD. One can do conditioning/reduction and marginalization on these factors. The reduction operation is like currying in [[Functional programming]]

http://cs.brown.edu/courses/cs242/lectures/

!__Representation__

[[Coursera course|https://www.coursera.org/learn/probabilistic-graphical-models/home]] -- [[Knowledge engineering|https://www.youtube.com/watch?v=bPGr5kFQbaw]]

[[Graph]]s:

* [[Undirected graphical model|Markov network]]
* Directed graphical models, or [[Bayesian network]]s --> [[Hidden Markov model]]

See [[here|https://www.youtube.com/watch?v=ZYUnyyVgtyA&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&index=25#t=7m10s]] for the distinction of directed vs undirected graphical models. The difference, is that a directed graphical model is an undirected one, but where the factors that correspond to the edges, are normalized, because they correspond to [[Conditional probability]]es

!!__[[Directed graphical models|Bayesian network]]__ (Bayesian nets)

!!!__[[Template model]]s__

Ways of representing graphical models that have a lot of internal shared structure (repeated variables and topologies), like events that occur over time, or relation types found over and over in a graph.. See [[vid|https://www.youtube.com/watch?v=ogs4Oj8KahQ&index=13&list=PL50E6E80E8525B59C]]

* Temporal models for temporal processes.
** [[Dynamic Bayesian network]]s 
*** [[Hidden Markov model]]
* [[Object-relational model]]s
** Directed: [[Plate model]]s
** Undirected

An importance class are those that show [[Structured CPD|https://www.youtube.com/watch?v=gkRBlXj8h-w&index=24&list=PL50E6E80E8525B59C]]s

!!__[[Undirected graphical models|Markov network]]__ (Markov nets)

!!!__Independencies__

[[I-maps and perfect maps|https://www.youtube.com/watch?v=obhBzPaESes&list=PL50E6E80E8525B59C&index=32]].

An [[I-map]] (independence map) for a probability distribution $$P$$ is any graphical model $$G$$ such that the set of independencies implied by the network ($$I(G)$$) is a subset of the set of independences of $$P$$ ($$I(P)$$) (see [[here|http://courses.cms.caltech.edu/cs155/slides/cs155-03-dseparation-marked.pdf]]), i.e. $$I(G) \subset I(P)$$

A perfect (independence) map is one such that $$I(G) = I(P)$$

!!__[[Sum-product network]]s__

!__[[Inference|Inference in graphical models]]__

!!![[Conditional Probability Queries|https://www.youtube.com/watch?v=Qa04kw1gKHk&list=PL50E6E80E8525B59C&index=36]] 

--  [[Exact inference and even approximate inference are|https://www.youtube.com/watch?v=Qa04kw1gKHk&list=PL50E6E80E8525B59C&index=36#t=2m]] [[NP-hard]]. <small>This comes about because the sum-product calculation over all possibilities when doing [[Marginalization]] involves exponentially many terms</small>. However, this is for worst case, and for [[general/average cases, there are practical inference algorithms!|https://www.youtube.com/watch?v=Qa04kw1gKHk&list=PL50E6E80E8525B59C&index=36#t=4m40s]] 

* [[Basic inference|https://www.youtube.com/watch?v=Qa04kw1gKHk&list=PL50E6E80E8525B59C&index=36#t=6m30s]]
* [[Inference with evidence|https://www.youtube.com/watch?v=Qa04kw1gKHk&list=PL50E6E80E8525B59C&index=36#t=8m50s]], use [[Baye's theorem]]

!!![[Maximum a posteriori]] inference

[[video|https://www.youtube.com/watch?v=bL9Fvz3fx4c&index=37&list=PL50E6E80E8525B59C]]. Hm, what about MAP, not {over all unobserved variables}?, i.e. with some marginalization... In any case this is also [[NP-hard]]

!!__Algorithms__

!!![[Probability query algorithms|https://www.youtube.com/watch?v=Qa04kw1gKHk&list=PL50E6E80E8525B59C&index=36#t=14m]]

* [[Variable elimination]], using [[Dynamic programming]]
* Message passing
** [[Belief propagation]]
** [[Variational inference]]
* [[Monte Carlo]] methods, random sampling..
** [[Markov chain Monte Carlo]]
** [[Importance sampling]]

!!![[Maximum a posteriori algorithms|https://www.youtube.com/watch?v=bL9Fvz3fx4c&index=37&list=PL50E6E80E8525B59C#t=7m18s]]

* [[Variable elimination]]
* [[Message passing]]
** Max-product [[Belief propagation]]
* [[Integer programming]] methods
* [[Graph-cut]] methods, and other [[Graph algorithms]]s
* [[Combinatorial search]]

Other [[Optimization]] algorithms.

[[Viterbi algorithm]]

!__[[Learning|Graphical model learning]]__



--------------------

[[1.0 - Welcome-Probabilistic Graphical Models - Professor Daphne Koller|https://www.youtube.com/watch?v=WPSQfOkb1M8&list=PL50E6E80E8525B59C]]

[[Jeffrey A. Bilmes|https://www.youtube.com/channel/UCvPnLF7oUh4p-m575fZcUxg/videos]]


__[[Graphical models|https://en.wikipedia.org/wiki/Graphical_model]]__

They can often be represented as kinds of [[Artificial neural network]]s

[[Energy minimization|http://mpawankumar.info/teaching/cdt-optimization/lecture2_2.pdf]]

!!![[Composing graphical models with neural networks|https://www.youtube.com/watch?v=btr1poCYIzw]]