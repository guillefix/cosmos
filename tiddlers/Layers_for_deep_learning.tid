created: 20160709021804967
creator: guillefix
modified: 20161104134329298
modifier: cosmos
title: Layers for deep learning
tmap.id: b18eca1f-fd2e-4ace-af72-9ebce6a80ac4
type: text/vnd.tiddlywiki

!!__Local functions__

Apply some nonlinear function $$\sigma$$ to each element of the input vector.

[[Linear layer|https://www.youtube.com/watch?v=NUKp0c4xb8w&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&index=9#t=43m19s]]. Linear function

[[ReLU layer|https://www.youtube.com/watch?v=NUKp0c4xb8w&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&index=9#t=46m20s]]. Rectified linear unit. Very popular. For x=0, may use subderivatives..

[img[http://cs231n.github.io/assets/nn1/relu.jpeg]]

[[maxout unit|http://stats.stackexchange.com/questions/155422/exact-definition-of-maxout]]

!!__Max-pooling__

Compute the maximum of all input vector elements

!!__[[Softmax]]__

Exponentiate all vector elements and normalize them to so sum to unity