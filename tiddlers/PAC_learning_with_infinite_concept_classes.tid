created: 20170306112628528
creator: cosmos
modified: 20171031143206202
modifier: cosmos
tags: [[Probably approximately correct]]
title: PAC learning with infinite concept classes
tmap.id: f55893ec-a713-43d4-87da-32fab7578508
type: text/vnd.tiddlywiki

What happens when $$H$$ or $$C$$ is infinite?

!!!-->[[VC dimension]]

Roughly, VC-dimension for infinite class $$\approx \log|H|$$ for finite classes. This will enter an extended version of the Occam's razor theorem..

__Consistent learning algorithm for half-spaces__

We will use [[Linear programming]] (for finding feasible solutions) four our consistent learner.

Is there a minimum number of examples needed to be seen for PAC learning?

__YES__

> If $$C$$ has VC dimension d, then any PAC-learning algo for C that outputs $$h\in C$$, requires at least $$\frac{d-1}{32\epsilon}$$ examples.

The more accuracy we want, or the more complex class, the more data we need.

One can't learn a class of infinite VC dimenison.

__Chernoff bound__: let $$X_1,..X_n$$ be independent random variables with $$X_i =1$$ w.p $$p_i$$, and $$X_i=0$$ w.p. $$1=p_i$$. Let $$\mu = E[\sum_i X_i]=\sum_i p_i$$

$$P[|X-\mu| \geq \alpha \mu] \leq 2e^{-\,u\alpha^2/3}$$

Let $$S$$ be the set which gives the VC-dimension d. (i.e. has maximal cardinality while being shattered by C).

$$D$$ is uniform over S.

Suppose your algo sees d/10 examples. outputs some h.

For all examples in S\{examples}, h makes error w.p at least 1/2.

If your number of examples is less than the vc dimension, then, there's not hope of doing well in general, because the unobserved points could be anything..

See more in book.. <mark>what's the meaning of $$\epsilon$$, here?</mark>

--------------

__[[Growth function]]__

--------------

!!!__[[Sample complexity]] upper bound__

See [[here|http://www.cs.ox.ac.uk/people/varun.kanade/teaching/AML-HT2017/lectures/lecture03.pdf]]. These are upper bounds on the minimum sample complexity to generalize. So that if we are above these, we know we are above the minimum and we will generalize.

> Theorem (''Occam's razor, VC dim version''): Let $$C$$ be a concept class of VC-dimension $$d$$, and $$H$$ a hypothesis class. Let $$L$$ be a consistent learner for $$C$$ using $$H$$. Then, $$\forall n, \forall c \in C_n, \forall D$$ over $$X_n$$, if $$L$$ is given $$m$$ examples drawn from $$EX(c,D)$$ s.t. $$m \geq \frac{1}{\epsilon} (d\log{1/\epsilon}+ \log{1/\delta})$$ then $$L$$ outputs $$h$$, that w.p $$\geq 1-\delta$$, satisfies $$err(h) \leq \epsilon$$.

...... proof. see pic. Uses [[Epsilon-net]] (see more explanation there). Use trick of doubling your training set, to get finite things which allow to bound probabilities..<mark>but need to understand it better...</mark> See [[here|http://www.cs.ox.ac.uk/people/varun.kanade/teaching/AML-HT2017/lectures/lecture03.pdf]], and [[here|https://www.cs.princeton.edu/courses/archive/spring14/cos511/scribe_notes/0220.pdf]]

!!!__Sample complexity lower bound__

See [[here|http://www.cs.ox.ac.uk/people/varun.kanade/teaching/AML-HT2017/lectures/lecture03.pdf#page=6]]

---------------

|PAC learning power changes when you relax the requirement that the algo should work for any distribution on the input data|

''Boosting'', relaxing the $$\epsilon$$ condition doesn't increase power..

Can also give better bounds for finite concept classes, I think..