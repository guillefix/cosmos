created: 20171023212745821
creator: cosmos
modified: 20190516223952176
modifier: cosmos
tags: [[VC dimension]]
title: Growth function
tmap.id: 80e73181-8413-49f3-8b6d-cbc5fada38e7
type: text/vnd.tiddlywiki

The growth function is defined, for every natural number $$m$$, as

: $$\Pi_C(m) = max\{|\Pi_C(S)| | |S| = m\}$$

where $$\Pi_C(S)$$ is the number of [[dichotomies|Dichotomy (concept class)]] of [[Concept class]] $$C$$ on the set $$S$$.

For a concept class of [[VC dimension]] $$d$$, we have

* For $$m \leq d$$, $$\Pi_C (m) = 2^m$$

* $$m \geq d$$, $$\Pi_C(m)$$ is bounded a degree $$d$$ polynomial in $$m$$! See [[here|http://www.cs.ox.ac.uk/people/varun.kanade/teaching/AML-HT2017/lectures/lecture03.pdf]] and [[VC dimension]], [[PAC learning with infinite concept classes]]

See [[here|http://www.stats.ox.ac.uk/~rebeschi/teaching/AFoL/18/material/lecture4.pdf#page=2]]

!!!__Growth function-based [[Generalization error]] bounds__

[[Growth function generalization error bound]]

Can be shown using [[Rademacher complexity]], and is part of most proofs of [[VC dimension]]-based bounds (so uses the double-sample argument, etc)


One can obtain __realizable__ versions of the bounds as well. See the proof of growth function generalization error bound for realizable case (see my notes, or {{UML}}).

[img[growth_function_generalization_error_bound2.png]] From {{UML}} page 51

[img[rademacher_complexity_bound_by_growth_function.png]] From [[here|http://www.stats.ox.ac.uk/~rebeschi/teaching/AFoL/18/material/lecture4.pdf#page=2]]

__Tighter bound (but not uniform over training errors)__

[img[growth_function_generalization_error_bound1.png]] See [[here|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=705570&tag=1#page=3]], [[A result of Vapnik with applications|http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.8106&rep=rep1&type=pdf]]. Note that this is a bound that can be used to get bounds of the form if training error less than blah, then test error is less than blah. But it doesn't give {a bound on the test error, that is dependent on the training error}, which holds uniformly over training errors!

<small>
> In [[here|http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.8106&rep=rep1&type=pdf#page=3]] they prove a tighter version of the [[Growth function generalization error bound]], which allows them to bound a sort of relative version of the generalization gap (divided by the square root of the true error, or by the actual true error, if they assume a bound on it too). 
> The bound they get is not uniform over training error, it's more like a generalization of the realizable bound, it shows that with high probability, if training error is less than blah (this is fixed before drawing the sample), then the generalization error is less than blah (just like this other blah).
> That is why they later do the weighted union bound in [[here|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=705570#page=3]], because they don't really have an agnostic bound. They have a bound that holds can be applied to any training error, but doesn't work __uniformly__ over all training errors. It's a different kind of thing.
</small>


-----------



----------

!!__[[Sauer's lemma]]__

bound the Growth function of sets with bounded VC dimension.

