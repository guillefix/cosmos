created: 20200131151425040
creator: guillefix
modified: 20200131153348820
modifier: guillefix
tags: [[Natural language understanding]]
title: Language representation
type: text/vnd.tiddlywiki

[[Embedding]]s of parts of [[Language]]s (e.g. [[Word]]s, [[Sentence]]s, etc)

There  are  two  existing  strategies  for  apply-ing pre-trained language representations to down-stream tasks:feature-basedandfine-tuning.  

* The feature-based  approach,  such  as  ELMo  (Peterset al., 2018a), uses task-specific architectures thatinclude  the  pre-trained  representations  as  addi-tional features. 

* The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAIGPT) (Radford et al., 2018),  introduces minimaltask-specific  parameters,  and  is  trained  on  thedownstream  tasks  by  simply  fine-tuningallpre-trained parameters. 

in the feature-based approach, we had [[ELMo (2018)|https://arxiv.org/abs/1802.05365]] which was bi-directional but in a simple way. In the fine-tuning approach, we had GTP, which however was trained in a unidirectional way (as a predictive [[Language model]])

[[BERT|https://arxiv.org/abs/1810.04805]] proposed a deeply bidirectional fine-tuning based language representation based on a [[Cloze task]] (<- this seems to be one of the main innovations here)

All of these learned language representations are trained by embedding words/sentences/tokens in the text into certain embeddings, and then through a neural network predict the embeddings of other parts of the text. I think they use cosine distance with the same embeddings as the input embeddings or a new set of output embeddings (linear layer), passed through a softmax to predict the text and then use cross entropy loss typically? The embedding themselves are parameters which are learned, and after the pretraining are supposed to encode useful information about the things (words/sentences/etc) they represent.