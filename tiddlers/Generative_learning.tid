created: 20160821081415623
creator: cosmos
modified: 20160821085404889
modifier: cosmos
tags: [[Supervised learning]]
title: Generative learning
type: text/vnd.tiddlywiki

''Generative learning'' refers to a kind of [[Supervised learning]] task where the objective is learning the function $$p(\text{input}|\text{output})$$, together with $$p(\text{output})$$, which can be used to find $$p(\text{output}|\text{input})$$ using [[Baye's theorem]]. 
See [[notes|http://cs229.stanford.edu/notes/cs229-notes2.pdf]]. See [[lecture video|https://www.youtube.com/watch?v=qRJ3GKMOFrE&index=5&list=PLA89DCFA6ADACE599#t=51s]] [[def|https://www.youtube.com/watch?v=qRJ3GKMOFrE&index=5&list=PLA89DCFA6ADACE599#t=4m50s]]

!!!__Learning method__

The learning method is similar to that described in [[Discriminative learning]]. However, the crucial differences is in how we model the joint likelihood

$$p((x,y)|\theta) = \prod_{i}p(x^{(i)}|y^{(i)};\theta)p(y^{(i)};\phi)$$

where $$\phi$$ are some more parameters in the generative model.

Now to compare with the discriminative case, we can compute

$$p(y^{(i)}|x^{(i)};\theta, \phi) = \frac{p(x^{(i)}|y^{(i)};\theta) p(y^{(i)};\phi) }{p(x^{(i)};\phi,\theta)}$$

where $$p(x^{(i)};\phi,\theta) = \sum_{y\in M} p(x^{(i)}|y;\theta) p(y;\phi) $$

where $$M$$ is the range of values that $$y$$ can take.

Then, another way to write the joint likelihood is:

$$p((x,y)|\theta) = \prod_{i}p(y^{(i)}|x^{(i)};\theta, \phi)p(x^{(i)};\theta, \phi)$$

We can see that <span style="color:coral;">the generative model also models $$p(x)$$</span> 

See more at [[Generative vs discriminative models]]

!!__Examples__

[[Gaussian discriminant analysis]]