created: 20180525152908885
creator: cosmos
modified: 20190319181847978
modifier: cosmos
tags: [[Simplicity bias]] [[Generalization in deep learning]]
title: Simplicity bias in neural networks
tmap.id: 9ccb460c-9dce-4f70-a463-482421602514
type: text/vnd.tiddlywiki

[[Deep learning generalizes because the parameter-function map is biased towards simple functions|https://arxiv.org/abs/1805.08522]]

[[Deep neural networks are biased towards simple functions|https://arxiv.org/pdf/1812.10156.pdf]]

https://www.facebook.com/guillermovalleperez/posts/10156112403591223?comment_id=10156117816756223&notif_id=1527255728896180&notif_t=feedback_reaction_generic

[[Learning in feedforward Boolean networks|https://journals.aps.org/pra/abstract/10.1103/PhysRevA.42.6210]] -- Zipf law distribution of functions

--------------

Good point Диана. Here is the thing, what really matters is the fact that they are biased. What we may call "simple" (and even what some people mathematically define as "simple") is an a priori arbitrary notion. For example, the most common formal definition of simple is "something that has a short description", but that depends totally on the encoding scheme which you use to describe things. Use some encoding scheme, and you get some things being simple, use another one, and you get some other ones being simple! One common encoding scheme is the following idea: give me a probability distribution P(x) over a space X, and then assign a code (string of bits) of length -log(P(x)) for any x in X. This is called Shannon code. Then we can say an x is simple if it has a short description, that is small -log(P(x)), that is high probability. This way, we have a (almost) 1-to-1 relationship between codes (and their associated notion of simplicity), and probability distributions. Whatever your probability distribution is biased towards, is what is "simple" for the associated encoding.

That being said, the interesting result in the "input-output maps" paper, and another paper by Hector Zenil is the finding that a lot of systems in nature appear to produce suprisingly similar probability distributions (in the sense that they are biased towards similar set of patterns). This is not obvious! And the result is that the notions of simplicity that each of this systems represent (by the above equivalence) are very similar! In particular, we can choose one of the notions/measures of simplicity of one common system, which is called Lempel-Ziv (LZ). And then what we found is that, patterns with low LZ tend to be assigned high probability by most of the other systems (i.e., by the equivalence, most of these other systems consider this string "simple" as well)!

The new paper basically finds, that just like most of the other systems we looked at, neural networks also find "simple" what the other systems found simple. So LZ of a function, for instance, is a good predictor of the probability of that function (low LZ high probability, and viceversa).

Finally, from Bayes inference stuff (and also intuitively), we know the following: we want our prior to be as similar as possible to the real distribution of patterns in the world. Well, by the above findings, we know that things in the world tend to be biased in similar ways, and ways which are captured by a variety of common complexity measures (like LZ). So, it makes sense, to do inference well, to have a prior which assigns high prob to things with low LZ for e.g... Our brain probably has a prior similar to this.. But these are all rough similarities.. In reality, the brain, as the good learning system that it is, will have found a rather accurate prior for the world (more accurate that that given by just LZ, say). Given that prior, what we call "simple" is basically "things which are common in the world"! [[this is related to compression, and stuffff]]

------------

Coming more to Aleksandar's point. The idea is this. People already knew that neural networks had to somehow be intrinsically biased towards "simple functions" (where simple now means "things common in the nature", as per above discussion). This was actually only noticed quite recently, but still known since a few years ago. But people don't know the origin of this bias. They thought it was because of using stochastic gradient descent (SGD). Perhaps SGD was a biased algorithm. 

What our paper shows is that the origin of the bias seems to be the parameter-function map itself. So we pushed the mystery a little bit forward. This is progress, but you are very right, that we still don't quite know why the param-fun map is biased like this! As Chico pointed out, we expect it to be, because many things are. But this is far from a rigorous argument!

How could we understand what makes the param-fun map biased? and biased in this way? Well, I don't know. I'm still thinking about it. 

It turns out that even a single neuron (a perceptron) is already biased in a similar way! I tried to find some analysis to get the form of the bias from theory. But the only progress I've really made is to show that getting it exactly is intractable (it is NP-complete to find the exact probability that the perceptron encodes a certain function).
But there could well be some way to get an approximate expression for the probability, or perhaps some inequalities (bounding a range of probabilities) or something.

More generally, any way to show any concentration (concentration is like the technical word for bias..) in the space of functions, due to the param-function map, would be progress! I have some (not published yet) experiments showing a concentration like that. I don't have a theoretical argument for it, but it looks much more promising than trying to predict the bias with LZ.