created: 20170516120241177
creator: cosmos
draft.of: Structural risk minimization
draft.title: Structural risk minimization
modified: 20190515175032415
modifier: cosmos
tags: [[Probably approximately correct]] [[Model selection]]
title: Draft of 'Structural risk minimization'
tmap.id: 89e170d4-ca1a-467d-adc2-e50e633716a3
type: text/vnd.tiddlywiki

//aka SRM//

''Structural risk minimization'' is an extension of [[Probably approximately correct]] learning, where

# one partitions the [[Hypothesis class]] into a nested family of hypothesis classes $$\mathcal{H} = \cup_i \mathcal{H}_i$$, where $$\mathcal{H}_1 \subseteq \mathcal{H}_2 \subseteq ...$$ (although I think it can be extended easily to the case where the subclasses are not nested)

# One applies the weighted version of the [[Union bound]] to get a bound (on the [[Generalization error]]) that is __universal__ over the classes, rather than __uniform__. Meaning, this is a bound that works with high probability probability __simultaneously for functions in all the classes__ but the value of the bound is class-dependent.

<small>//I've sometimes in this wiki used SRM to refer to the weighted Union bound, abusing nomenclature a bit//</small>

One typically combines this with an algorithm that finds a hypothesis with the best bound on its error, which can imply making a tradeoff between the training error and the complexity of the class to which the bound belongs.

See [ext[here|http://www.cs.tufts.edu/~roni/Teaching/CLT/LN/lecture9.pdf]] -- [[A framework for structural risk minimization|http://www.svms.org/survey/SBWA96.pdf]] -- chap 7 in {{UML}}

One can have agnostic or realizable versions of SRM bounds.

__Realizable VC dimension-based SRM generalization error bound__

[img[realizable_vcdim_structural_risk_minimization_theorem.png]] See [[here|http://www.svms.org/survey/SBWA96.pdf]] and [[here|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=705570&tag=1#page=2]]

!!!__The "unlukiness" framework__

[[Structural Risk Minimization Over Data-Dependent Hierarchies|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=705570&tag=1]]. We address a shortcoming of the
SRM method which Vapnik [48, p. 161] highlights: according
to the SRM principle the structure has to be defined a priori
before the training data appear. 

One can ask the question if one can obtain better generalization error bounds by making the hierarchy of classes data-dependent. 

<small>See more at [[Introduction to supervised learning theory]]</small>. SRM is a bound that is hypothesis-dependent, but is not explicitly data-dependent! One also has approaches like [[Rademacher complexity]]-based bounds, or [[Growth function]]-based bounds which are data-dependent, but not hypothesis-subclass, or hypothesis dependent. One can combine both Rademacher with SRM, to obtain hypothesis and data-dependent bounds, and this is what's done to obtain [[Margin-based generalization bound]]s (applied e.g. to [[Support vector machine]]s). In fact, there are ways to obtain margin bounds using  [[PAC-Bayes]] also, which is a general (and probably the tightest in most cases) method to obtain hypothesis and data-dependent bounds!

In [[here|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=705570&tag=1]], Shawe-Taylor et al. propose the ''"unlukiness" framework'' as a general framework for hypothesis and data-dependent bounds. In fact, [[they later applied this to Bayesian-like algorithms|https://pdfs.semanticscholar.org/58d0/7463298078bbb7c45faf6b26ab0e87fb0e6d.pdf]] where they showed that the [[Bayesian evidence]] works as a luckiness function which has the right property to obtain generalization bounds.

[[Apparently|https://twitter.com/roydanroy/status/1127563098552983553]] this was in fact a precuresor of McAllister's [[PAC-Bayes]]

they show that the function
which measures the VC dimension of the set of hypotheses on
the sample points is a valid (un)luckiness function. This leads
to a bound on the generalization performance in terms of this
measured dimension rather than the “worst case” bound which
involves the VC dimension of the set of hypotheses over the
whole input space.

> Our approach can be interpreted as a general way of encoding our bias, or prior assumptions, and possibly taking advantage of them if they happen to be correct. In the case of the fixed hierarchy, we expect the target (or a close approximation to it) to be in a class with small . In the maximal separation case, we expect the target to be consistent with some classifying hyperplane that has a large separation from the examples. This corresponds to ''a collusion between the probability distribution and the target concept'', which would be impossible to exploit in the standard PAC distribution-independent framework. If these assumptions happen to be correct for the training data, we can be confident we have an accurate hypothesis from a small data set (at the expense of some small penalty if they are incorrect)

This is as general as the general (KL divergence) version of [[PAC-Bayes]], which allows the posterior, and thus the bound to depend on the inputs distributions. Actually the simpler Bayesian PAC-Bayes, also implicitly depends on the input-distribution. But in a less general way?

<mark>The Bayesian PAC-Bayes can't encode priors over the pair (function,data distribution), while the general one (and margin bounds also), implictly can! Interesting!</mark>

---------------

Some intuition for //Agnostic structural risk minimization//: I can either do really close to the best among a small set of expert, but they all do pretty badly anyways, or I can do somewhat worse than the best among a bigger set of experts, but as the best does significantly better, I will do better. This tradeoff is what structural risk minimization does. 

Assuming realizability, SRM would just choose the smallest class that has zero training error, as that would give the best bound on generalization error!

-------------

What about the algorithm: Check the size of the training set, $$m$$, and see what is the largest $$H$$ in the collection such that $$m$$ guarantees generalization gap smaller than $$\epsilon$$. Then just do ERM with that $$H$$? Would that work for [[Nonuniform learnability]]?