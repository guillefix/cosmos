created: 20160327021039036
creator: guillefix
modified: 20171129131649578
modifier: cosmos
tags: [[Machine learning]]
title: Supervised learning
tmap.id: d55a0869-d29f-45d0-a772-013b14fcf7c3
type: text/vnd.tiddlywiki

//aka predictive learning//

The goal is to learn a mapping from inputs $$x$$ to outputs $$y$$, given a labeled set of input-output pairs $$D = \{(x^{(i)}, y^{(i)})\}_{i=1}^N $$. Here $$D $$ is called the ''training set'', and $$N $$ is the number of training examples.

Training data consisting on ''inputs and outputs''.
<small>Other names for inputs: predictors, independent variables, features, attributes, covariates. Other names for outputs: responses, response variables, dependent variables.</small>

In supervised learning, we want to find function relating inputs to outputs, to then be able to predict new outputs from new inputs. Often, we need ''a way to represent the function approximation'', with some parameters (the ''model''; with some subtleties for [[non-parametric|Nonparametric statistics]] ones) and a ''learning algorithm'' to find best parameters for the data, so that the model can predict well. 

-- See [[Learning theory]], to learn about the way learning algorithms work, overfitting, underfitting, generalization  ''model selection'', etc.. 

New paradigm: [[Deep learning]]

!!__Types of supervised learning algorithms__

!!!__[[Generative vs discriminative models|https://www.youtube.com/watch?v=oTtow2Ui8vg&index=5&list=PLD0F06AA0D2E8FFBA]]__.

See below

!!!__Parametric vs non-parametric model__

* ''Parametric''. There is a fixed number of parameters that the algorithm fits.
* ''Non-parametric''. Formally, the number of "parameters" grows with the training set. Here number of parameters basically refers to "amount of information in learned  function". For See [[Nonparametric statistics]] An example is [[Nearest-neighbour classification]].

!!!__Continuous vs categorical output__

//Categorical//, or //nominal//. Output $$y$$ belongs to some finite [[Set]]. The learning problem is called [[Classification]].

//continuous//. $$y $$ is a [[Real number]], or belongs to $$R^n $$ more generally. Learning is then known as [[Regression|Regression analysis]].

//ordinal//. When the output belongs to some set with some natural [[Ordering]]. Learning is then known as //ordinal ordering//

!!__[[Regression|Regression analysis]]__

Output value is continuous, and quantiative (i.e. it has an ordering, and a notion of closeness (matrix)).

!!__[[Classification]]__

Output value is discrete, or categorical, or qualitative. No implicit ordering, or closeness on the variables. Simple approach: [[Logistic regression]]

!!__[[Discriminative learning]]__

Learning the function $$p(\text{output}|\text{input})$$. See [[notes|http://cs229.stanford.edu/notes/cs229-notes1.pdf]]

!!__Structured learning__

[[wiki|https://www.wikiwand.com/en/Structured_prediction]].

Often this actually deals with problems where input space has a different structure than a vector space ([[video|https://youtu.be/SFxypsvhhMQ?t=28m17s]])

!!!__General methods__

[[Generalized linear model]]

[[Artificial neural network]] (see [[Deep learning]])

[[Support vector machine]]

[[Decision tree]]

!!__[[Generative supervised learning]]__

Learning the function $$p(\text{input}|\text{output})$$, together with $$p(\text{output})$$, which can be used to find $$p(\text{output}|\text{input})$$ using [[Baye's theorem]]. 
See [[notes|http://cs229.stanford.edu/notes/cs229-notes2.pdf]]. See [[lecture video|https://www.youtube.com/watch?v=qRJ3GKMOFrE&index=5&list=PLA89DCFA6ADACE599#t=51s]] [[def|https://www.youtube.com/watch?v=qRJ3GKMOFrE&index=5&list=PLA89DCFA6ADACE599#t=4m50s]]

!!!__[[Gaussian discriminant analysis]]__

!!!__[[Naive Bayes]]__