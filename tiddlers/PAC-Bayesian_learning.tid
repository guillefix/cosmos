created: 20170510213348476
creator: cosmos
modified: 20191014121918186
modifier: cosmos
tags: [[Learning theory]]
title: PAC-Bayesian learning
tmap.id: 2d6d1446-17eb-4acd-95df-1b3bb34b689f
type: text/vnd.tiddlywiki


An extension of [[Probably approximately correct]] learning to the cases where not all concepts in the concept class are equally likely, but instead have a general probability distribution over them, which can be interpreted as a [[Prior distribution]].

''PAC-Bayes theorem (Gibbs posterior version)'' ([[Some PAC-Bayesian Theorems|https://link.springer.com/article/10.1023/A:1007618624809]])

>For any measure $$P$$ on any concept space and any measure on a space of instances we have, for $$0< \delta \leq 1 $$, that with probability at least $$1-\delta$$ over the choice of sample of $$m$$ instances all measurable subsets $$U$$ of the concepts such that every element of $$U$$ is consistent with the sample and with $$P(U) > 0$$ satisfies the following:

> $$\epsilon(U) \leq \frac{\ln{\frac{1}{P(U)}}+\ln{\frac{1}{\delta}} + 2\ln{m} + 1}{m}$$

> where $$P(U)=\sum_{c\in U} P(c)$$, and where $$\epsilon(U) := E_{c\in U} \epsilon(c)$$, i.e. the expected value of the generalization errors over concepts $$c$$ in $$U$$ with probability given by the posterior $$\frac{P(c)}{P(U)}$$. Here, $$\epsilon(c)$$ is the generalization error (probability of the concept $$c$$ disagreeing with the target concept, when sampling inputs).

See proof [[Some PAC-Bayesian Theorems|https://link.springer.com/article/10.1023/A:1007618624809]] and also in my notebook in the office.

The Gibbs posterior version is a special case of the ''General PAC-Bayes theorem'' ([[PAC-Bayes model averaging|http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.46.6957&rep=rep1&type=pdf]]):

[img[PAC-Bayesian model averaging.jpg]]

This is actually a particular case of the ''Relative entropy general PAC-Bayes theorem'' from Langford which tightens the above bound, specially at large values of the bound (in particular the Langford one is automatically smaller than $$1$$). The theorem is here:

: [[Bounds for Averaging Classifiers|http://hunch.net/~jl/projects/prediction_bounds/averaging/averaging_tech.pdf]]


See <big>[ext[here for proof|https://courses.cs.washington.edu/courses/cse522/11wi/scribes/lecture13.pdf]]</big> of general (KL) version. See derivation on blackboard (pic..) for connection with Gibbs learner version ([[graph of bounds for KL of binary variables|https://www.desmos.com/calculator/hr4qquya5e]])

To see how the Theorem 1 from 
<small>[[Some PAC-Bayesian Theorems|https://link.springer.com/article/10.1023/A:1007618624809]]</small> follows from the general theorem below ( <small>[[PAC-Bayes model averaging|http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.46.6957&rep=rep1&type=pdf]]</small> ), see picture of blackboard derivation, or see [[here|https://youtu.be/mxc3jqz2gBE?t=1396]].

One can also show that the Gibbs posterior gives the optimal bound of the form given by the general theorem. See discussion in [[Evidence lower bound]]. 

-- [[Tutorial on Practical Prediction Theory for Classification|http://www.jmlr.org/papers/volume6/langford05a/langford05a.pdf]] (__has nice proof in here too__)

__History__

PAC-Bayes is inspired by [[Structural risk minimization]]. See [[this paper by Shaw and Taylor|https://pdfs.semanticscholar.org/58d0/7463298078bbb7c45faf6b26ab0e87fb0e6d.pdf]]

-----------

There is a version that is essentially just [[Structural risk minimization]]/Weighted Union bound:

[img[PAC-Bayesian model selection.jpg]]

[[PAC-Bayesian Theory|https://link.springer.com/chapter/10.1007/978-3-642-41136-6_10]]

[[video lecture on PAC-Bayesian learning|https://www.youtube.com/watch?v=OkftNAp1_Fg]]

Nonvacuous bounds for stochastic DNNs paper: https://arxiv.org/pdf/1703.11008.pdf

__Distribution-dependent bounds__

See [[here|https://arxiv.org/abs/0712.0248]]
 and [[here|https://www.sciencedirect.com/science/article/pii/S0304397512009346]]

[[Data-dependent PAC-Bayes priors via differential privacy|https://arxiv.org/abs/1802.09583]]

[[A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks|https://arxiv.org/abs/1707.09564]]

[[Deep learning generalizes because the parameter-function map is biased towards simple functions|https://arxiv.org/abs/1805.08522]]

[[PAC-Bayesian Generalisation Error Bounds for Gaussian Process Classification|http://www.jmlr.org/papers/volume3/seeger02a/seeger02a.pdf]]

-----------------

__Optimal priors__

See notes on paper

https://photos.google.com/search/blackboard/photo/AF1QipMudW2vT2w7OTE5t0xflAJRCT7yPC8fEN5oC1tC

See notes on Telegram

I think you get that the average bound is some average entropy independent of the pacbayes prior, play the cross entropy between the average probability of obtaining a function and the pb prior. Therefore the optimal prior is the average probability of obtaining a function.

find this by expressing KL div a sum over functions (definition), and the sum over U|X as  another sum over functions (as I did in the other derivations), and then swap the sums.

to be precise, optimal prior is pi(c)=sum_X D(X) sum_c' P(c')Q_U(c) = sum_c' P(c') sum_X D(X) Q_U(c)

where U is a function of c' and X