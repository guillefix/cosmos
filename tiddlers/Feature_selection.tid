created: 20160914090040608
creator: cosmos
modified: 20160919093333616
modifier: cosmos
tags: [[Model selection]]
title: Feature selection
type: text/vnd.tiddlywiki

[[video|https://www.youtube.com/watch?v=0kWZoyNRxTY&index=10&list=PLA89DCFA6ADACE599#t=54m28s]]

In feature selection, we would like to select a subset of the features (input variables to a supervised learning algo) that are the most ''relevant'' ones for a specific learning problem, so as to get a simpler hypothesis class, and reduce the risk of overfitting. This is useful mostly when we have many features.

What we do is use heuristics to search the [[huge space|https://www.youtube.com/watch?v=0kWZoyNRxTY&index=10&list=PLA89DCFA6ADACE599#t=56m54s]].

__"Wrapper" feature selection__

[[vid|https://www.youtube.com/watch?v=0kWZoyNRxTY&index=10&list=PLA89DCFA6ADACE599#t=1h01m47s]]. Feature selections that repeatedly call your learning algo. Work well, but are computationally expensive.

* [[Forward search|https://www.youtube.com/watch?v=0kWZoyNRxTY&index=10&list=PLA89DCFA6ADACE599#t=57m30s]]
* [[Backward search|https://www.youtube.com/watch?v=0kWZoyNRxTY&index=10&list=PLA89DCFA6ADACE599#t=1h02m30s]]

Number of features to include can be chosen by optimizing generalization error (estimated by cross-validation), or by chosen a plausible number..

__"Filter" feature selection__

[[vid|https://www.youtube.com/watch?v=0kWZoyNRxTY&index=10&list=PLA89DCFA6ADACE599#t=1h05m47s]]. Less computationally expensive, but often less effective.  For each feature i, we'll compute some measure of how informative x,,i,, is about y, for instance by computing:

* [[Correlation]] between x,,i,, and y, or
* Empirical [[Mutual information]]

!!__Learning meaningful ''representations'' of the data__

Can learn from [[Unsupervised learning]], or [[Supervised learning]] algorithms!

See [[Transfer learning]]

__[[Restricted Boltzmann machine]] feature learning__

See [[here|https://www.youtube.com/watch?v=S0kFFiHzR8M&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&index=41#t=0m50s]]