created: 20160328182624640
creator: guillefix
modified: 20170325150709328
modifier: cosmos
tags: [[Machine learning]]
title: Deep learning
tmap.id: 6c429899-5ef7-49f2-9b8e-9dd84cd593cb
type: text/vnd.tiddlywiki

Deep learning [[Machine learning]] in a modular way using ''layers'', like in [[Torch|Torch (Deep learning framework)]]. [[Artificial neural network]]s, with many layers..

[[Two+ Minute Papers - How Does Deep Learning Work?|https://www.youtube.com/watch?v=He4t7Zekob0&index=5&list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2]]
-- 
[[The computer that mastered Go|https://www.youtube.com/watch?v=g-dKXOlsf98]]

[[Oxford course (with video)|https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/]] on lecture 12

[[matlab|https://uk.mathworks.com/discovery/deep-learning.html]]

The idea is also that layers are //recursive//, i.e. layers can be made up of layers.

[[future|https://www.youtube.com/watch?v=x1kf4Zojtb0#t=43m05]]

Concepts as programs; programs as networks

[[Probabilistic programming]], [[Program induction]]

trainning models based on demonstration

Multi-agents, and [[Communication]]

Generating programs is not that different from generating [[explanations|Explainable artificial intelligence]]

[[Augmented RNN]]s

http://www.thespermwhale.com/jaseweston/

NIPS2016

----------------

!!Deep learning methods

!!!__Neural networks for spatially structured data__

[[Convolutional neural network]]

[[Multi-scale networks|Deep multi-scale video prediction beyond mean square error|http://arxiv.org/abs/1511.05440]] and [[an application|http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf]].

[[Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers|http://arxiv.org/abs/1202.2160]]

http://www.clement.farabet.net/research.html#parsing

[[Computer vision]]

[[Residual neural network]]

!!!__Neural networks for [[sequential|Sequence]] data__

[[Recurrent neural network]]

!!!__[[Transfer learning]]__

good for generalizing models, ''transfer learning'', ''multi-task learning''. Good when don't have much supervision data.

!!!__[[Neural networks with memory]]__

''Memory'' is good for recognizing time sequence data. See [[Long short-term memory]].

!!!__[[Attention in machine learning]]__

!!!__[[Integrating symbols into deep learning]]__

* [[Neural Turing machine]]

* [[Neural programmer-interpreter]]

* [[Deep symbolic reinforcement learning]]

!!!__[[Deep reinforcement learning]]__

//more...//

[[Structured learning|https://www.youtube.com/watch?v=x1kf4Zojtb0#t=20m]] -- Learning to learn and compositionality with deep recurrent neural networks

------------------

!!!Some techniques for deep learning

__[[Layers for deep learning]]__

!![[New advances in deep learning]]

[[Dropout]]. [[usefulness of dropout|https://www.youtube.com/watch?v=NUKp0c4xb8w&list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&index=9#t=50m20s]]

[[Batch normalization]]

[[Predicting Parameters in Deep Learning|file:///home/guillefix/Dropbox/Oxford/Systems%20Biology%20DPhil/Research/DeFreitas-NIPS2013_5025.pdf]] The intuition motivating the techniques in this paper is the well known observation that the first layer features of a neural network trained on natural image patches tend to be globally smooth with local edge features, similar to local Gabor features [6, 13]. I.e. they are seizing the [[Simplicity]] often found in real-world structures. Given this structure, representing the value of each pixel in the feature separately is redundant, since it is highly likely that the value of a pixel will be equal to a weighted average of its neighbours.  

The core of the technique is based on representing the weight matrix as a low rank product of two smaller matrices. 

-----------

!![[Deep learning theory]]

!!Deep learning applications

[[Deep art]]

[[Applications of AI]]

--------

__[[Hardware for deep learning]]__

__[[Software for deep learning]]__

---------

__[[People in deep learning]]__


[[History of deep learning]]

-----------

__Books and reources__

[[Deep learning in neural networks: An overview|http://www.sciencedirect.com/science/article/pii/S0893608014002135]]

https://deepmind.com/publications.html

http://www.deeplearningbook.org/

http://carpedm20.github.io/

--------------