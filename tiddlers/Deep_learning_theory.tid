created: 20160913070239061
creator: cosmos
modified: 20181109220844784
modifier: cosmos
tags: [[Deep learning]] [[Learning theory]]
title: Deep learning theory
tmap.id: dca63f03-b5f2-4b10-9487-ab8f22b4c32f
type: text/vnd.tiddlywiki

[[Neural network theory]] -- [[Learning theory]]

//Better understanding the shortcomings and potential of deep learning may suggest ways of improving it, both to make it more capable and to make it more robust [2].//

* [[Generalization]] -- [[Generalization in deep learning]]
* Optimization -- [[Optimization for learning]], [[Optimization for training deep models]]

Nice lecture videos: https://www.researchgate.net/project/Theories-of-Deep-Learning -- https://stats385.github.io/readings

[[ICML 2018 tutorial |https://www.youtube.com/watch?v=KDRN-FyyqK0&index=5&list=PLhuJd8bFXYJtxv2Y9ZoX7vClP96M28VnW]]

[[A Theoretical Framework for Deep Learning Networks|https://www.youtube.com/watch?v=YVjvRvvVn4w]]

[[The mathematics of deep learning|https://arxiv.org/abs/1712.04741]]

!!![[Learning theory and neural networks gingko tree|https://gingkoapp.com/vehvff]]

[[Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes|https://arxiv.org/abs/1706.10239]]

[[Understanding deep learning requires rethinking generalization|https://openreview.net/forum?id=Sy8gdB9xx&noteId=Sy8gdB9xx]]

[[Why and when can deep - but not shallow - networks avoid the curse of dimensionality: a review|https://arxiv.org/pdf/1611.00740.pdf]], see also [[Expressivity of neural networks]]

The approximation of functions with a compositional structure â€“ can be achieved with the same degree of accuracy by deep and shallow networks but that the number of parameters are much smaller for the deep networks than for the shallow network with equivalent approximation accuracy.

[[Lectures from Beg Rohu 2018 summer school|https://www.youtube.com/channel/UCuuXUxfZlPTfvKxGdzXLEIw]]

!!__[[Generalization in deep learning]]__

[[Deep learning generalizes because the parameter-function map is biased towards simple functions|https://openreview.net/forum?id=rye4g3AqFm]]

!!__[[Expressibility of neural networks]]__

[[Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations|https://arxiv.org/abs/1708.02691]]

[[Exponential expressivity in deep neural networks through transient chaos|http://papers.nips.cc/paper/6322-exponential-expressivity-in-deep-neural-networks-through-transient-chaos]]

[ext[slides|http://www.mit.edu/~rakhlin/papers/myths.pdf]]

[[bounds for the computational power and learning complexity of analog neural nets|https://epubs.siam.org/doi/pdf/10.1137/S0097539793256041]]

[[On the Depth of Deep Neural Networks: A Theoretical View|https://arxiv.org/abs/1506.05232]]

-----------------------

!!__Information propagation in deep networks__

!!!__[[Information bottleneck in deep learning]]__

See [[gingkoapp tree|https://gingkoapp.com/app#72f84d34b9dce3b61f00003d]]

[[Information Theory of Deep Learning. Naftali Tishby|https://www.youtube.com/watch?v=bLqJHjXihK8&feature=youtu.be]]

[[Deep Learning and the Information Bottleneck Principle|https://arxiv.org/abs/1503.02406]]

[[Opening the Black Box of Deep Neural Networks via Information|https://arxiv.org/pdf/1703.00810.pdf]]

See more at [[Information bottleneck]]

!!!__[[Deep Information Propagation|https://arxiv.org/abs/1611.01232]]__
[[Exponential expressivity in deep neural networks through transient chaos|http://papers.nips.cc/paper/6322-exponential-expressivity-in-deep-neural-networks-through-transient-chaos]]

[[Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice|https://arxiv.org/abs/1711.04735]]

!!!__[[Gradients problems in deep learning]]__

[[Vanishing gradients problem]]

[[Exploding gradients problem]]

[[Residual neural networks]]s -- [[Highway network]]s

--------------------------------

[[Empirical Analysis of the Hessian of Over-Parametrized Neural Networks|https://arxiv.org/abs/1706.04454]]

[[On the Expressive Power of Deep Neural Networks|https://2017.icml.cc/Conferences/2017/Schedule?showEvent=841]]

!!__[[Optimization]] dynamics, generalization__

[[A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks|https://arxiv.org/abs/1810.02281]]

[[Backprop minimizes the free energy|https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?utm_content=buffer2f5bd&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer]]

[[Geometry, Optimization and Generalization in Multilayer Networks|https://www.youtube.com/watch?v=ACdjYP0-cMw]]. He talks about the capacity of the network in a similar way to the theory of neural nets (see [[this video|https://www.youtube.com/watch?v=Vguh9vUb0zc]]). I think it's the same as [[VC dimension]] actually. It's more or less proportional to the number of edges, apparently.

[[Time-bounded functions can be represented cheaply|https://www.youtube.com/watch?v=ACdjYP0-cMw#t=4m30s]] Feedforward neural nets are the ultimately learning machines but 
 -- [[Learning is hard|https://www.youtube.com/watch?v=ACdjYP0-cMw#t=6m30s]]

[[The fact that we can learn so effectively is quite mysterious|https://www.youtube.com/watch?v=ACdjYP0-cMw#t=8m]]

|[[Optimization and capacity control|https://www.youtube.com/watch?v=ACdjYP0-cMw#t=9m35s]], which avoids [[Overfitting]]!|

[img width=300 [https://s29.postimg.org/lfinrt49j/Selection_016.jpg]]

[[Relation to |https://www.youtube.com/watch?v=ACdjYP0-cMw#t=17m5s]][[Matrix factorization]]. Better and worse inductive biases is what seems to cause things. For the case of matrix factorization in real use cases it seems that matrices have low trace norm more than they have low rank. Given the right inductive bias, we can learn efficiently, and generalize well.

[[Good inductive bias for matrix factorization seems to be trace norm rather than rank|https://www.youtube.com/watch?v=ACdjYP0-cMw#t=26m45s]] If we have trace norm giving us the inductive bias, then decreasing rank (less neurons) often doesn't actually help with generalization..., as increasing the rank/number of hidden units can allow for lower trace norms.

Need to find right measure of [[Complexity]], for NN "path norm" seems best from those they tried.

[[Real vs random labels|https://www.youtube.com/watch?v=ACdjYP0-cMw#t=36m40s]], behaviour is as expected with random data

But we didn't tell the network to minimize the path norm (complexity). [[So where is the regularization coming from?|https://www.youtube.com/watch?v=ACdjYP0-cMw#t=39m5s]]. He thinks it's the [[Optimization]] algorithm that is biasing us towards simple global optima (work on this for convex opti?), but couldn't it be a [[GP map|Genotype-phenotype map]]-like [[Simplicity bias]]. He approaches it from [[Geometry]]. I think he is right in that the algorithm plays a role in biasing. But GP bias probably does also, or it could at least be seized (as in [[Neuroevolution]] with indirect encodings..)

In any case the experiments where more hidden units allow for better generalization means that more hidden units allow to drive the complexity measure lower (so rank isn't the right complexity measure. See [[here|https://www.youtube.com/watch?v=ACdjYP0-cMw#t=27m50s]])

[[Mollifying Networks|https://arxiv.org/abs/1608.04980]] and smoothening landscapes

----------------------

!!__Deep learning, [[Spin glass]]es, [[Protein folding]], [[Loss surface|Loss surface of neural networks]]s__

[[Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior|https://openreview.net/forum?id=SkPoRg10b]]

<div style="display:inline-block; width:350px;">[img width=350 [deep_learning_random_energy_model_glass_transition.png]]</div>
<div style="display:inline-block;width:300px;">

<p>[[why deep learning works -- spin glasses, protein folding, etc.|https://charlesmartin14.wordpress.com/2015/03/25/why-does-deep-learning-work/]]</p>

<big>-->[[Why Deep Learning Works|https://www.youtube.com/watch?v=kIbKHIPbxiU]]</big>

-- [[Improving RBMs with physical chemistry|https://charlesmartin14.wordpress.com/2016/10/21/improving-rbms-with-physical-chemistry/]]

-- [ext[Energy landscapes of deep networks|http://vision.ucla.edu/~pratikac/pub/chaudhari.cs269.16.pdf]]

</div>

[[A Correspondence Between Random Neural Networks and Statistical Field Theory|https://arxiv.org/abs/1710.06570]]

|The primary argument about the funnel is that these learning systems are strongly correlated, and therefore not readily treated by mean field theory. Specifically, the classical idea was that strongly correlated random models lie in a different universality class. So they behave completely differently than a spin glass , and this gives rise to the convexity .|

See discussion in comments [[here|https://charlesmartin14.wordpress.com/2016/09/24/mmds-video-presentation/]] and [[here|https://charlesmartin14.wordpress.com/2016/10/21/improving-rbms-with-physical-chemistry/]]. [[Comments on why deep learning works: perspectives from theoretical chemistry]]

[[The Loss Surfaces of Multilayer Networks |http://www.jmlr.org/proceedings/papers/v38/choromanska15.pdf]] -- loss surfaces paper. uncorrelated inputs, and outputs?: Yeah basically everything is random, so they can't show dependence of behvaior on properties of the function being learned. They focus on random functions really..

See [[Statistical mechanics of neural networks]]

!!!-->[[Theoretical neuroscience and deep learning theory|http://videolectures.net/deeplearning2016_ganguli_theoretical_neuroscience/]]

---------------------

!!__[[Renormalization group]] and [[Deep learning]]__

[[Why Deep Learning Works II: the Renormalization Group|https://charlesmartin14.wordpress.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/]]

[[An exact mapping between the Variational Renormalization Group and Deep Learning|http://arxiv.org/abs/1410.3831]]

[[WHY DOES UNSUPERVISED DEEP LEARNING WORK?- A PERSPECTIVE FROM GROUP THEORY|http://arxiv.org/pdf/1412.6621v3.pdf]]

[[Supervised Learning with Quantum-Inspired Tensor Networks|https://arxiv.org/abs/1605.05775]]

[[Deep learning and the renormalization group|http://128.84.21.199/pdf/1301.3124v1.pdf]]

<small>See also comments on [[Why does deep and cheap learning work so well?]]</small>

Renormaliation group can be seen through the lens of compression/[[Information bottleneck]]

-----------------------

!!__Simplicity and deep learning__

See also [[Simplicity and learning]]

!!![[Why does deep and cheap learning work so well?]]

----------------------------

!!__[[Neuroscience]] and deep learning__

!!!-->[[Theoretical neuroscience and deep learning theory|http://videolectures.net/deeplearning2016_ganguli_theoretical_neuroscience/]]

--> Francois Chollet. Ablation studies are a good idea when we don't have much theory.. For world models, apparently LSTM even with random weights work well <<--

---------------------

[[Integrating symbols into deep learning]]

---------------------------

[img[deep_learning_autoencdoer_thonk_emoji.jpg]]

//More resources//

[[Representational Power of Restricted Boltzmann Machines and Deep Belief Networks|http://www.mitpressjournals.org/doi/abs/10.1162/neco.2008.04-07-510]]
 -- [[Deep Belief Networks Are Compact Universal Approximators|http://www.mitpressjournals.org/doi/abs/10.1162/neco.2010.08-09-1081]]
 -- [ext[Scaling learning algorithms towards AI|http://www.iro.umontreal.ca/~lisa/bib/pub_subject/language/pointeurs/bengio+lecun-chapter2007.pdf]]
 -- [[Hierarchical model of natural images and the origin of scale invariance|http://www.pnas.org/content/110/8/3071.abstract]]
 -- [[Why does Deep Learning work?|https://charlesmartin14.wordpress.com/2015/03/25/why-does-deep-learning-work/]], [[Spin glass]], [[Why Deep Learning Works III: a preview|https://charlesmartin14.wordpress.com/2016/06/20/why-deep-learning-works-iii-a-preview/#comments]]
 -- [[Lagrangian Relaxation for MAP Estimation in Graphical Models|https://arxiv.org/abs/0710.0013]]
 -- [[Models of object recognition|http://www.nature.com/neuro/journal/v3/n11s/full/nn1100_1199.html]]
 -- [[Deep neural networks are easily fooled: High confidence predictions for unrecognizable images|http://www.evolvingai.org/fooling]] [[video|https://www.youtube.com/watch?v=M2IebCN9Ht4]]
 -- https://scholar.google.co.uk/citations?user=iqDZ9WYAAAAJ
 -- [[Accelerated Learning in Layered Neural Networks|http://www.complex-systems.com/pdf/02-6-1.pdf]]
 -- [[Yann LeCun: "Deep Learning, Graphical Models, Energy-Based Models, Structured Prediction, Pt. 1"|https://www.youtube.com/watch?v=oOB4evKlEmQ#t=35m]]
 -- [[Why does Deep Learning work? - A perspective from Group Theory|https://arxiv.org/abs/1412.6621]]

Connections to [[Signal processing]]: https://arxiv.org/abs/1707.00372


-------------------

As someone who is just entering the field of deep learning theory (coming from physics), I find this discussion very interesting. I commend your approach of respecting the truth about the universe. Basically, Feynman's principle of science "If it disagrees with experiment it is wrong. In that simple statement is the key to science. It does not make any difference how beautiful your guess is. It does not make any difference how smart you are, who made the guess, or what his name is â€“ if it disagrees with experiment it is wrong. That is all there is to it."

When one goes deep into theory it's easy to loose track of that empirical grounding. Of course it's perfectly fine to do pure maths. But I do find many theorists who actually wanted to approach some real-world problem (like why deep learning works, or whatever), and ended up just proving a bunch of theorems which don't really say much about the real thing.. I have to keep reminding myself to stay in touch with real applied ML, because I am interested in theory about the real thing!

In any case, I do like a lot one thing that Ali says. He puts emphasis on pedagogy, and the value of simple theorems and experiments for that. At the end of the day, a "proof" 's objective (even mathematicians say so) is to convince someone else of something. It's all about communication. Simple proofs, simple experiments, simple explanations, are not very useful for engineering, on a vacuum. But put those things amongst people, and it fosters collaboration, understanding, etc. To me that is that is the biggest value of theory :)

------------------

!!!__[[Do Deep Nets Really Need to be Deep?|https://arxiv.org/abs/1312.6184]]__

<small>see my annotated version in Kami</small>

The introduction is nice. It basically says "yes, in practice deep nets tend to work better than shallow ones, but why?":

You are given a training set with 1M labeled points. When you train a shallow neural net with one
fully connected feed-forward hidden layer on this data you obtain 86% accuracy on test data. When
you train a deeper neural net as in [1] consisting of a convolutional layer, pooling layer, and three
fully connected feed-forward layers on the same data you obtain 91% accuracy on the same test set.

What is the source of this improvement? Is the 5% increase in accuracy of the deep net over the
shallow net because: 
  a) the deep net has more parameters;
  b) the deep net can learn more complex functions given the same number of parameters;
  c) the deep net has better inductive bias and thus learns more interesting/useful functions (e.g., because the deep net is deeper it learns hierarchical representations [5]);
  d) nets without convolution canâ€™t easily learn what nets with convolution can learn;
  e) current learning algorithms and regularization methods work better with deep architectures than  shallow architectures[8];
  f) all or some of the above;
  g) none of the above?

They basically show that a) and b) are not the case, at least not often. They are basically arguing for reason e) in this paper, by showing that shallow nets can implement the same functions that deep nets learn (in a series of experiments and task they try at least..), however, they just don't find them when trained with the algorithms we have.

If deep nets really do have more simplicity bias (still need to run more experiments on that!), then it could be a reason for them working better in this way.