created: 20160913070239061
creator: cosmos
modified: 20161104101301174
modifier: cosmos
tags: [[Deep learning]]
title: Deep learning theory
type: text/vnd.tiddlywiki

[[Neural network theory]] -- [[Learning theory]]

//Better understanding the shortcomings and potential of deep learning may suggest ways of improving it, both to make it more capable and to make it more robust [2].//

[[Integrating symbols into deep learning]]

----------------------

!!__Deep learning, [[Spin glass]]es, [[Protein folding]], [[Energy landscape]]s__

<div style="display:inline-block; width:350px;">[img width=350 [deep_learning_random_energy_model_glass_transition.png]]</div>
<div style="display:inline-block;width:300px;">

<p>[[why deep learning works -- spin glasses, protein folding, etc.|https://charlesmartin14.wordpress.com/2015/03/25/why-does-deep-learning-work/]]</p>

<big>-->[[Why Deep Learning Works|https://www.youtube.com/watch?v=kIbKHIPbxiU]]</big>

-- [[Improving RBMs with physical chemistry|https://charlesmartin14.wordpress.com/2016/10/21/improving-rbms-with-physical-chemistry/]]

-- [ext[Energy landscapes of deep networks|http://vision.ucla.edu/~pratikac/pub/chaudhari.cs269.16.pdf]]

</div>

|The primary argument about the funnel is that these learning systems are strongly correlated, and therefore not readily treated by mean field theory. Specifically, the classical idea was that strongly correlated random models lie in a different universality class. So they behave completely differently than a spin glass , and this gives rise to the convexity .|

See discussion in comments [[here|https://charlesmartin14.wordpress.com/2016/09/24/mmds-video-presentation/]] and [[here|https://charlesmartin14.wordpress.com/2016/10/21/improving-rbms-with-physical-chemistry/]]. [[Comments on why deep learning works: perspectives from theoretical chemistry]]

[[The Loss Surfaces of Multilayer Networks |http://www.jmlr.org/proceedings/papers/v38/choromanska15.pdf]]

See [[Statistical mechanics of neural networks]]

!!!-->[[Theoretical neuroscience and deep learning theory|http://videolectures.net/deeplearning2016_ganguli_theoretical_neuroscience/]]

---------------------

!!__[[Renormalization group]] and [[Deep learning]]__

[[Why Deep Learning Works II: the Renormalization Group|https://charlesmartin14.wordpress.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/]]

[[An exact mapping between the Variational Renormalization Group and Deep Learning|http://arxiv.org/abs/1410.3831]]

[[WHY DOES UNSUPERVISED DEEP LEARNING WORK?- A PERSPECTIVE FROM GROUP THEORY|http://arxiv.org/pdf/1412.6621v3.pdf]]

[[Supervised Learning with Quantum-Inspired Tensor Networks|https://arxiv.org/abs/1605.05775]]

[[Deep learning and the renormalization group|http://128.84.21.199/pdf/1301.3124v1.pdf]]

<small>See also comments on [[Why does deep and cheap learning work so well?]]</small>

-----------------------

!!__Simplicity and deep learning__

See also [[Simplicity and learning]]

!!![[Why does deep and cheap learning work so well?]]

----------------------------

!!__[[Neuroscience]] and deep learning__

!!!-->[[Theoretical neuroscience and deep learning theory|http://videolectures.net/deeplearning2016_ganguli_theoretical_neuroscience/]]

---------------------------

//More resources//

[[Representational Power of Restricted Boltzmann Machines and Deep Belief Networks|http://www.mitpressjournals.org/doi/abs/10.1162/neco.2008.04-07-510]]
 -- [[Deep Belief Networks Are Compact Universal Approximators|http://www.mitpressjournals.org/doi/abs/10.1162/neco.2010.08-09-1081]]
 -- [ext[Scaling learning algorithms towards AI|http://www.iro.umontreal.ca/~lisa/bib/pub_subject/language/pointeurs/bengio+lecun-chapter2007.pdf]]
 -- [[Hierarchical model of natural images and the origin of scale invariance|http://www.pnas.org/content/110/8/3071.abstract]]
 -- [[Why does Deep Learning work?|https://charlesmartin14.wordpress.com/2015/03/25/why-does-deep-learning-work/]], [[Spin glass]], [[Why Deep Learning Works III: a preview|https://charlesmartin14.wordpress.com/2016/06/20/why-deep-learning-works-iii-a-preview/#comments]]
 -- [[Lagrangian Relaxation for MAP Estimation in Graphical Models|https://arxiv.org/abs/0710.0013]]
 -- [[Models of object recognition|http://www.nature.com/neuro/journal/v3/n11s/full/nn1100_1199.html]]
 -- [[Deep neural networks are easily fooled: High confidence predictions for unrecognizable images|http://www.evolvingai.org/fooling]] [[video|https://www.youtube.com/watch?v=M2IebCN9Ht4]]
 -- https://scholar.google.co.uk/citations?user=iqDZ9WYAAAAJ
 -- [[Accelerated Learning in Layered Neural Networks|http://www.complex-systems.com/pdf/02-6-1.pdf]]
 -- [[Yann LeCun: "Deep Learning, Graphical Models, Energy-Based Models, Structured Prediction, Pt. 1"|https://www.youtube.com/watch?v=oOB4evKlEmQ#t=35m]]
 -- [[Why does Deep Learning work? - A perspective from Group Theory|https://arxiv.org/abs/1412.6621]]