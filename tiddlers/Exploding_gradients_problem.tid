created: 20180427161738016
creator: cosmos
modified: 20180427163823226
modifier: cosmos
tags: [[Deep learning theory]]
title: Exploding gradients problem
tmap.id: c6075ff4-bc7f-4e2b-927a-5d8249895b46
type: text/vnd.tiddlywiki

[[video|https://www.youtube.com/watch?v=5v41xjKauY0]]

# For some types of nonlinearities/layers, the gradients w.r.t. to parameters at different layers increase with the number of layers (it increases as we go from output to input). ([[See visualization of decision surfaces on parameter space for fixed input|https://youtu.be/5v41xjKauY0?t=21m51s]]

# To avoid pseudorandom walk due to the decision surface from lower layers, we have to choose a smaller learning rate, as the we add more layers (see [[here|https://youtu.be/5v41xjKauY0?t=28m8s]])

# For any network, we can do the [[residual trick|https://youtu.be/5v41xjKauY0?t=31m59s]] to convert to a residual network. [[The size of the residuals is then found to be small when the learning rate is small|https://youtu.be/5v41xjKauY0?t=34m18s]]

# By using the [[residual trick|https://youtu.be/5v41xjKauY0?t=31m59s]], and then doing [[a Taylor expansion|https://youtu.be/5v41xjKauY0?t=37m42s]] (where we assume residual terms are small), we can reduce the number of layers, while still approximating well, giving a notion of ''effective depth'' (note: depending on for how many layers we do the Taylor expansion, we get different number of layers in the approximate network). [[The effective depth decreases if the residuals are smaller|https://youtu.be/5v41xjKauY0?t=38m42s]].

# [[Therefore error is large|https://youtu.be/5v41xjKauY0?t=39m5s]]. Although for the nets with exploding gradients (and thus small residuals), the depth of the approximate nets is small, they have a lot of layers of nonliearities (with fixed parameters, that's why they are not counted for in the effective depth). These __needlessly randomize the features__, they argue