created: 20170512224700922
creator: cosmos
modified: 20181001173928481
modifier: cosmos
tags: [[Deep learning theory]] Generalization
title: Generalization in deep learning
tmap.id: 3ae55414-1998-44f0-8fb4-9f318db105f7
type: text/vnd.tiddlywiki


[[Deep learning generalizes because the parameter-function map is biased towards simple functions|https://arxiv.org/abs/1805.08522]]

See gingkoapp tree: [[Kolmogorov complexity and generalization in deen neural networks|https://gingkoapp.com/app#7abe722f5a31aa3e1000001b]]

--> Even for [[Kernel method]]s, the classical learning theory doesn't predict what we observe (-- [[see here|https://youtu.be/T4T63TT-Hy4?list=PL04QVxpjcnjhtL3IIVyFRMOgdhWtPn7YJ&t=21m44s]]). [[Overparametrization in deep learning|https://youtu.be/T4T63TT-Hy4?list=PL04QVxpjcnjhtL3IIVyFRMOgdhWtPn7YJ&t=53m10s]]

<small>[[Current state of generalization theory for deep learning (2018)|https://youtu.be/KDRN-FyyqK0?t=52m22s]])</small>

https://www.offconvex.org/2018/02/17/generalization2/ -- [[Stronger generalization bounds for deep nets via a compression approach|https://arxiv.org/abs/1802.05296]]. See stability analysis from "Threshold logic and its applications book"

[ext[High-dimensional dynamics of generalization error in neural networks|http://www.people.fas.harvard.edu/~asaxe/papers/Advani,%20Saxe%20-%202017%20-%20High-dimensional%20dynamics%20of%20generalization%20error%20in%20neural%20networks.pdf]] Using random matrix theory and exact so-
lutions in deep linear models, we derive the generalization error and training error dynamics of
learning and analyze how they depend on the dimensionality of data and signal to noise
ratio of the learning problem.  

We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks.

Overtraining is worst at intermediate network sizes. For both linear and nonlinear nets, catastrophic overtraining is a symp- tom of a model whose complexity is exactly matched to the size of the training set, and can be combated either by making the model smaller or larger .

We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models:

* first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; [[Singular learning theory]], [[Sloppy model]]s. These subspaces, are spaces are the ''neutral spaces'' of the parameter--function GP map!

* and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining

 derive a generalization error bound
which incorporates the frozen subspace and conditioning effects and qualitatively matches
the behavior observed in simulation.

the optimal stopping time (for minimizing generalization error) in these
systems growths with the signal to noise ratio (SNR), and we provide arguments that this
growth is approximately logarithmic.  When the quality of the data is higher (high SNR),
we expect the algorithm to weigh the data more heavily and thus run gradient descent for
longer. 

"while a large
deep network can indeed fit random labels, gradient-trained DNNs initialized with small-
norm weights learn simpler functions first and hence generalize well if there is a consistent
rule to be learned."

Remarkably,  even  without  early  stopping,  generalization  can  still  be  better  in  larger
networks.  To understand this, we derive an alternative bound on the Rademacher complex-
ity of a two layer non-linear neural network with fixed first layer weights that incorporates
the dynamics of gradient descent.  We show that complexity is limited because of a frozen
subspace in which no learning occurs, and overtraining is prevented by a larger gap in the
eigenspectrum of the data in the hidden layer in overcomplete models 

[[A Closer Look at Memorization in Deep Networks|https://arxiv.org/abs/1706.05394]]

See emails with Ard

----------------

!!![[Implicit regularization in deep learning|https://arxiv.org/pdf/1709.01953.pdf]]

Regularization caused by optimization algorithm.. "we investigate the transformations under which the function computed by a network remains the same and therefore argue for complexity measures and optimization algorithms that have similar invariances. We find complexity measures that have similar invariances to neural networks and optimization algorithms that implicitly regularize them". [[Path-norm]] a metric in parameter space, that is closer to model distance? [[Sloppy systems]]?

* We prove generalization bounds for the class of fully connected feedforward networks with the bounded norm. We further show that for some norms, this bound is independent of the number of hidden units.

* We show how PAC-Bayes framework can be employed to obtain generalization bounds for neural networks by making a connection between sharpness and PAC-Bayes theory.

* Implicit Regularization by SGD (Chapter 6): We show that networks learned by SGD satisfy several conditions that lead to flat minima. (Hmm, doesn't seem to be in chapter 6)

[28] proved that the Rademacher complexity of fully connected feedforward networks on set S can be bounded based on the 1 norm of the weights of hidden units in each layer  In [[Chapter 5|https://arxiv.org/pdf/1709.01953.pdf#page=31]] we show
how the capacity can be controlled for a large family of norms. See [[Learning real-valued functions]]

[[Algorithmic robustness]], similar to [[Algorithmic stability]]

<small>[[Sharpness|https://arxiv.org/pdf/1709.01953.pdf#page=22]]. we advocate viewing a related notion of expected sharpness in the context of the PAC-Bayesian framework. Viewed this way, it becomes clear that sharpness controls only one of two relevant terms, and must be balanced with some other measure such as norm. Together, sharpness and norm do provide capacity control and can explain many of the observed phenomena. This connection between sharpness and the PAC-Bayes framework was also recently noted by Dziugaite and Roy [32].</small>

<small>[[On the Role of Implicit Regularization in Generalization|https://arxiv.org/pdf/1709.01953.pdf#page=25]] we draw an analogy to matrix
factorization and understand dimensionality versus norm control there. Based on this analogy we suggest that implicit norm regularization might be central also for deep learning, and also there we should think of bounded-norm models with capacity independent of number of hidden units. See also [[Deep learning theory]]</small>

Focusing on networks with RELU activations in this section, we observe that scaling down the incoming
edges to a hidden unit and scaling up the outgoing edges by the same factor yields an equivalent network
computing the same function. Since predictions are invariant to such rescalings, it is natural to seek a
geometry, and corresponding optimization method, that is similarly invariant. In this chapter, we study
invariances in feedforward networks with shared weights. --- [[Sloppy systems]]!

<small>Revisiting the choice of gradient descent, we recall that optimization is also inherently tied to a choice
of geometry or measure of distance, norm or divergence. Gradient descent for example is tied to the l2
norm as it is the steepest descent with respect to l2 norm in the parameter space, while coordinate descent
corresponds to steepest descent with respect to the l1 norm and exp-gradient (''multiplicative weight'')
updates is tied to an entropic divergence.</small>

[[Sensitivity]]-based bounds

----------------

[[Generalization in deep learning|https://arxiv.org/pdf/1710.05468.pdf]] (Bengio et al) -- en general, no me parecio tan interesante, excepto un bound de [[Rademacher complexity]], q tampoco es q le permita decir mucho per bueno. val just tells us that we generalize. It doesn't tells us why. Just like PAC bounds don't tells us why the target function lies within our test set, for e.g.. 

[[On Generalization and Regularization in Deep Learning|https://arxiv.org/abs/1704.01312]]

[[FLAT MINIMA|http://www.bioinf.jku.at/publications/older/3304.pdf]]

[[Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data|https://arxiv.org/abs/1703.11008]]

[[Unreasonable Effectiveness of Learning Neural Nets: Accessible States and Robust Ensembles|https://pdfs.semanticscholar.org/a13e/ab6052cc9f85054d70d3ba395b0d77652172.pdf]]

[[Sharp Minima Can Generalize For Deep Nets|https://arxiv.org/abs/1703.04933]] -- maybe it's not sharp, but ''frequent''! ([[Arrival of the frequent]])

[[Data-Dependent Stability of Stochastic Gradient Descent|https://arxiv.org/abs/1703.01678]]

[[Train faster, generalize better: Stability of stochastic gradient descent|https://arxiv.org/abs/1509.01240]]

[[What size network is good for generalization of a specific task of interest|http://www.sciencedirect.com/science/article/pii/0893608094900264]] -- We show that for some tasks increasing network size leads to worse generalization. This is not surprising. The striking feature is that there exist other tasks for which increasing network size improves generalization. We give an explanation of this phenomenon in terms of the information entropy. I think what this paper is missing is the concept of universal complexity measures. You can see that tasks of “medium complexity” are the hardest to learn, because their measure of complexity isn’t very good. Even just entropy, would be better (as highest entropy corresponds to medium complexity in their case)

[[related paper|http://www.inderscienceonline.com/doi/abs/10.1504/IJAACS.2014.065198]], [[pdf|http://sci-hub.cc/10.1504/ijaacs.2014.065198]]

----------------------

!!!__Franco's [[Generalization complexity]]__

[ext[Generalization ability of Boolean functions implemented in feedforward neural networks|http://www.lcc.uma.es/~lfranco/Franco-complex06.pdf]]


-------------------

From [[Understanding deep learning requires rethinking generalization|https://arxiv.org/abs/1611.03530]]

"Moreover, we did not observe any overfitting: the generalization error does not degrade by reaching zero training error, or by using larger networks."

"what we really want is to minimize the variance of the net functions induced by weights near the actual weight vector."

----------------

Flat minima good to help fight catastrophic forgetting.

------------------

!!!__[[Simplicity bias]] in neural networks__

See [[this gingko tree|https://gingkoapp.com/app#7abe722f5a31aa3e1000001b]] and [[this overleaf document|https://www.overleaf.com/9939721prrtpqvjmdxd#/36478572/]] (from my short project in summer 2017 with [[Ard Louis]]