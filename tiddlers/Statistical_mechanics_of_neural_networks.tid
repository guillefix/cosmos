created: 20160915182124591
creator: cosmos
modified: 20181108185702145
modifier: cosmos
tags: [[Artificial neural network]] [[Statistical physics and inference]]
title: Statistical mechanics of neural networks
tmap.id: 7ec059ec-3460-4def-99a5-4a337533c757
type: text/vnd.tiddlywiki

See also [[Neural network theory]] (mostly about [[Artificial neural network]] [[Learning theory]]), [[Deep learning theory]], [[Hopfield network]], [[Spin glass]]

[[Thouless-Anderson-Palmer equations for neural networks|http://journals.aps.org/pre/abstract/10.1103/PhysRevE.61.1839]]

[ext[Nonequilibrium analysis of simple neural nets|http://pcteserver.mi.infn.it/~caraccio/Lauree/Amelio.pdf]]

!!![[Sompolinsky II Beg Rohu 2018|https://www.youtube.com/watch?v=c_ez2O2QnCM]]

[[High-dimensional dynamics of generalization error in neural networks|https://arxiv.org/abs/1710.03667]] Generalization error diverges when number of examples equals number of dimensions, for target function with noise 

To understand phenomenon described by Saxe and in the video at 43:00, we can think of this:
Low eigenvalues in XX^T correspond to directions with little variations in the input. However, by the random fluctuation eta, the output could have an O(1) variation, even for arbitrarily small input variation, which requires a large weight to fit, and produces large generalization error.

for $\alpha<1$ the probability of this directions in input space with low variation decreases, as we get less directions overall with points I think (directions with no points/variation are ignored for the algorithm which projects weight into input subspace, and these are the 0 eigenvalue parts of the Marchenko-Pastur distr)

!!__Neural networks and [[Spin glass]]es__

See [[Deep learning theory]] and [[Hopfield network]]

See more at Neural Networks: An Introduction [2 ed.] by Muller et al.

The ferromagnetic case corresponds to a neural network that has stored
a single pattern, and has no [[Frustration]]. The network which has been loaded with a large number of
randomly composed patterns resembles a spin glass.


[[The Loss Surfaces of Multilayer Networks |http://www.jmlr.org/proceedings/papers/v38/choromanska15.pdf]]

[[How glassy are neural networks?|http://ezproxy-prd.bodleian.ox.ac.uk:2839/article/10.1088/1742-5468/2012/07/P07009/meta;jsessionid=B582D39D6A769C5181263198B74EA025.c2.iopscience.cld.iop.org]]

[[Spin-glass models of neural networks|http://journals.aps.org/pra/abstract/10.1103/PhysRevA.32.1007]]

[[Temperature based RBM]]s

[[Statistical Mechanics of Neural Networks near Saturation |http://sci-hub.cc/10.1016/0003-4916(87)90092-3]]

[[Statistical mechanics of learning|https://www.youtube.com/watch?v=hfBHELbk2Yw]] 

[ext[Learning to generalize|http://www.ki.tu-berlin.de/fileadmin/fg135/publikationen/opper/Op01.pdf]]

[[Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior|https://openreview.net/forum?id=SkPoRg10b]]

----------------

There is a hierarchical fragmentation of the state space as more local minima appear in the free energy as one decreases the temperature in a [[Spin glass]]

[img[spin_glass_free_energy_continuous_phase_transition.png]]

This seems related qualitatively to how the sublevel sets become more irregular as one decreases the energy level for neural network loss landscapes, see [[Topology and Geometry of Deep Rectified Network Optimization Landscapes|https://pdfs.semanticscholar.org/76fe/7cce8f8d28e35536f53b41308eb5e074f5e3.pdf]]

----------------

[[Video|https://www.youtube.com/watch?v=cjj9ihYJkOQ#t=1m30s]]

https://www.youtube.com/watch?v=fZ8WCic5u2I&list=PLYq7WW565SZgC_0OqyRjTcWKH3odod3j5

__Asynchronous irregular firing__

See also [[Neuronal network]]