created: 20181130161711022
creator: cosmos
modified: 20190321040929859
modifier: cosmos
tags: [[Bayesian inference]]
title: Amortized inference
type: text/vnd.tiddlywiki

''Amortized inference'' refers to methods which [[learn|Learning]] [[Posterior]] distributions, for a given [[Generative model]] (which may be known but very complicated, or learnt simultaneously as the posterior, as in [[VAE|Variational autoencoder]]s).

For a generative model $$p(\mathbf{z},\mathbf{x})$$, one can use several methods of [[Approximate Bayesian inference]] (ABI) to estimate $$p(\mathbf{z}|\mathbf{x}_1)$$ for certain data $$\mathbf{x}_1$$. One could then apply these methods again to estimate or compute $$p(\mathbf{z}|\mathbf{x}_2)$$ for different data $$\mathbf{x}_2$$. This would be very computationally intensive if one needs to do inference on many data $$\mathbf{x}_i$$. In practice, one may expect that the distribution $$p(\mathbf{z}|\mathbf{x}_1)$$ has some information about $$p(\mathbf{z}|\mathbf{x}_2)$$, so that one can instead learn a function $$q(\mathbf{z}|\mathbf{x})$$ from the (approximated) true posterior $$p(\mathbf{z}|\mathbf{x}_i)$$ at some data points $$\mathbf{x}_i$$, and hope that then for future test points $$\mathbf{x}$$, $$q(\mathbf{z}|\mathbf{x})$$ would approximate the true posterior well (an instance of [[Generalization]]). Because evaluating $$q(\mathbf{z}|\mathbf{x})$$ may be much quicker than computing the true posterior at $$\mathbf{x}$$ using some of the approximate inference methods, the average time for evaluating (or sampling, etc) the posterior over many test points will be greatly reduced! We have thus (potentially) increased the computation cost at some points, so that the total/average cost may be reduced (i.e. [[Amortization]])

The problem is very related and algorithmically equivalent to [[Supervised learning]]. The difference is that in supervised learning, we assume the generative model isn't known, so that it may not be possible to compute or estimate the posterior at test points, so that one learns $$q(\mathbf{z}|\mathbf{x})$$ not out a desire to be more computationally efficient, but because there is no other way to estimate the posterior in the test points, but to rely on [[Generalization]].


[[Faithful Inversion of Generative Models for Effective Amortized Inference|https://arxiv.org/pdf/1712.00287.pdf]]