created: 20170426172534994
creator: cosmos
modified: 20171130202638988
modifier: cosmos
tags: [[Kernel method]]
title: Kernel trick
tmap.id: 98b54bb6-acbd-4b51-8bbd-2cf81845f822
type: text/vnd.tiddlywiki

See [[Kernel method]], and [[here|https://youtu.be/Vm5QE54y6mw?t=51m59s]]

The idea of generalizing [[Dictionary learning]] to learning over [[Reproducing kernel Hilbert space]], where function evaluation can be substituted with an inner product.  Operationally, one often just has to substitute inner products between input vectors with a Kernel function evaluated at these input vectors, and one gets a new learning algorithm. This learning algorithm actually can now have an infinite dimensional hypothesis class (refering to dimension of [[Hilbert space]] of functions), while it was finite dimensional for dictionary learning.

See a derivation here: https://arxiv.org/pdf/1611.03530.pdf , and original reference


Note that this kernel solution has an appealing interpretation in terms of implicit regularization.
Simple algebra reveals that it is equivalent to the minimum l2-norm solution of Xw = y. That is,
out of all models that exactly fit the data, SGD will often converge to the solution with minimum
norm.

[[A generalized representer theorem|https://pdfs.semanticscholar.org/17d2/f027221d60cda373ecf15b03706c9e60269b.pdf]]