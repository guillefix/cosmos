created: 20160709020641282
creator: guillefix
modified: 20161109180910283
modifier: cosmos
title: Support vector machine
tmap.id: de46d9cf-ac43-41a3-9643-77b35dc44df8
type: text/vnd.tiddlywiki

//aka SVM//

A method for discriminative [[Supervised learning]], that is for [[Classification]], and [[Regression analysis]]. It allows to [[learn|Machine learning]] non-linear functions, like [[Artificial neural network]]s.

[[Andrew Ng lec intro|https://www.youtube.com/watch?v=qyyJKd-zXRE&index=6&list=PLA89DCFA6ADACE599#t=44m05s]] -- [[Notes|http://cs229.stanford.edu/notes/cs229-notes3.pdf]] -- 
[[Wiki|https://www.wikiwand.com/en/Support_vector_machine]]
 -- [[Notation|https://www.youtube.com/watch?v=qyyJKd-zXRE&index=6&list=PLA89DCFA6ADACE599#t=51m15s]]

[[Support Vector Machine Intro and Application - Practical Machine Learning Tutorial with Python p.20|https://www.youtube.com/watch?v=mA5nwGoRAOo&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&index=20]]

See page 39 [[here|http://mpawankumar.info/teaching/cdt-optimization/lecture1_2.pdf]], and [[here|http://www.cs.ox.ac.uk/people/varun.kanade/teaching/ML-MT2016/slides/slides09.pdf]], and [[these notes|http://www.cs.ox.ac.uk/people/varun.kanade/teaching/ML-MT2016/lectures/lecture09.pdf]]

!!__Maximum/Optimal margin classifier__

[[Max-margin learning]]

[[Summary|https://www.youtube.com/watch?v=s8B4A5ubw6c&index=7&list=PLA89DCFA6ADACE599]]

* [[Functional margin]]

* [[Geometric margin]]

[[Definition|https://www.youtube.com/watch?v=qyyJKd-zXRE&index=6&list=PLA89DCFA6ADACE599#t=1h8m40s]] of the optimization (learning) problem: choose classification hyperplane (parametrized by $$\omega, b$$), that maximizes the [[Geometric margin]] wrt to training data set, with the contratint $$||w||=1$$.

[[Alternative formulationt|https://www.youtube.com/watch?v=s8B4A5ubw6c&index=7&list=PLA89DCFA6ADACE599#t=9m45s]], that however, leaves $$||w||$$ arbitrary.

[[Here is a constraint|https://www.youtube.com/watch?v=s8B4A5ubw6c&index=7&list=PLA89DCFA6ADACE599#t=17m]] that fixes the arbitrariness of the magnitude of $$w$$: $$\hat{\gamma} = 1$$ ([[Functional margin]]), we get the following [[optimization problem|https://www.youtube.com/watch?v=s8B4A5ubw6c&index=7&list=PLA89DCFA6ADACE599#t=18m30s]]:

<<<
$$\min \frac{1}{2} ||w||^2$$

s.t. $$y^{(i)} (w^T x + b ) \geq 1$$
<<<

This is an example of a [[quadratic program|Quadratic programming]]

This is the same as maximizing the margin, under the constraint See page 8 [[here|http://www.cs.ox.ac.uk/people/varun.kanade/teaching/ML-MT2016/slides/slides09.pdf]], and divide consrtaints by norm of $$w$$, giving the geometric margin.

!!!__SVM optimization problem__

<b>An SVM works by solving the [[dual problem to the optimization problem|Optimization]] of OMCs defined above</b>. The good thing is that the resulting optimization problem is [[convex|Convex optimization]], for which there are very efficient methods for solving it, as there are no local minima!

Primal and dual [[Optimization]] problems: [[intro|https://www.youtube.com/watch?v=s8B4A5ubw6c&index=7&list=PLA89DCFA6ADACE599#t=48m05s]] --> [[primal problem|https://www.youtube.com/watch?v=s8B4A5ubw6c&index=7&list=PLA89DCFA6ADACE599#t=49m15s]] --> [[Dual problem|https://www.youtube.com/watch?v=s8B4A5ubw6c&index=7&list=PLA89DCFA6ADACE599#t=56m]] ([[Skip some algebra|https://www.youtube.com/watch?v=s8B4A5ubw6c&index=7&list=PLA89DCFA6ADACE599#t=1h02m30s]])

__Support vectors__

([[Definition of support vectors|https://www.youtube.com/watch?v=s8B4A5ubw6c&index=7&list=PLA89DCFA6ADACE599#t=53m50s]], the $$\alpha$$s [[are Lagrange multipliers|https://www.youtube.com/watch?v=s8B4A5ubw6c&index=7&list=PLA89DCFA6ADACE599#t=28m20s]])

Support vectors satisfy the contraint inequality with equality.

The approach for deriving the optimal margin classifier for a support vector machine, is that we'll solve the dual optimization problem above.

Prediction can be written down using inner products: see [[here|https://www.youtube.com/watch?v=s8B4A5ubw6c&index=7&list=PLA89DCFA6ADACE599#t=1h11m]]

--[[Summary|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=1m]]

!!__Kernels__

,,[[Kernels Introduction - Practical Machine Learning Tutorial with Python p.29|https://www.youtube.com/watch?v=9IfT8KXX_9c&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&index=29]],,

Allow the computation of the inner products, that appear in the formulas for SVMs, more effectively

[[Idea of a Kernel|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=4m]]

[[How to choose Kernels|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=14m30s]]

__[[How to check if a function is a valid Kernel|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=17m30s]]__

If $$K(x,z)$$ is a valid kernel, then the matrix with elements $$K_{ij} = K(x^{(i)}, x^{(j)}$$, for any set of $$x^{(i)}$$,  must be symmetric positive semi-definite. It turns out [[the converse is true|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=24m15s]].

[[How to use a SVM with a kernel|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=27m30s]]

!!![[How SVM work (to classify non-linearly separable data)|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=30m]]

__Examples of kernels__

* Gaussian kernel

!!![[Kernels can be applied to other learning algorithms|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=34m30s]]!

!!__$$L1$$ norm soft margin SVM__

[[Soft Margin SVM - Practical Machine Learning Tutorial with Python p.31|https://www.youtube.com/watch?v=JHaqodAQqiI&index=31&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v]]

Soft margin is a method to <b>deal with non-linearly separable decision boundaries</b>. 

[[Intro|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=35m40s]]

[[Mathematical formulation|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=37m40s]]

<<<
$$\min \frac{1}{2} ||w||^2 + C \sum_{i=1}^n \xi_i$$

s.t. $$y^{(i)} (w^T x + b ) \geq 1 - \xi_i$$

$$y^{(i)} (w^T x + b ) \geq 1 - \xi$$

$$\xi_i  \geq 0$$

$$i = 1, ..., n$$
<<<

basically weight how each point contributes, so that if there is some outlier it doesn't break the algorithm.. [[Explanation|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=39m]]

We need $$\xi_i  \geq 0$$ so that we don't reward easy to classify terms, with negative $$\xi_i$$. 

|This constrained optimization problem can be reformulated as an unconstrained optimization problem with a regularized [[Hinge loss]] function: $$\frac{1}{2}||w||^2 + C \sum_i \max{\{0,1-y_i(\mathbf{w}^T \mathbf{x}_i)\}}$$|

To see this we have to minimize w.r.t $$\xi$$ first, and then w.r.t $$\mathbf{w}$$ (which can always be done). So we consider fixing the parameters $$w$$, and minimizing w.r.t $$\xi$$, which gives $$\xi=\max{\{0,1-y_i(\mathbf{w}^T \mathbf{x}_i)\}}$$, which gives the [[Hing loss]].

Can also work out [[the dual of this optimization problem|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=39m50s]] (see more on [[Optimization]] and [[Linear programming]] about dual optimization problems). See [[slides here|http://www.cs.ox.ac.uk/people/varun.kanade/teaching/ML-MT2016/slides/slides09.pdf]]

[[Convergence conditions|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=43m23s]]

!!__SMO algorithm__

''Sequential minimal optimization'' is an optimization algorithm, that is a variation of [[Coordinate descent]]: [[Application to SVM dual optimization problem|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=52m48s]]. [[This is the reason we use it|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=51m]]

-- [[Description of the algorithm|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=53m40s]] -- [[algorithm|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=54m15s]] (we are maximizing a function $$w(\alpha_1, ..., \alpha_m)$$

!!__[[Applications of SVMs|https://www.youtube.com/watch?v=bUv9bfMPMb4&index=8&list=PLA89DCFA6ADACE599#t=1h9m25s]]__