<p>&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Decision-time%20planning&quot;&gt;Decision-time planning&lt;/a&gt; where the value of possible actions is estimated by &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Monte%20Carlo&quot;&gt;Monte Carlo&lt;/a&gt; sampling of future trajectories, for each of which we compute the cumulative reward (see &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Reinforcement%20learning&quot;&gt;Reinforcement learning&lt;/a&gt;), and averaging. We then often choose the most rewarding action (but with some probability we may not &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Exploration-exploitation%20trade-off&quot;&gt;to allow some exploration&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Monte%20Carlo%20tree%20search&quot;&gt;Monte Carlo tree search&lt;/a&gt; is an example of this.&lt;/p&gt;</p>