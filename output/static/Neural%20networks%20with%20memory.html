<p><em>aka <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Memory">Memory</a>-augmented neural networks</em></p><p><strong>Memory</strong> is good for recognizing time sequence data.</p><p><strong><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=jCGplSKrl2Y&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;index=11#t=48m40s" rel="noopener noreferrer" target="_blank">Memory networks</a></strong>. Apply max-margin. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=jCGplSKrl2Y&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;index=11#t=51m52s" rel="noopener noreferrer" target="_blank">Actual drescription</a>. <a class="tc-tiddlylink-external" href="http://arxiv.org/pdf/1410.3916v11.pdf" rel="noopener noreferrer" target="_blank">Paper</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=jCGplSKrl2Y&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;index=11#t=51m52s" rel="noopener noreferrer" target="_blank">Time constraints for facts</a></p><p><strong><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=56TYLaQN4N8&amp;index=12&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=7m48s" rel="noopener noreferrer" target="_blank">Recurrent neural nets</a></strong>. Vanishing gradient problem, naively, RNNs don't give you long term memory.. <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Recurrent%20neural%20network">RNNs</a></p><p><strong>Long Short-Term Memory</strong> (<a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Long%20short-term%20memory">LSTM</a>) was introduced to solve this problem. </p><p>See also <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Content-addressable%20memory">Content-addressable memory</a></p><p><a class="tc-tiddlylink-external" href="https://www.semanticscholar.org/paper/Associative-Long-Short-Term-Memory-Danihelka-Wayne/3ed48ecf9e70a513247e3a710ede484e4114c2f0" rel="noopener noreferrer" target="_blank">Associative Long Short-Term Memory</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1606.02355" rel="noopener noreferrer" target="_blank">Active Long Term Memory Networks</a></p><p><a class="tc-tiddlylink-external" href="http://distill.pub/2016/augmented-rnns/" rel="noopener noreferrer" target="_blank">Attention and Augmented Recurrent Neural Networks</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Neural%20Turing%20machine">Neural Turing machine</a>s</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Integrating%20symbols%20into%20deep%20learning">Integrating symbols into deep learning</a></p>