<p>&lt;p&gt;&lt;em&gt;aka &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Memory&quot;&gt;Memory&lt;/a&gt;-augmented neural networks&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Memory&lt;/strong&gt; is good for recognizing time sequence data.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=jCGplSKrl2Y&amp;amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;amp;index=11#t=48m40s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Memory networks&lt;/a&gt;&lt;/strong&gt;. Apply max-margin. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=jCGplSKrl2Y&amp;amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;amp;index=11#t=51m52s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Actual drescription&lt;/a&gt;. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://arxiv.org/pdf/1410.3916v11.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=jCGplSKrl2Y&amp;amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;amp;index=11#t=51m52s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Time constraints for facts&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=56TYLaQN4N8&amp;amp;index=12&amp;amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=7m48s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Recurrent neural nets&lt;/a&gt;&lt;/strong&gt;. Vanishing gradient problem, naively, RNNs don't give you long term memory.. &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Recurrent%20neural%20network&quot;&gt;RNNs&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Long Short-Term Memory&lt;/strong&gt; (&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Long%20short-term%20memory&quot;&gt;LSTM&lt;/a&gt;) was introduced to solve this problem. &lt;/p&gt;&lt;p&gt;See also &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Content-addressable%20memory&quot;&gt;Content-addressable memory&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.semanticscholar.org/paper/Associative-Long-Short-Term-Memory-Danihelka-Wayne/3ed48ecf9e70a513247e3a710ede484e4114c2f0&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Associative Long Short-Term Memory&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1606.02355&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Active Long Term Memory Networks&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://distill.pub/2016/augmented-rnns/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Attention and Augmented Recurrent Neural Networks&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Neural%20Turing%20machine&quot;&gt;Neural Turing machine&lt;/a&gt;s&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Integrating%20symbols%20into%20deep%20learning&quot;&gt;Integrating symbols into deep learning&lt;/a&gt;&lt;/p&gt;</p>