<p><code>&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=utf-8&quot; /&gt;
&lt;meta name=&quot;generator&quot; content=&quot;TiddlyWiki&quot; /&gt;
&lt;meta name=&quot;tiddlywiki-version&quot; content=&quot;</code>5.1.17<code>&quot; /&gt;
&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot; /&gt;
&lt;meta name=&quot;apple-mobile-web-app-capable&quot; content=&quot;yes&quot; /&gt;
&lt;meta name=&quot;apple-mobile-web-app-status-bar-style&quot; content=&quot;black-translucent&quot; /&gt;
&lt;meta name=&quot;mobile-web-app-capable&quot; content=&quot;yes&quot;/&gt;
&lt;meta name=&quot;format-detection&quot; content=&quot;telephone=no&quot;&gt;
&lt;link id=&quot;faviconLink&quot; rel=&quot;shortcut icon&quot; href=&quot;favicon.ico&quot;&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;static.css&quot;&gt;
&lt;title&gt;</code>Manifold learning: Cosmos — Everything there was, there is, <span class="subtitle-dark">and there will be</span><code>&lt;/title&gt;
&lt;/head&gt;
&lt;body class=&quot;tc-body&quot;&gt;
</code><code>
&lt;section class=&quot;tc-story-river&quot;&gt;
</code>
&lt;p&gt;&lt;div class=&quot;tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Dimensionality%20reduction &quot; data-tags=&quot;[[Dimensionality reduction]]&quot; data-tiddler-title=&quot;Manifold learning&quot;&gt;&lt;div class=&quot;tc-tiddler-title&quot;&gt;
&lt;div class=&quot;tc-titlebar&quot;&gt;
&lt;span class=&quot;tc-tiddler-controls&quot;&gt;
&lt;span class=&quot; tc-reveal&quot;&gt;&lt;button aria-label=&quot;more&quot; class=&quot;tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions&quot; title=&quot;More actions&quot;&gt;&lt;/button&gt;&lt;div class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/div&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot;&gt;&lt;button aria-label=&quot;edit&quot; class=&quot;tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit&quot; title=&quot;Edit this tiddler&quot;&gt;&lt;/button&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot;&gt;&lt;button aria-label=&quot;close&quot; class=&quot;tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose&quot; title=&quot;Close this tiddler&quot;&gt;&lt;/button&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot;&gt;&lt;button aria-label=&quot;tiddlymap&quot; class=&quot;tc-btn-invisible tc-btn-%24%3A%2Fplugins%2Ffelixhayashi%2Ftiddlymap%2Fmisc%2FquickConnectButton &quot; title=&quot;Toggle TiddlyMap actions&quot;&gt;


&lt;/button&gt;&lt;/span&gt;
&lt;/span&gt;

&lt;span&gt;

&lt;span class=&quot;tc-tiddler-title-icon&quot; style=&quot;fill:;&quot;&gt;

&lt;/span&gt;



&lt;h2 class=&quot;tc-title&quot;&gt;
Manifold learning
&lt;/h2&gt;

&lt;/span&gt;

&lt;/div&gt;

&lt;div class=&quot;tc-tiddler-info tc-popup-handle tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/div&gt;
&lt;/div&gt;&lt;div class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/div&gt;
&lt;div class=&quot; tc-reveal&quot;&gt;
&lt;div class=&quot;tc-subtitle&quot;&gt;
&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;cosmos.html&quot;&gt;
cosmos
&lt;/a&gt; 29th November 2017 at 6:32am
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot; tc-reveal&quot;&gt;
&lt;div class=&quot;tc-tags-wrapper&quot;&gt;&lt;span class=&quot;tc-tag-list-item&quot;&gt;


&lt;span class=&quot;tc-tag-label tc-btn-invisible&quot; draggable=&quot;true&quot; style=&quot;background-color:;
fill:#333333;
color:#333333;&quot;&gt;
 Dimensionality reduction
&lt;/span&gt;

&lt;span class=&quot;tc-drop-down tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;

&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;tc-tiddler-body tc-reveal&quot;&gt;&lt;p&gt;&lt;em&gt;aka nonlinear dimensionality reduction&lt;/em&gt;&lt;/p&gt;&lt;p&gt;nonlinear dimensionality reduction consist of two steps: first, they start with con - structing a representation of local affinity of the data points (typically, a sparsely connected graph). Second, the data points are embedded into a low-dimensional space, trying to preserve some criterion of the original affinity. For example, spectral embeddings tend to map points with many connec- tions between them to nearby locations, and multidimension- al scaling (MDS)-type methods try to preserve global information, such as graph geodesic distances. Examples of manifold learning include different flavors of MDS [26] , locally linear embedding [27] , sto- chastic neighbor embedding [28 ], spectral embeddings, such as Laplacian eigenmaps [29] and diffusion maps [30] , and deep models [31] . Instead of embedding the vertices, the graph structure can be pro - cessed by decomposing it into small subgraphs called motifs [36] or graphlets [37]. Finally, most recent approaches [32]– [34] tried to apply the successful word-embedding model [35] to graphs.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=SFxypsvhhMQ#=44m&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Input distribution lies in a manifold when there is structure/correlations in high dimensional space&lt;/a&gt; – like &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Sloppy%2520model.html&quot;&gt;Sloppy model&lt;/a&gt;s&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Laplacian_eigenmaps&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Laplacian_eigenmaps&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Several efficient manifold learning techniques have been proposed.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Isometric feature mapping&lt;/strong&gt; (ISOMAP) (Balasubramanian et al., 2002) estimates the geodesic distances on the manifold and uses them for projection. &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Multidimensional%2520scaling.html&quot;&gt;Multidimensional scaling&lt;/a&gt; with &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Geodesic.html&quot;&gt;Geodesic&lt;/a&gt; distance, basically. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://science.sciencemag.org/content/290/5500/2319?ijkey=4459d99d55dbcf7cf47149bee86b1e483a2b4437&amp;amp;keytype2=tf_ipsecsha&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;A Global Geometric Framework for Nonlinear Dimensionality Reduction&lt;/a&gt; – &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://science.sciencemag.org/content/295/5552/7&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;The Isomap Algorithm and Topological Stability&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Locality Preserving Projections (LPP)&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Locally linear embedding&lt;/strong&gt; (LLE) (Roweis and Saul, 2000) projects data points to a low-dimensional space that preserves local geometric properties. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.robots.ox.ac.uk/~az/lectures/ml/lle.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;paper&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Laplacian eigenmaps (LE) (Belkin and Niyogi, 2003) uses the weighted distance between two points as the loss function to get the dimension reduction results.&lt;/li&gt;&lt;li&gt;Local tangent space alignment (LTSA) (Zhang and Zha, 2004) constructs a local tangent space for each point and obtains the global low-dimensional embedding results through affine transformation of the local tangent spaces.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Yan et al. (2007) present a general formulation known as graph embedding to unify different dimensionality reduction algorithms within a common framework.&lt;/p&gt;&lt;/div&gt;


&lt;/div&gt;

&lt;/p&gt;
<code>
&lt;/section&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></p>