<p>&lt;p&gt;See &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Learning%20theory&quot;&gt;Learning theory&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;Generalization bound techniques&lt;/u&gt;&lt;/p&gt;&lt;p&gt;Basically, anything that restricts/controls the capacity. That is, gives an inductive bias!&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#VC%20dimension&quot;&gt;VC dimension&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#PAC-Bayesian%20learning&quot;&gt;PAC-Bayesian learning&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Fourier concentration-based bounds (see &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Sensitivity&quot;&gt;Sensitivity&lt;/a&gt; (&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=HIKTt9vaElM&amp;amp;feature=youtu.be&amp;amp;t=16m35s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;), &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Wavelet&quot;&gt;Wavelet&lt;/a&gt;s..)&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Rademacher%20complexity&quot;&gt;Rademacher complexity&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Statistical%20physics%20and%20inference&quot;&gt;Statistical physics framework&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Algorithmic%20robustness&quot;&gt;Algorithmic robustness&lt;/a&gt; and &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Algorithmic%20stability&quot;&gt;Algorithmic stability&lt;/a&gt;. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/pdf/1611.01838.pdf#page=8&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Continuity of loss functions implies algorithmic stability&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://pdfs.semanticscholar.org/4b7c/12945532833289ba7449dfc103f7f7993337.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Uniform stability&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Norm-based bounds (see &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Generalization%20in%20deep%20learning&quot;&gt;Generalization in deep learning&lt;/a&gt; for examples).&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Margin&quot;&gt;Margin&lt;/a&gt;-based bounds (see &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Support%20vector%20machine&quot;&gt;Support vector machine&lt;/a&gt;s for instance).&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Simplicity%20bias&quot;&gt;Simplicity bias&lt;/a&gt; in the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Parameter-function%20map&quot;&gt;Parameter-function map&lt;/a&gt; (see my paper)&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Kolmogorov%20complexity&quot;&gt;Kolmogorov complexity&lt;/a&gt;. See &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://link.springer.com/chapter/10.1007/978-3-642-44958-1_17&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt; for instance.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://dl.acm.org/citation.cfm?id=2073909&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Almost-everywhere algorithmic stability and generalization error&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Generalization%20in%20evolution&quot;&gt;Generalization in evolution&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Generalization%20in%20deep%20learning&quot;&gt;Generalization in deep learning&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.lcc.uma.es/~lfranco/Franco-complex06.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Generalization ability of Boolean functions implemented in feedforward neural networks&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.ki.tu-berlin.de/fileadmin/fg135/publikationen/opper/Op01.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Learning to generalize&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#One-shot%20learning&quot;&gt;One-shot learning&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Zero-shot%20learning&quot;&gt;Zero-shot learning&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Entropy dependence of generalization in matrix map, learned via &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Logistic%20regression&quot;&gt;Logistic regression&lt;/a&gt;. Should I try with &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Perceptron%20algorithm&quot;&gt;Perceptron algorithm&lt;/a&gt;?&lt;/p&gt;&lt;p&gt;[Btw side thought. For the 0/1 matrices, the only way of explaining what we see that I can think of, is that if the VC dimension of the set of epsilon-bad functions is higher for maps with the medium entropies (intuitively: there are more functions which predict wrongly when the true function has medium entropy, than when the true function has either high or low entropy)..&lt;/p&gt;&lt;p&gt;After looking back at some learning theory, I think this pattern can be understood if, for some reason, there are more functions (in the class of matrix map functions) which disagree significantly with the functions with row entropy 0.9, than with other functions. In technical jargon: the VC dimension of the set of epsilon-bad functions relative to functions with entropy 0.9 is larger than for other functions..&lt;/p&gt;&lt;p&gt;See WWIS, emails, etc..&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=2j0rrgr4bUc&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;NIPS 2017-Variance-based Regularization with Convex Objectives&lt;/a&gt; â€“ &lt;em&gt;optimal&lt;/em&gt; bias/variance tradeoff, &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1610.02581&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Variance-based regularization with convex objectives&lt;/a&gt;
&lt;/p&gt;</p>