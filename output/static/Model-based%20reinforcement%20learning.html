<p><a name="Model-based reinforcement learning">
<div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Reinforcement%20learning " data-tags="[[Reinforcement learning]]" data-tiddler-title="Model-based reinforcement learning"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class=" tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class=" tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class=" tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="tiddlymap" class=" tc-btn-%24%3A%2Fplugins%2Ffelixhayashi%2Ftiddlymap%2Fmisc%2FquickConnectButton " title="Toggle TiddlyMap actions">


</button></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Model-based reinforcement learning
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="#cosmos">
cosmos
</a> 23rd May 2018 at 6:39am
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p><em>aka <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Planning">Planning</a></em></p><p>Solving the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Bellman%20equation">Bellman equation</a>s</p><p><a class="tc-tiddlylink-external" href="https://worldmodels.github.io" rel="noopener noreferrer" target="_blank">https://worldmodels.github.io</a></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Linear%20programming">Linear programming</a></u></h2><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Dynamic%20programming">Dynamic programming</a></u></h2><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=dV80NAlEins&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;index=16#t=13m05" rel="noopener noreferrer" target="_blank">idea</a>
 – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=RtxI449ZjSc&amp;list=PLA89DCFA6ADACE599&amp;index=16#t=1h01m45s" rel="noopener noreferrer" target="_blank">Tradeoffs</a>. The idea is to solve consistency equations (derived by a <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h4m25s" rel="noopener noreferrer" target="_blank">look ahead tree</a> and <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h1m48s" rel="noopener noreferrer" target="_blank">principle of optimality</a>) iteratively (see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Fixed-point%20iteration">Fixed-point iteration</a>). – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h23m55s" rel="noopener noreferrer" target="_blank">Summary of methods</a></p><h3 class=""><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=dV80NAlEins&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;index=16#t=3m" rel="noopener noreferrer" target="_blank">Neuro-dynamic programming</a></h3><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Policy%20iteration">Policy iteration</a></h3><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Value%20iteration">Value iteration</a></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h29m" rel="noopener noreferrer" target="_blank">Extensions to dynamic programming</a>:</p><ul><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h29m35s" rel="noopener noreferrer" target="_blank">Asynchronous dynamic programming (DP)</a><ul><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h30m52s" rel="noopener noreferrer" target="_blank">In-place DP</a></li><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h33m30s" rel="noopener noreferrer" target="_blank">Prioritized sweeping</a></li><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h35m38s" rel="noopener noreferrer" target="_blank">Real-time dynamic programming</a></li></ul></li></ul><p>There are other algorithms described in the <a class="tc-tiddlylink-external" href="https://www.wikiwand.com/en/Reinforcement_learning" rel="noopener noreferrer" target="_blank">Wiki page</a>:</p><ul><li>Trust Region Policy Optimization [1]</li><li>Proximal Policy Optimization (i.e., TRPO, but using a penalty instead of a constraint on KL divergence), where each subproblem is solved with either SGD or L-BFGS</li><li>Cross Entropy Method</li></ul><table><tbody><tr class="evenRow"><td><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=3#t=1h36m30s" rel="noopener noreferrer" target="_blank">final comment on DP methods</a>, DP uses full-width look ahead, plus it assumes we know dynamics. Instead we can <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Monte%20Carlo">sample</a>) –&gt; leads to <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Model-free%20reinforcement%20learning">Model-free reinforcement learning</a></td></tr></tbody></table><h3 class=""><u>Asynchronous DP</u></h3><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Real-time%20dynamic%20programming">Real-time dynamic programming</a> (RTDP), which uses <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#On-policy%20trajectory%20sampling">On-policy trajectory sampling</a></p><hr><h2 class=""><u>Combining <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Model-free%20reinforcement%20learning">model-free</a> with model-based RL</u></h2><p><a class="tc-tiddlylink-external" href="https://deepmind.com/blog/agents-imagine-and-plan/" rel="noopener noreferrer" target="_blank">https://deepmind.com/blog/agents-imagine-and-plan/</a> – <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1707.06170" rel="noopener noreferrer" target="_blank">Learning model-based planning from scratch</a></p><p>Using <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Generative%20model">Generative model</a>s, and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Environment%20model">Environment model</a>s</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Empowerment">Empowerment</a></p></div>


</div>


</a></p>