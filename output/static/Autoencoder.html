<p>&lt;p&gt;A type of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Artificial%20neural%20network&quot;&gt;Artificial neural network&lt;/a&gt; where the output has the same dimensionality as the input, and the network is train to be able to reproduce the output in the input. The key point is that there is an &lt;strong&gt;information bottleneck&lt;/strong&gt; in some of the hidden layers, where the number of neurons is limited, so that the network is forced to learn a &lt;strong&gt;sparse representation&lt;/strong&gt; of the data. For this reason, they can be used for &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Data%20compression&quot;&gt;Data compression&lt;/a&gt;, and other areas where such a representation may be useful. &lt;/p&gt;&lt;p&gt;As they are designed to extract important features of the data, they are a form of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Unsupervised%20learning&quot;&gt;Unsupervised learning&lt;/a&gt;, and they can be used as &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Generative%20model&quot;&gt;Generative model&lt;/a&gt;s&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;http://nghiaho.com/wp-content/uploads/2012/12/autoencoder_network1.png&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=FzS3tMl4Nsc&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;amp;index=44&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Neural networks [6.1] : Autoencoder - definition&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=Rdpbnd0pCiI&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Two Minute Papers - What is an Autoencoder?&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Convolutional autoencoder&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://pgaleone.eu/neural-networks/deep-learning/2016/12/13/convolutional-autoencoders-in-tensorflow/?utm_content=buffer3ec98&amp;amp;utm_medium=social&amp;amp;utm_source=twitter.com&amp;amp;utm_campaign=buffer&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://pgaleone.eu/neural-networks/deep-learning/2016/12/13/convolutional-autoencoders-in-tensorflow/?utm_content=buffer3ec98&amp;amp;utm_medium=social&amp;amp;utm_source=twitter.com&amp;amp;utm_campaign=buffer&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Variational autoencoder&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=P78QYjWh5sM&amp;amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;amp;index=14&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Deep Learning Lecture 14: Karol Gregor on Variational Autoencoders and Image Generation&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Denoising autoencoder&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://deeplearning.net/tutorial/dA.html&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;http://deeplearning.net/tutorial/dA.html&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Sparse autoencoder&lt;/u&gt;&lt;/h2&gt;&lt;hr&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Generative%20adversarial%20network&quot;&gt;Generative adversarial network&lt;/a&gt; are similar, but we learn the cost function, instead of just using l2 loss  (&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=QPkb5VcgXAM#t=58m50&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;vid&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.wikiwand.com/en/Autoencoder&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://www.wikiwand.com/en/Autoencoder&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Information%20bottleneck&quot;&gt;Information bottleneck&lt;/a&gt; seems to be basically the principle behind autoencoders&lt;/p&gt;</p>