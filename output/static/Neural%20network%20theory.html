<p>&lt;p&gt;The theory (mainly &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Learning%20theory&quot;&gt;Learning theory&lt;/a&gt;) of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Artificial%20neural%20network&quot;&gt;Artificial neural network&lt;/a&gt;s. See also &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Deep%20learning%20theory&quot;&gt;Deep learning theory&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Mathematical%20modelling%20of%20neural%20networks&quot;&gt;Mathematical modelling of neural networks&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Statistical%20mechanics%20of%20neural%20networks&quot;&gt;Statistical mechanics of neural networks&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Neural networks class - Universit√© de Sherbrooke (Hugo Larochelle)&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;small&gt;Note that the notation in &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://arxiv.org/abs/1608.08225&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;this paper&lt;/a&gt; is opposite to that standard in &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Machine%20learning&quot;&gt;Machine learning&lt;/a&gt;. &lt;u&gt;In the paper, &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;y&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.03588em;&quot;&gt;y&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the input, and &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;x&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot;&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the output.&lt;/u&gt;
&lt;/small&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Expressivity%20of%20neural%20networks&quot;&gt;Expressivity of neural networks&lt;/a&gt;&lt;/u&gt;&lt;/h2&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Learning of neural networks&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;See &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Learning%20theory&quot;&gt;Learning theory&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Artificial%20neural%20network&quot;&gt;Artificial neural network&lt;/a&gt;s, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Deep%20learning%20theory&quot;&gt;Deep learning theory&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Non-convex%20optimization&quot;&gt;Non-convex optimization&lt;/a&gt;...&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://gingkoapp.com/vehvff&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Learning theory and neural networks gingko tree&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;See talk at ICLR2017 by &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://staff.polito.it/riccardo.zecchina/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Riccardo Zecchina&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://iopscience.iop.org/article/10.1209/0295-5075/27/2/002/meta&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Domains of Solutions and Replica Symmetry Breaking in Multilayer Neural Networks&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.75.2432&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Weight Space Structure and Internal Representations: A Direct Approach to Learning and Generalization in Multilayer Neural Networks&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.115.128101&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Computational efficiency and optimization: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://pdfs.semanticscholar.org/a13e/ab6052cc9f85054d70d3ba395b0d77652172.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Unreasonable effectiveness of learning neural nets: Accessible states and robust ensembles&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Generalization&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://dspace.mit.edu/handle/1721.1/107841&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Theory of Deep Learning III: Generalization Properties of SGD&lt;/a&gt;&lt;/p&gt;&lt;p&gt;See &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Generalization&quot;&gt;Generalization&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Generalization%20in%20deep%20learning&quot;&gt;Generalization in deep learning&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Statistical%20physics%20and%20inference&quot;&gt;Statistical physics and inference&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;small&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=hfBHELbk2Yw&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Statistical mechanics of learning&lt;/a&gt; &lt;/small&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.29.6931&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Uniqueness of the weights for minimal feedforward nets with a given input-output map&lt;/a&gt;,. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.researchgate.net/profile/Vera_Kurkova/publication/240374504_UNIQUENESS_OF_NETWORK_PARAMETERIZATIONS_AND_FASTER_LEARNING/links/547024480cf216f8cfa9e99e.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;more here&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://ieeexplore.ieee.org/abstract/document/371799/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;For neural networks, function determines form&lt;/a&gt;, for 0-hidden layer, neural nets...&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.mitpressjournals.org/doi/abs/10.1162/neco.1994.6.3.543?journalCode=neco&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Functionally Equivalent Feedforward Neural Networks&lt;/a&gt;&lt;/p&gt;&lt;p&gt;See more at &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Singular%20learning%20theory&quot;&gt;Singular learning theory&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Neural network dynamics, see &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Dynamical%20system&quot;&gt;Dynamical system&lt;/a&gt;s&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://pearl.plymouth.ac.uk/handle/10026.1/8647&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Dynamical Systems Theory for Transparent Symbolic Computation in Neuronal Networks&lt;/a&gt; We show that a correspondence can be found between these networks and &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Finite-state%20transducer&quot;&gt;Finite-state transducer&lt;/a&gt;s, and use the derived abstraction to investigate how noise affects computation in this class of systems, unveiling a surprising facilitatory effect on information transmission.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://ieeexplore.ieee.org.sci-hub.cc/document/58339/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;http://ieeexplore.ieee.org.sci-hub.cc/document/58339/&lt;/a&gt;
&lt;/p&gt;</p>