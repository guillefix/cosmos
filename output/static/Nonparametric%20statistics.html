<p>Formally, the number of &quot;parameters&quot; grows with the training set. Here number of parameters basically refers to &quot;amount of information in learned  function&quot;. For example, in neares-neighbour models, the parameters are the position of the neighbourhoods, and the categories to which those regions map. The number of these parameters clearly grows as more data points are added.. A more intuitive way of looking at it is that the algorithm doesn't fit parameters, and instead uses some other method to generate the function it is trying to learn. See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Nonparametric%20statistics">Nonparametric statistics</a> An example is <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Nearest-neighbour%20classification">Nearest-neighbour classification</a> (note that the model has parameters, but they are not fitting parameters, instead they are chosen before using the algorithm, and then are fixed)</p><p><a class="tc-tiddlylink-external" href="https://en.wikipedia.org/wiki/Nonparametric_statistics" rel="noopener noreferrer" target="_blank">https://en.wikipedia.org/wiki/Nonparametric_statistics</a></p><p>not based on parameterized families of probability distributions</p><p><u>Nonparametric models</u></p><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#KNN">KNN</a> â€“ <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Nearest-neighbour%20classification">Nearest-neighbour classification</a> </li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Locally-weighted%20linear%20regression">Locally-weighted linear regression</a></li><li>Infinite-dimensional <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Kernel%20method">Kernel method</a>s</li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Gaussian%20process">Gaussian process</a>es</li></ul><p>They are both based on looking at local neighbours near points for which we are going to make the prediction.
</p>