<p>A <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Regularization">Regularization</a> method for <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Deep%20learning">deep</a> <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Artificial%20neural%20network">neural networks</a>, which works by dropping out random neurons/nodes in the network, while training. This avoids <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Overfitting%20and%20underfitting">overfitting</a> by forcing the network to learn more robust models, that tend to be simpler.</p><p><a class="tc-tiddlylink-external" href="https://www.quora.com/How-does-the-dropout-method-work-in-deep-learning" rel="noopener noreferrer" target="_blank">How does the dropout method work in deep learning?</a></p><p><a class="tc-tiddlylink-external" href="http://jmlr.org/papers/v15/srivastava14a.html" rel="noopener noreferrer" target="_blank"> Dropout: A Simple Way to Prevent Neural Networks from Overfitting </a></p><p>See also <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#New%20advances%20in%20deep%20learning">New advances in deep learning</a></p>