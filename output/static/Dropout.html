<p>&lt;p&gt;A &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Regularization&quot;&gt;Regularization&lt;/a&gt; method for &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Deep%20learning&quot;&gt;deep&lt;/a&gt; &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Artificial%20neural%20network&quot;&gt;neural networks&lt;/a&gt;, which works by dropping out random neurons/nodes in the network, while training. This avoids &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Overfitting%20and%20underfitting&quot;&gt;overfitting&lt;/a&gt; by forcing the network to learn more robust models, that tend to be simpler.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.quora.com/How-does-the-dropout-method-work-in-deep-learning&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;How does the dropout method work in deep learning?&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://jmlr.org/papers/v15/srivastava14a.html&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt; Dropout: A Simple Way to Prevent Neural Networks from Overfitting &lt;/a&gt;&lt;/p&gt;&lt;p&gt;See also &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#New%20advances%20in%20deep%20learning&quot;&gt;New advances in deep learning&lt;/a&gt;&lt;/p&gt;</p>