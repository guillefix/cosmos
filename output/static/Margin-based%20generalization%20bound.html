<p>&lt;p&gt;See Understanding Machine Learning by Shai and Shai for &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Support%20vector%20machine&quot;&gt;SVM&lt;/a&gt; bounds; they arell all based on &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Structural%20risk%20minimization&quot;&gt;Structural risk minimization&lt;/a&gt; and bounding &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Rademacher%20complexity&quot;&gt;Rademacher complexity&lt;/a&gt; by bounding norms of the parameters/&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.esaim-ps.org/articles/ps/pdf/2005/01/ps0420.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Theory of classification: A survey of some recent advances.&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.springer.com/gb/book/9780387308654&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Estimation of Dependences Based on Empirical Data&lt;/a&gt; – &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.svms.org/training/BOGV92.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;a training algorithm for optimal margin classifiers&lt;/a&gt; – &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://link.springer.com/article/10.1007/BF00994018&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Support-vector networks.&lt;/a&gt; –  &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.springer.com/gb/book/9780387987804&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;The Nature of Statistical Learning Theory&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The algorithmic idea of large margin classifiers was introduced in the linear case by &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.springer.com/gb/book/9780387308654&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Vapnik (1982)&lt;/a&gt; (see also (&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.svms.org/training/BOGV92.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Boser et al., 1992&lt;/a&gt;; &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://link.springer.com/article/10.1007/BF00994018&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Cortes and Vapnik, 1995&lt;/a&gt;)). &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.springer.com/gb/book/9780387987804&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Vapnik (1995)&lt;/a&gt; gave an intuitive explanation ofthe performance of these methods based on a sample-dependent VC-dimension calculation, but withoutgeneralization bounds. The first rigorous generalization bounds for large margin linear classifiers (&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://pdfs.semanticscholar.org/db5f/533d9f06d8d86e4e003478b3dc4bba15b848.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Shawe-Taylor et al., 1998&lt;/a&gt;) required a scale-sensitive complexity analysis of real-valued function classes. At thesame time, a large margin analysis was developed for two-layer networks (&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://papers.nips.cc/paper/1204-for-valid-generalization-the-size-of-the-weights-is-more-important-than-the-size-of-the-network.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Bartlett, 1996&lt;/a&gt;), indeed with a proof technique that inspired the layer-wise induction used to prove Theorem 1.1 in the &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1706.08498&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;present work&lt;/a&gt; .Margin theory was quickly extended to many other settings (see for instance the survey by &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.esaim-ps.org/articles/ps/pdf/2005/01/ps0420.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Boucheron et al.(2005)&lt;/a&gt;), one major success being an explanation of the generalization ability of boosting methods, whichexhibit an explicit growth in the size of the function class over time, but a stable excess risk (Schapireet al., 1997)&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://pdfs.semanticscholar.org/db5f/533d9f06d8d86e4e003478b3dc4bba15b848.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Structural risk minimization over data-dependent hierarchies&lt;/a&gt; – &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://papers.nips.cc/paper/1204-for-valid-generalization-the-size-of-the-weights-is-more-important-than-the-size-of-the-network.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;For valid generalization the size of the weights is more important than the size of thenetwork&lt;/a&gt; – &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://pdfs.semanticscholar.org/9f1e/b4445219fbc994eb3e47e76cf1428d99815c.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;The sample complexity of pattern classification with neural networks: the size ofthe weights is more important than the size of the network.&lt;/a&gt;&lt;/p&gt;</p>