<p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Kernel%20method">Kernel method</a>, and <a class="tc-tiddlylink-external" href="https://youtu.be/Vm5QE54y6mw?t=51m59s" rel="noopener noreferrer" target="_blank">here</a></p><p>The idea of generalizing <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Dictionary%20learning">Dictionary learning</a> to learning over <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Reproducing%20kernel%20Hilbert%20space">Reproducing kernel Hilbert space</a>, where function evaluation can be substituted with an inner product.  Operationally, one often just has to substitute inner products between input vectors with a Kernel function evaluated at these input vectors, and one gets a new learning algorithm. This learning algorithm actually can now have an infinite dimensional hypothesis class (refering to dimension of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Hilbert%20space">Hilbert space</a> of functions), while it was finite dimensional for dictionary learning.</p><p>See a derivation here: <a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1611.03530.pdf" rel="noopener noreferrer" target="_blank">https://arxiv.org/pdf/1611.03530.pdf</a> , and original reference</p><p>Note that this kernel solution has an appealing interpretation in terms of implicit regularization.
Simple algebra reveals that it is equivalent to the minimum l2-norm solution of Xw = y. That is,
out of all models that exactly fit the data, SGD will often converge to the solution with minimum
norm.</p><p><a class="tc-tiddlylink-external" href="https://pdfs.semanticscholar.org/17d2/f027221d60cda373ecf15b03706c9e60269b.pdf" rel="noopener noreferrer" target="_blank">A generalized representer theorem</a></p>