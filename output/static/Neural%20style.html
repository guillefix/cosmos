<p>&lt;p&gt;See &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Convolutional%20neural%20network&quot;&gt;Convolutional neural network&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Deep%20art&quot;&gt;Deep art&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=GHVaaHESrlY&amp;amp;list=PL16j5WbGpaM0_Tj8CRmurZ8Kk1gEBc7fg&amp;amp;index=9#t=52m&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;video&lt;/a&gt;. Achieved with optimization using &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Convolutional%20neural%20network&quot;&gt;ConvNets&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We have &lt;strong&gt;content image&lt;/strong&gt; and &lt;strong&gt;style image&lt;/strong&gt;, and want to transfer the style into the content.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://arxiv.org/pdf/1508.06576v1.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;A Neural Algorithm of Artistic Style&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=FzvTLEB_3KY&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Pikazo - neural style video tech demo&lt;/a&gt; &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=Khuj4ASldmU&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Artistic style transfer for videos&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=h0xF6R5MpyA&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Analogy-Driven 3D Style Transfer&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.reddit.com/r/MachineLearning/comments/3imx1m/a_neural_algorithm_of_artistic_style/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;reddit discussion&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Code: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://gitxiv.com/posts/jG46ukGod8R7Rdtud/a-neural-algorithm-of-artistic-style&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;code&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://github.com/jcjohnson/neural-style&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;neural-style&lt;/a&gt; for torch&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://github.com/anishathalye/neural-style&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Neural style in TensorFlow&lt;/a&gt; â€“ &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://github.com/woodrush/neural-art-tf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;&amp;quot;Neural Art&amp;quot; in TensorFlow&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=-R9bJGNHltQ&amp;amp;list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2&amp;amp;index=6&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Two Minute Papers - Deep Neural Network Learns Van Gogh's Art&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Neural-style applied to videos too.&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Method&lt;/u&gt;&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;Step 1: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=GHVaaHESrlY&amp;amp;list=PL16j5WbGpaM0_Tj8CRmurZ8Kk1gEBc7fg&amp;amp;index=9#t=53m14s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Extract content targets&lt;/a&gt;, by passing content image through convnet, and record all raw activations in the convnet, which we say correspond to the &amp;quot;content&amp;quot; of the image.&lt;/li&gt;&lt;li&gt;Step 2: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=GHVaaHESrlY&amp;amp;list=PL16j5WbGpaM0_Tj8CRmurZ8Kk1gEBc7fg&amp;amp;index=9#t=53m40s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Extract style targets&lt;/a&gt;. Pass style image through convent, and look at pairwise statistics in style gram matrices. They have found these are good statistics for the style of an image. They are basically, for a certain convolutional layer of size nxn, and p features, the ~covariance of the features over the nxn space. That is take a particlar point in the nxn space and look at all p features. The vector of p features is called a fiber. Take the outer product of that vector with itself (which is like a covariance matrix), and average it over all points in the nxn space, can also be written in other ways using matrices.. Intuitively, it represents &lt;strong&gt;how often each pair of features fire together&lt;/strong&gt;. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=GHVaaHESrlY&amp;amp;list=PL16j5WbGpaM0_Tj8CRmurZ8Kk1gEBc7fg&amp;amp;index=9#t=1h20s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Could use other spatially-invariant statistics&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=GHVaaHESrlY&amp;amp;list=PL16j5WbGpaM0_Tj8CRmurZ8Kk1gEBc7fg&amp;amp;index=9#t=55m45s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Optimization of image step&lt;/a&gt;. Loss consists of two terms:&lt;ol&gt;&lt;li&gt;A content loss: match activations&lt;/li&gt;&lt;li&gt;a style loss: match gram matrices&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Best optimized with &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.wikiwand.com/en/Limited-memory_BFGS&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;LBFGS&lt;/a&gt;, because everything fits in memory, as only have one image..&lt;/p&gt;</p>