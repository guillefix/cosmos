<p><u>Algorithms</u></p><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Gradient%20descent">Gradient descent</a> <ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Stochastic%20gradient%20descent">Stochastic gradient descent</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Newton's%20method">Newton's method</a></li></ul></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#EM%20algorithm">EM algorithm</a></li></ul><h2 class=""><u>Parameter initialization</u></h2><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sLfogkzFNfc&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=15" rel="noopener noreferrer" target="_blank">Neural networks [2.9] : Training neural networks - parameter initialization</a></p><h2 class=""><u>Batch learning</u></h2><p>The most common procedure, described above and in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Machine%20learning">Machine learning</a>, where the algorithm is run on a particular data set all at once.</p><h3 class=""><u>Mini-batch learning</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Bver7Ttgb9M&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=17#t=11m37s" rel="noopener noreferrer" target="_blank">video</a>. Performing the optimization algo on a sample of a given size from the data set, per iteration.</p><h3 class=""><u>Momentum</u></h3><p>To get through plateaus, for instance.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0qUAb94CpOw&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;index=6#t=50m40s" rel="noopener noreferrer" target="_blank">Momentum</a>. You add inertia to the particle so that the gradient descent is not just velocity = gradient (as it'd be in viscous fluid), but it is acceleration = (viscosity) + gradient.</p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Backpropagation">Backpropagation</a></u></h3><p>for <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Artificial%20neural%20network">Artificial neural network</a>s</p><h2 class=""><u>Online learning</u></h2><p>as opposed to <em>batch learning</em></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=17m30s" rel="noopener noreferrer" target="_blank">video</a>. You have to make predictions even in the process of learning.</p><p><sub>
(<em>Online algorithm</em>, you process the data sequentially, by chunks. You need this if you do not access to all of it at the same time, or you have so much data that not all of it fits on your RAM..)
</sub></p><p>What we care about is the <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=19m25s" rel="noopener noreferrer" target="_blank">online error</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=19m57s" rel="noopener noreferrer" target="_blank">Can apply batch learning algos for online learning</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=21m58s" rel="noopener noreferrer" target="_blank">Several theoretical results exist</a></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Simplicity%20and%20learning">Simplicity and learning</a></u></h2><p>The simplicity and structure in signals in the real-world is often seized to make the learning problem easier to solve.</p><h2 class=""><u><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=24m55s" rel="noopener noreferrer" target="_blank">Advice for applying machine learning algorithms</a></u></h2><h3 class=""><u><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=28m37s" rel="noopener noreferrer" target="_blank">Diagnosis for debugging learning algorithms</a></u></h3><p>Diagnostic:</p><ul><li>High variance (overfitting) -&gt; Training error would be much lower than test error. (<a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=34m05s" rel="noopener noreferrer" target="_blank">video</a>)</li><li>High bias (underfitting) -&gt; Test error will also be high. (<a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=35m50s" rel="noopener noreferrer" target="_blank">video</a>)</li></ul><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=37m50s" rel="noopener noreferrer" target="_blank">Ways to fix high variance or bias</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=37m50s" rel="noopener noreferrer" target="_blank">Is the algo converging?</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=43m30s" rel="noopener noreferrer" target="_blank">Are you optimizing the right function?</a> –&gt; <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=45m" rel="noopener noreferrer" target="_blank">Diagnostic</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=50m45s" rel="noopener noreferrer" target="_blank">One more example</a></p><h3 class=""><u><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=1h03m20s" rel="noopener noreferrer" target="_blank">Error analysis</a> and <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=1h08m48s" rel="noopener noreferrer" target="_blank">ablative analysis</a></u></h3><h3 class=""><u><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=1h11m45s" rel="noopener noreferrer" target="_blank">How to get started on a machine learning problem</a></u></h3><p>– Premature (statistical) optimization (<a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=27m30s" rel="noopener noreferrer" target="_blank">video</a>)</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=1h16m45s" rel="noopener noreferrer" target="_blank">The dangers of over-theorizing</a></p><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Optimization%20for%20training%20deep%20models">Optimization for training deep models</a></h3>