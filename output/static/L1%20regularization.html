<p>&lt;p&gt;A regularization technique that penalizes the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#L1%20norm&quot;&gt;L1 norm&lt;/a&gt; (that is the sum of the absolute values of the model coefficients).&lt;/p&gt;&lt;p&gt;It can be seen as a &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Convex%20relaxation&quot;&gt;Convex relaxation&lt;/a&gt; of the hard problem of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Sparsity-based%20regularization&quot;&gt;Sparsity-based regularization&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;It can also &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://youtu.be/eyxjhdoNXaw?t=35m51s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;be seen, when applied to feature selection in the principal components basis as&lt;/a&gt; s a filter where we include those features which correlate best with the output, basically.&lt;/p&gt;&lt;p&gt;When used in a least-squares problem, the method is called &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Lasso&quot;&gt;Lasso&lt;/a&gt;&lt;/p&gt;</p>