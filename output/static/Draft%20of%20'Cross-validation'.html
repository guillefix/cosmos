<p>&lt;p&gt;Test the model on data you haven't used for training.&lt;/p&gt;&lt;p&gt;min-max, average&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.cs.cmu.edu/~schneide/tut5/node42.html&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://www.cs.cmu.edu/~schneide/tut5/node42.html&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Wikipedia has good explanations: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://en.wikipedia.org/wiki/Cross-validation_(statistics)&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://en.wikipedia.org/wiki/Cross-validation_(statistics)&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Hold-out cross-validation&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0kWZoyNRxTY&amp;amp;index=10&amp;amp;list=PLA89DCFA6ADACE599#t=41m30s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;video&lt;/a&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Randomly split training set S into two subsets, the training subset S&lt;sub&gt;train&lt;/sub&gt;, (70%) and the cross-validation subset S&lt;sub&gt;CV&lt;/sub&gt; (30%).&lt;/li&gt;&lt;li&gt;Train the model on the training set, and test it on S&lt;sub&gt;CV&lt;/sub&gt;&lt;/li&gt;&lt;li&gt;Pick the model (set of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Hyperparameter&quot;&gt;Hyperparameter&lt;/a&gt;s) with smallest error on S&lt;sub&gt;CV&lt;/sub&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;To search for best configuration of hyperparameters acoording to the validation error, there are several methods. Some popular ones are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=Fs-raHUnF2M&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;amp;index=16#t=4m10s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Grid search&lt;/a&gt;, try all possible configurations from chosen trial values.&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=Fs-raHUnF2M&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;amp;index=16#t=6m05s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Random search&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Sometimes, once a model complexity (and other &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Hyperparameter&quot;&gt;Hyperparameter&lt;/a&gt;s) are picked, the model is trained on the whole data set.&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;&lt;strong&gt;Test set&lt;/strong&gt;&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;To predict the generalization error (see &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Learning%20theory&quot;&gt;Learning theory&lt;/a&gt;) of the chosen hyperparameters that are best, we can't just look at their result at S&lt;sub&gt;CV&lt;/sub&gt;, as that set has been by the algorithm to choose the final answer. As a result, the error on S&lt;sub&gt;CV&lt;/sub&gt; is a biased estimator of generalization error. To have an unbiased estimator, we need a test set, that is only used once the learning algorithm has finished completely. &lt;/p&gt;&lt;p&gt;See &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=Fs-raHUnF2M&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;amp;index=16&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;k-fold cross-validation&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0kWZoyNRxTY&amp;amp;index=10&amp;amp;list=PLA89DCFA6ADACE599#t=44m49s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;video&lt;/a&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Split data into k equal pieces. Common k=10.&lt;/li&gt;&lt;li&gt;For i from 1 to k:&lt;/li&gt;&lt;/ol&gt;&lt;dl&gt;&lt;dd&gt;&lt;dl&gt;&lt;dd&gt;Hold-out the ith piece for testing, and use the other k-1 pieces for training.&lt;/dd&gt;&lt;/dl&gt;&lt;/dd&gt;&lt;dd&gt;3. Average errors from the k iterations&lt;/dd&gt;&lt;/dl&gt;&lt;p&gt;More computationally expensive&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Leave-one-out cross-validation&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;k-fold CV, for whem k={number of training examples}, so for each iteration, you leave one out.&lt;/p&gt;&lt;p&gt;Even more computationally expensive, but more accurate estimate of generalization error. Only done when the data is very scarce.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;em&gt;Some thoughts&lt;/em&gt; when I misunderstood train/validation/test learning.&lt;/p&gt;&lt;p&gt;What I describe here is some sort of hyperlearning where we have two steps, where we learn two sets of hyperparameters, and use two different validation sets (which I call below validation, and test, mistakenly...)&lt;/p&gt;&lt;p&gt;When we have trained the data using a method as hold-out CV, and with some fixed learning &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Hyperparameter&quot;&gt;Hyperparameter&lt;/a&gt;s. If we want to learn the hyperparameters, we can do this whole training procedure with several values of the hyperparameter.&lt;/p&gt;&lt;p&gt;However, to choose the hyperparameters that are best, we can't just look at their result at S&lt;sub&gt;CV&lt;/sub&gt;, as that set has been by the algorithm to choose the final answer. As a result, the error on S&lt;sub&gt;CV&lt;/sub&gt; is a biased estimator of generalization error (see &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Learning%20theory&quot;&gt;Learning theory&lt;/a&gt;). To have an unbiased estimator, we need a test set, that is only used once the learning algorithm has finished completely. We can compare the results on the test set to choose the best set of hyperparameters. Note, that once we have done that, the test error is no longer an unbiased estimate of generalization error, as we have used it to output our very final answer; i.e. our final answer depended on it!! We'd need a hypertest set
&lt;/p&gt;</p>