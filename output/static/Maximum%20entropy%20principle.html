<p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PVirThf1iyY#t=1h20m" rel="noopener noreferrer" target="_blank">Video</a></p><p>A principle of statistical inference for <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Density%20estimation">inferring probability distributions</a> when we know <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Moment%20(mathematics)">moments</a> of the distribution. It states that we should look for the distribution which maximizes <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Entropy">Entropy</a> while satisfying the moment contraints. This problem has an analytic solution in terms of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Lagrange%20multiplier">Lagrange multiplier</a>s, as a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Boltzmann%20distribution">Gibbs distribution</a>.</p><p><a class="tc-tiddlylink-external" href="https://www.wikiwand.com/en/Principle_of_maximum_entropy" rel="noopener noreferrer" target="_blank">https://www.wikiwand.com/en/Principle_of_maximum_entropy</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PVirThf1iyY#t=1h31m30s" rel="noopener noreferrer" target="_blank">The key problem is to find the appropriate moments to measure/estimate</a></p>