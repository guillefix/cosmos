<p>&lt;p&gt;Inspiration from neuroscience -&amp;gt; neural networks.&lt;/p&gt;&lt;p&gt;Convolutional network. Matthew Zeiler &amp;amp; Rob Fergus&lt;/p&gt;&lt;p&gt;Supervised vs unsupervised.&lt;/p&gt;&lt;p&gt;A good principle for learning is for the machine trying to reconstruct the things it wants to learn using its neural net. If what it reconstructs doesn't agree with what it then sees, it should learn. This sounds like learning by imitation.&lt;/p&gt;&lt;p&gt;Regularity helps..     &lt;/p&gt;&lt;p&gt;Multimodal, learning combining different kinds of data&lt;/p&gt;&lt;p&gt;Sequence learning and recurrent nets: have memory, can predict sequences (in time say). Can &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://en.wikipedia.org/wiki/Parsing&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;parse&lt;/a&gt; words, and they show that grammar can be learned.&lt;/p&gt;&lt;p&gt;Being able to fill gaps in the information you receive (like our brain does, or like machines do with generative models, which also learn) is useful for decision making, as you can know what to expect, even with incomplete info.&lt;/p&gt;&lt;p&gt;Siamese neuronal network &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://stats.stackexchange.com/questions/154652/how-does-the-back-propagation-work-in-a-siamese-neural-network&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Q&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Reinforcement learning&lt;/p&gt;&lt;p&gt;Imitation learning&lt;/p&gt;&lt;p&gt;Back-propagation.
&lt;/p&gt;</p>