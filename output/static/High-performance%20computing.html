<p>&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Parallel%20computing&quot;&gt;Parallel computing&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Distributed%20computing&quot;&gt;Distributed computing&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#GPU%20computing&quot;&gt;GPU computing&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Neuromorphic%20computing&quot;&gt;Neuromorphic computing&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Roofline model&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;Value of the &lt;em&gt;computational intensity&lt;/em&gt; of an algorithm versus its floating-point operations per second, usually plotted in a 2D graph. It gives an idea of how the data bandwidth affects flops. &lt;/p&gt;&lt;p&gt;&lt;em&gt;computational/arithmetic intensity&lt;/em&gt; (AI): arithmetic operations (ao) per byte &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=-40jMw4HN-o&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;AI in parallel comp&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Low AI means we do few aos per bit of data, and so data bandwidth is important. As soon as we do enough aos per byte, then the blottleneck is our actual processing unit (depending on hardware/architecture), whether CPU or GPU.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.wikiwand.com/en/Roofline_model&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://www.wikiwand.com/en/Roofline_model&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;High-performance computing with &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Matlab&quot;&gt;Matlab&lt;/a&gt;&lt;/u&gt;&lt;/h2&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;Code profiling&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;Identify bottlenecks in the code.&lt;/p&gt;&lt;p&gt;&lt;code&gt;tic&lt;/code&gt;, &lt;code&gt;toc&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;code&gt;profile&lt;/code&gt;&lt;/p&gt;&lt;p&gt;code analyzer&lt;/p&gt;&lt;p&gt;&lt;u&gt;Vector preallocation&lt;/u&gt;. Arrays which dynamically change size can be slow, because array memory has to be reallocated.&lt;/p&gt;&lt;p&gt;Use backlash, and store sparse matrices in sparse format.&lt;/p&gt;&lt;p&gt;vectorization&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Parallel%20computing&quot;&gt;Parallel computing&lt;/a&gt;&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;parfor: parallel for loops. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://uk.mathworks.com/help/distcomp/classification-of-variables-in-parfor-loops.html&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Variable types in parfors&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Multithreading. Several execution threads, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Concurrent%20computing&quot;&gt;Concurrent computing&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;MEX functions&lt;/p&gt;&lt;p&gt;Parallel computing with &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Computer%20cluster&quot;&gt;Computer cluster&lt;/a&gt;s&lt;/p&gt;&lt;p&gt;spmd mode:	single	program	multiple	data. &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Concurrent%20computing&quot;&gt;Concurrent computing&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#GPU%20computing&quot;&gt;GPU computing&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=13&amp;amp;v=v-gSOY9-RgI&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Minimize time spent on memory&lt;/a&gt;
&lt;/p&gt;</p>