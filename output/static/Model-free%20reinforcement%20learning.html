<p>See more at <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Reinforcement%20learning">Reinforcement learning</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=1h34m10s" rel="noopener noreferrer" target="_blank">summary</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1803.07055.pdf" rel="noopener noreferrer" target="_blank">Simple random search provides a competitive approach to reinforcement learning</a> – Our findings contradict the common belief that policy gradient techniques,
which rely on exploration in the action space, are more sample efficient than methods based on
finite-differences [25, 26]. In more detail, our contributions are as follows:</p><h1 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Policy%20evaluation">Prediction</a></u></h1><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h7m" rel="noopener noreferrer" target="_blank">comparing approaches</a></p><p>Evaluation the value function given a policy</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT" rel="noopener noreferrer" target="_blank">Introduction, monte carlo model-free prediction</a>, just <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Monte%20Carlo">sample</a> over runs of the MDP+policy, and average empirical returns (discounted sum of rewards).</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Incremental%20average">Incremental</a> <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=29m5s" rel="noopener noreferrer" target="_blank">Monte Carlo update</a></p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Temporal%20difference%20learning">Temporal difference learning</a></u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=59m" rel="noopener noreferrer" target="_blank">Simple example comparing monte carlo vs TD0</a> </p><h1 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Model-free%20control">Model-free control</a> (tabular solutions)</u></h1><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;index=5&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT" rel="noopener noreferrer" target="_blank">intro video</a>!</p><p><b><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=9m55s" rel="noopener noreferrer" target="_blank">actually need to use the action-value function to be model-free</a></b></p><p>We are basically going to use <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Policy%20iteration">Policy iteration</a> with the <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Action-value%20function">Action-value function</a>, with different ways to do the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Policy%20evaluation">Policy evaluation</a> (by sampling) and policy update step (in a way that explores enough, given that the sampling means we don't see everything). This is an instance <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Generalized%20policy%20iteration">Generalized policy iteration</a> with <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Q%20function">Q function</a> <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Policy%20evaluation">evaluated</a> by sampling (model-free)</p><h2 class=""><u>Policy improvement <small>in the model free setting</small></u></h2><h3 class=""><u><span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">ϵ</span></span></span></span></span>-greedy exploration</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=14m30s" rel="noopener noreferrer" target="_blank">motivation</a> – <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Exploration%20versus%20exploitation">Exploration versus exploitation</a>. We need to carry on exploring everything to make sure we understand the value of all options!</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=15m55s" rel="noopener noreferrer" target="_blank">epsilon-greedy exploration</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=17m50s" rel="noopener noreferrer" target="_blank">theorem of policy improvement by epsilon-greedy policy iteration</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=23m40s" rel="noopener noreferrer" target="_blank">Making the policy iteration more efficient by only partial policy evaluation</a></p><h3 class=""><u>Greedy in the limit with infinite exploration (GLIE)</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=25m37s" rel="noopener noreferrer" target="_blank">GLIE is a method that is guaranteed to converge to the optimal policy in a model-free manner</a> – </p><p>An example is <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">ϵ</span></span></span></span></span>-greedy policy iteration with gradual decay of <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">ϵ</span></span></span></span></span></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=28m35s" rel="noopener noreferrer" target="_blank">GLIE Monte Carlo control</a></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#On-policy%20learning">On-policy learning</a> methods</u></h2><h3 class=""><u>Monte Carlo</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=7m" rel="noopener noreferrer" target="_blank">first attempt</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Policy%20iteration">Policy iteration</a> with Monte-Carlo policy evaluation – but this isn't very efficient, so we use TD learning methods.</p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Temporal%20difference%20learning">Temporal difference learning</a> methods</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=38m45s" rel="noopener noreferrer" target="_blank">introduction to TD learning for control</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Sarsa">Sarsa</a></p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Action-critic%20method">Action-critic method</a>s</u></h3><p><a class="tc-tiddlylink-external" href="https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node66.html" rel="noopener noreferrer" target="_blank">https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node66.html</a></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Off-policy%20learning">Off-policy learning</a> methods</u></h2><hr><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Curiosity">Curiosity</a> – <a class="tc-tiddlylink-external" href="https://pathak22.github.io/noreward-rl/" rel="noopener noreferrer" target="_blank">Curiosity-driven Exploration by Self-supervised Prediction</a>, see work by Schmidhuber
</p>