<p>&lt;p&gt;See more at &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Reinforcement%20learning&quot;&gt;Reinforcement learning&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;amp;index=5#t=1h34m10s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;summary&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/pdf/1803.07055.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Simple random search provides a competitive approach to reinforcement learning&lt;/a&gt; – Our findings contradict the common belief that policy gradient techniques,
which rely on exploration in the action space, are more sample efficient than methods based on
finite-differences [25, 26]. In more detail, our contributions are as follows:&lt;/p&gt;&lt;h1 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Policy%20evaluation&quot;&gt;Prediction&lt;/a&gt;&lt;/u&gt;&lt;/h1&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;amp;index=4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h7m&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;comparing approaches&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Evaluation the value function given a policy&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;amp;index=4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Introduction, monte carlo model-free prediction&lt;/a&gt;, just &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Monte%20Carlo&quot;&gt;sample&lt;/a&gt; over runs of the MDP+policy, and average empirical returns (discounted sum of rewards).&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Incremental%20average&quot;&gt;Incremental&lt;/a&gt; &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;amp;index=4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=29m5s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Monte Carlo update&lt;/a&gt;&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Temporal%20difference%20learning&quot;&gt;Temporal difference learning&lt;/a&gt;&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;amp;index=4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=59m&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Simple example comparing monte carlo vs TD0&lt;/a&gt; &lt;/p&gt;&lt;h1 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Model-free%20control&quot;&gt;Model-free control&lt;/a&gt; (tabular solutions)&lt;/u&gt;&lt;/h1&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;index=5&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;intro video&lt;/a&gt;!&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;amp;index=5#t=9m55s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;actually need to use the action-value function to be model-free&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;We are basically going to use &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Policy%20iteration&quot;&gt;Policy iteration&lt;/a&gt; with the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Action-value%20function&quot;&gt;Action-value function&lt;/a&gt;, with different ways to do the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Policy%20evaluation&quot;&gt;Policy evaluation&lt;/a&gt; (by sampling) and policy update step (in a way that explores enough, given that the sampling means we don't see everything). This is an instance &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Generalized%20policy%20iteration&quot;&gt;Generalized policy iteration&lt;/a&gt; with &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Q%20function&quot;&gt;Q function&lt;/a&gt; &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Policy%20evaluation&quot;&gt;evaluated&lt;/a&gt; by sampling (model-free)&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Policy improvement &lt;small&gt;in the model free setting&lt;/small&gt;&lt;/u&gt;&lt;/h2&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;&lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;-greedy exploration&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;amp;index=5#t=14m30s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;motivation&lt;/a&gt; – &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Exploration%20versus%20exploitation&quot;&gt;Exploration versus exploitation&lt;/a&gt;. We need to carry on exploring everything to make sure we understand the value of all options!&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;amp;index=5#t=15m55s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;epsilon-greedy exploration&lt;/a&gt; – &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;amp;index=5#t=17m50s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;theorem of policy improvement by epsilon-greedy policy iteration&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;amp;index=5#t=23m40s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Making the policy iteration more efficient by only partial policy evaluation&lt;/a&gt;&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;Greedy in the limit with infinite exploration (GLIE)&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;amp;index=5#t=25m37s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;GLIE is a method that is guaranteed to converge to the optimal policy in a model-free manner&lt;/a&gt; – &lt;/p&gt;&lt;p&gt;An example is &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;-greedy policy iteration with gradual decay of &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;amp;index=5#t=28m35s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;GLIE Monte Carlo control&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#On-policy%20learning&quot;&gt;On-policy learning&lt;/a&gt; methods&lt;/u&gt;&lt;/h2&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;Monte Carlo&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;amp;index=5#t=7m&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;first attempt&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Policy%20iteration&quot;&gt;Policy iteration&lt;/a&gt; with Monte-Carlo policy evaluation – but this isn't very efficient, so we use TD learning methods.&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Temporal%20difference%20learning&quot;&gt;Temporal difference learning&lt;/a&gt; methods&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;amp;index=5#t=38m45s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;introduction to TD learning for control&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Sarsa&quot;&gt;Sarsa&lt;/a&gt;&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Action-critic%20method&quot;&gt;Action-critic method&lt;/a&gt;s&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node66.html&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node66.html&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Off-policy%20learning&quot;&gt;Off-policy learning&lt;/a&gt; methods&lt;/u&gt;&lt;/h2&gt;&lt;hr&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Curiosity&quot;&gt;Curiosity&lt;/a&gt; – &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://pathak22.github.io/noreward-rl/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Curiosity-driven Exploration by Self-supervised Prediction&lt;/a&gt;, see work by Schmidhuber
&lt;/p&gt;</p>