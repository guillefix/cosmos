<p>Often we want the model to be better at generalizing, and this is done by reducing model complexity. This encourages &quot;encourages fitting signal rather than just noise&quot;. A lot of these methods are very much related to <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Model%20selection">Model selection</a> methods, as both try to make our model better, often by chosing &quot;simpler&quot; models.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=iO919hIhO-w&amp;list=PLyGKBDfnk-iDj3FBd0Avr_dLbrU8VG73O&amp;index=2" rel="noopener noreferrer" target="_blank">9.520 - 09/14/2015 - Class 02 - Prof. Tomaso Poggio: The Learning Problem and Regularization</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=SFxypsvhhMQ#t=7m15s" rel="noopener noreferrer" target="_blank">Class 02 - The Learning Problem and Regularization</a></p><p>We can penalize functions with high complexity, or limiting/penalizing other properties of the function. An example of this is penalizing the size of the weights or the norm of the parameter vector. We can do this by modifying certain parts of our learning algorithm for this purpose:</p><ul><li>by modifying our hypothesis class (like limiting the number of features in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Dictionary%20learning">Dictionary learning</a>, what is called <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Feature%20selection">Feature selection</a>). Another example is limiting the norm of the function in a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Reproducing%20kernel%20Hilbert%20space">Reproducing kernel Hilbert space</a>, which can correspond, for instance, to <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Band-limited%20function">Band-limited function</a>s, with no high frequency features (or few).</li><li>See <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=bBRX3OqNC9c#t=1h" rel="noopener noreferrer" target="_blank">here</a>. by modifying the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Loss%20function">Loss function</a>, which is perhaps the most common/basic form of regularization, explained <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=1m" rel="noopener noreferrer" target="_blank">here</a>. This one is what is often meant by regularization.</li><li>we can modify other aspects of the learning algorithm, which do other forms of more <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Implicit%20bias">implicit regularization</a>. </li></ul><p>–&gt; <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0UOaS-8E7Po" rel="noopener noreferrer" target="_blank">See comment here</a>: whenever we project our data, or we optimize, we may be able to think of these as forms of regularization!</p><p><a class="tc-tiddlylink-external" href="https://youtu.be/WwxbvHkuEpY?t=32m7s" rel="noopener noreferrer" target="_blank">Iterative regularization via early stopping</a> – <a class="tc-tiddlylink-external" href="https://youtu.be/WwxbvHkuEpY?t=43m29s" rel="noopener noreferrer" target="_blank">derivation for squared loss</a> – <a class="tc-tiddlylink-external" href="https://youtu.be/WwxbvHkuEpY?t=54m52s" rel="noopener noreferrer" target="_blank">result for finite time</a></p><p>Using cross-validation for regularization can be done using <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Early%20stopping">Early stopping</a> using the validation set</p><p>We can use a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Prior%20distribution">Prior distribution</a> from <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Bayesian%20statistics">Bayesian statistics</a>, to make <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Simplicity">simple</a> hypothesis more likely. See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Simplicity%20and%20learning">Simplicity and learning</a>.
 <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=12m" rel="noopener noreferrer" target="_blank">Intuition</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=JfkbyODyujw&amp;index=14&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH" rel="noopener noreferrer" target="_blank">Neural networks [2.8] : Training neural networks - regularization</a></p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Dropout">Dropout</a></u></h3><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Batch%20normalization">Batch normalization</a></u></h3><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Tikhonov%20regularization">Tikhonov regularization</a></u></h3><hr><p><a class="tc-tiddlylink-external" href="https://youtu.be/TwH_-MzhQ1I?t=22m1s" rel="noopener noreferrer" target="_blank">Amount of distance travelled is what regularizes in iterative regularization</a> </p><p><a class="tc-tiddlylink-external" href="https://youtu.be/TwH_-MzhQ1I?t=38m28s" rel="noopener noreferrer" target="_blank">Spectral filtering perspective</a>, many regularization algorithms can be seen as filtering out the high frequencies of the covariance matrix of the input data (i.e. make the prediciton not depend on small variations, as these can be easily affected by noise!)</p>