<p>&lt;p&gt;When we sample with a policy which can be different to the one which are trying to &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Optimal%20control&quot;&gt;optimize&lt;/a&gt; / &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Policy%20evaluation&quot;&gt;evaluate&lt;/a&gt;, called the &lt;strong&gt;target policy&lt;/strong&gt;. If the target policy is the same as the sampling policy, it becomes &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#On-policy%20learning&quot;&gt;On-policy learning&lt;/a&gt;, so off-policy methods are more general.&lt;/p&gt;&lt;p&gt;Useful for the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Exploration-exploitation%20trade-off&quot;&gt;Exploration-exploitation trade-off&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;amp;index=5#t=1h19m55s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;intro video&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;amp;index=5#t=1h25m33s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Can use&lt;/a&gt; &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Importance%20sampling&quot;&gt;Importance sampling&lt;/a&gt;, with weights which are the ratio of probability of trajectories for sampling and target policy&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;amp;index=5#t=1h28m45s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;The idea that works best&lt;/a&gt; is &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Q-learning&quot;&gt;Q-learning&lt;/a&gt;. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;amp;index=5#t=1h31m45s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Most well-known Q-learning type&lt;/a&gt;, where we allow both behaviour and target policies to improve&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Per-reward%20importance%20sampling&quot;&gt;Per-reward importance sampling&lt;/a&gt; (sec 5.9 in Sutton-Barto), Off-policy Returns. &lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Expected%20Sarsa&quot;&gt;Expected Sarsa&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Maximization%20bias&quot;&gt;Maximization bias&lt;/a&gt; and &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Double%20learning&quot;&gt;Double learning&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Afterstate&quot;&gt;Afterstate&lt;/a&gt;s&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Tree%20Backup%20Algorithm&quot;&gt;Tree Backup Algorithm&lt;/a&gt;&lt;/u&gt;&lt;/h3&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Q(sigma)%20algorithm&quot;&gt;Q(sigma) algorithm&lt;/a&gt;&lt;/u&gt;&lt;/h3&gt;&lt;h1 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Value%20function%20approximation&quot;&gt;Value function approximation&lt;/a&gt;&lt;/u&gt;&lt;/h1&gt;</p>