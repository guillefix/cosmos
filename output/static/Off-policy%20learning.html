<p>When we sample with a policy which can be different to the one which are trying to <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Optimal%20control">optimize</a> / <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Policy%20evaluation">evaluate</a>, called the <strong>target policy</strong>. If the target policy is the same as the sampling policy, it becomes <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#On-policy%20learning">On-policy learning</a>, so off-policy methods are more general.</p><p>Useful for the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Exploration-exploitation%20trade-off">Exploration-exploitation trade-off</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=1h19m55s" rel="noopener noreferrer" target="_blank">intro video</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=1h25m33s" rel="noopener noreferrer" target="_blank">Can use</a> <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Importance%20sampling">Importance sampling</a>, with weights which are the ratio of probability of trajectories for sampling and target policy</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=1h28m45s" rel="noopener noreferrer" target="_blank">The idea that works best</a> is <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Q-learning">Q-learning</a>. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=1h31m45s" rel="noopener noreferrer" target="_blank">Most well-known Q-learning type</a>, where we allow both behaviour and target policies to improve</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Per-reward%20importance%20sampling">Per-reward importance sampling</a> (sec 5.9 in Sutton-Barto), Off-policy Returns. </p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Expected%20Sarsa">Expected Sarsa</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Maximization%20bias">Maximization bias</a> and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Double%20learning">Double learning</a></p><p><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Afterstate">Afterstate</a>s</p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Tree%20Backup%20Algorithm">Tree Backup Algorithm</a></u></h3><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Q(sigma)%20algorithm">Q(sigma) algorithm</a></u></h3><h1 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Value%20function%20approximation">Value function approximation</a></u></h1>