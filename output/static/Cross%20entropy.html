<p>In information theory, the cross entropy between two probability distributions p and q over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an &quot;unnatural&quot; probability distribution q, rather than the &quot;true&quot; distribution p.</p><p><span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mo>(</mo><mi>p</mi><mo separator="true">,</mo><mi>q</mi><mo>)</mo><mo>=</mo><msub><mtext><mi mathvariant="normal">E</mi></mtext><mi>p</mi></msub><mo>[</mo><mo>−</mo><mi>log</mi><mi>q</mi><mo>]</mo><mo>=</mo><mi>H</mi><mo>(</mo><mi>p</mi><mo>)</mo><mo>+</mo><msub><mi>D</mi><mrow><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">L</mi></mrow></mrow></msub><mo>(</mo><mi>p</mi><mi mathvariant="normal">∥</mi><mi>q</mi><mo>)</mo><mo separator="true">,</mo><mspace width="-0.16667em"></mspace></mrow><annotation encoding="application/x-tex">H(p, q) = \text{E}_p[-\log q] = H(p) + D_{\mathrm{KL}}(p \| q),\!</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathit">p</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord"><span class="text mord textstyle uncramped"><span class="mord mathrm">E</span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">p</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">[</span><span class="mord">−</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mclose">]</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathit">p</span><span class="mclose">)</span><span class="mbin">+</span><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">D</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">K</span><span class="mord mathrm">L</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">p</span><span class="mord mathrm">∥</span><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mord mspace negativethinspace"></span></span></span></span></span></p><p><a class="tc-tiddlylink-external" href="https://www.wikiwand.com/en/Cross_entropy" rel="noopener noreferrer" target="_blank">https://www.wikiwand.com/en/Cross_entropy</a></p><p>Cross entropy is used in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Machine%20learning">Machine learning</a>, as a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Loss%20function">Loss function</a></p>