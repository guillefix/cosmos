<p><a class="tc-tiddlylink-external" href="file:///home/guillefix/Dropbox/Oxford/Systems%20Biology%20DPhil/Research/Chandra-overview.pdf" rel="noopener noreferrer" target="_blank">The Convex Geometry of Linear Inverse Problems</a>. Seizes <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Simplicity">Simplicity</a> of data to solve underdetermined problem. Provides a general framework to convert notions of simplicity into convex penalty functions, resulting in convex optimization solutions to linear, underdetermined in-verse problems. <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Convex%20optimization">Convex optimization</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Linear%20inverse%20problem">Linear inverse problem</a></p><p>&quot;many interesting signals or models in practice contain few degrees of freedom relative to their ambient dimension&quot;. We describe a model (or object) as <b>simple</b> if it can be written as a nonnegative linear combination of a few elements from an atomic set (for instance, a specified set of functions, vectors, matrices,...). As they explain, examples of simple objects are sparse vectors, and low-rank matrices. These have been shown to be recoverable from very incomplete information:</p><ul><li><a class="tc-tiddlylink-external" href="http://people.ee.duke.edu/~lcarin/01580791.pdf" rel="noopener noreferrer" target="_blank">Robust Uncertainty Principles: Exact Signal Reconstruction From Highly Incomplete Frequency Information</a></li><li><a class="tc-tiddlylink-external" href="http://pages.cs.wisc.edu/~brecht/papers/08.Candes.Recht.MatrixCompletion.pdf" rel="noopener noreferrer" target="_blank">Exact Matrix Completion via Convex Optimization</a></li><li><a class="tc-tiddlylink-external" href="http://statweb.stanford.edu/~donoho/Reports/2004/l1l0approx.pdf" rel="noopener noreferrer" target="_blank">For Most Large Underdetermined Systems of Equations, the Minimal l1-norm Near-Solution
Approximates the Sparsest Near-Solution</a></li><li><a class="tc-tiddlylink-external" href="http://www.signallake.com/innovation/Donoho0406.pdf" rel="noopener noreferrer" target="_blank">Compressed Sensing</a></li></ul><p><strong>Method</strong>: <span style="color:#FAF;">Under suitable conditions the convex hull <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">conv(A)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right:0.03588em;">v</span><span class="mopen">(</span><span class="mord mathit">A</span><span class="mclose">)</span></span></span></span></span> defines the unit ball of a norm, which is called the atomic norm induced by the atomic set <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">A</span></span></span></span></span>. We can then minimize the atomic norm subject to measurement constraints, which results in a convex programming heuristic for recovering simple models given linear measurements.</span></p><p>There is a tradeoff between the complexity of the recovery algorithm and the number of measurements required for recovery</p><h3 class=""><u>Examples</u></h3><p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Linear%20inverse%20problem">Linear inverse problem</a> for application to the Matrix completion problem</p>