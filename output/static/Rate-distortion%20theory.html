<p><em>Theory of <strong>losy</strong> data compression</em></p><p>The description of an arbitrary real number requires an infinite number of bits, so a finite representation of a continuous random variable can never be perfect. How well can we do? To frame the question appropri- ately, it is necessary to <strong>define the “goodness” of a representation of a source</strong>. This is accomplished by defining a distortion measure which is a measure of distance between the random variable and its representation. The basic problem in rate distortion theory can then be stated as follows: Given a source distribution and a distortion measure, what is the minimum expected distortion achievable at a particular rate?</p><p>Consider the problem of <u>representing a single sample from the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Information%20source">source</a></u>. Let the random variable be represented be X and let the representation of X be denoted as X̂(X). If we are given R bits to represent X, the function X̂ can take on 2 R values. The problem is to find the optimum set of values for X̂ (called the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Reconstruction%20point">Reconstruction point</a>s or <em>code points</em>) and the regions that are associated with each value X̂. Optimum means minimizing the expected <em>distortion</em>, with resepect to some <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Distortion%20function">Distortion function</a>, which for real values can be the <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Squared%20error">Squared error</a></p><p>The <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Lloyd%20algorithm">Lloyd algorithm</a> will converge to a <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Local%20minimum">Local minimum</a> of the distortion.</p><p>Instead of quantizing a single random variable, let us assume that we are given <u>a set of n <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#i.i.d.">i.i.d.</a> random variables</u> drawn according to a Gaussian distribution. These random variables are to be represented using nR bits. Since the source is i.i.d., the symbols are independent, and it may appear that the representation of each element is an independent problem to be treated separately. But this is not true, as the results on rate distortion theory will show. We will represent the entire sequence by a single index taking <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mn>2</mn><mrow><mi>n</mi><mi>R</mi></mrow></msup></mrow><annotation encoding="application/x-tex">2^{nR}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathrm">2</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">n</span><span class="mord mathit" style="margin-right:0.00773em;">R</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span> values. This treatment of entire sequences at once achieves a lower distortion for the same rate than independent quantization of the individual samples.</p><hr><p>By combining rate-distortion theory with minimum sufficient statistic theory, we get the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Information%20bottleneck">Information bottleneck</a> principle</p><hr><p><a class="tc-tiddlylink-external" href="https://www.wikiwand.com/en/Rate%E2%80%93distortion_theory" rel="noopener noreferrer" target="_blank">https://www.wikiwand.com/en/Rate%E2%80%93distortion_theory</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=ECpMEAWI_tk&amp;list=PLgKuh-lKre13UNV4ztsWUXciUZ7x_ZDHz&amp;index=2" rel="noopener noreferrer" target="_blank">has connections to unsupervised learning</a> (see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Autoencoder">Autoencoder</a>s for instance
</p>