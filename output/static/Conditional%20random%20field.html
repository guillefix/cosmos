<p>A discriminant <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Markov%20network">Markov network</a>, used for <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Supervised%20learning">Supervised learning</a> tasks.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=GF3iSJkgPbA&amp;index=18&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH" rel="noopener noreferrer" target="_blank">Neural networks [3.1] : Conditional random fields - motivation</a>.</p><p><a class="tc-tiddlylink-external" href="https://www.coursera.org/learn/probabilistic-graphical-models/lecture/UJ1Ke/conditional-random-fields" rel="noopener noreferrer" target="_blank">coursera video</a></p><p><u>Motivation</u>: <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Sequence%20classification">Sequence classification</a>, where the random variables in the sequence aren't i.i.d, in particular, they need not be independent! More generally, CRFs are used when we want to model random variables that have dependencies (<a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Graphical%20model">Graphical model</a>).</p><p><u>Approach of <strong>conditional random field</strong>s (CRF)</u>: <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=GF3iSJkgPbA&amp;index=18&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH#t=2m40s" rel="noopener noreferrer" target="_blank">model joint distribution</a>, i.e. model the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Stochastic%20process">Stochastic process</a> generating the data. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=GF3iSJkgPbA&amp;index=18&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH#t=3m30s" rel="noopener noreferrer" target="_blank">Notation</a></p><h2 class=""><u>Linear chain CRF</u></h2><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PGBlyKtfB74&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=19" rel="noopener noreferrer" target="_blank">Neural networks [3.2] : Conditional random fields - linear chain CRF</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PGBlyKtfB74&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=19#t=2m47s" rel="noopener noreferrer" target="_blank">Definition</a>. The current label prediction depends on the just observed input, and the adjacent inputs in the sequence.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PGBlyKtfB74&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=19#t=7m40s" rel="noopener noreferrer" target="_blank">Informal neural network representation</a></p><h3 class=""><u>Context window</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=G4lnHc2M1CA&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=20" rel="noopener noreferrer" target="_blank">Neural networks [3.3] : Conditional random fields - context window</a>. A context window refers to the set of inputs that affect a particular output label prediction</p><p><u><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=G4lnHc2M1CA&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=20#t=9m10s" rel="noopener noreferrer" target="_blank">Unary and pairwise log-factors</a></u></p><p>Terms that go into the exponential of a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Softmax">Softmax</a>. They are unary, or pairwise, depending on whether these terms depend on one label <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span></span> or on two labels.</p><h3 class=""><u>Partition function</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=fGdXkVv1qNQ&amp;index=21&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH" rel="noopener noreferrer" target="_blank">Neural networks [3.4] : Conditional random fields - computing the partition function</a>, gives rise to the <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.0037em;">α</span></span></span></span></span> and <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05278em;">β</span></span></span></span></span> vectors/tables. These give the partial sum in the partition function, involving all the terms to the left of a given position in the sequence for <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.0037em;">α</span></span></span></span></span>, and to the right for <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05278em;">β</span></span></span></span></span>. Thus they will be useful for things like computing <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Marginal%20probability">Marginal probability</a>es</p><p>Computing these tables is known as the <strong>forward-backward algorithm</strong> for CRFs, respetively. It has other names, such as <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Belief%20propragation">Belief propragation</a> (see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Dynamical%20systems%20on%20networks">Dynamical systems on networks</a>), <strong>sum-product</strong> algo. Actually more stable implementation by working on log space. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=fGdXkVv1qNQ&amp;index=21&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH#t=22m20s" rel="noopener noreferrer" target="_blank">Further trick to make it more numerically stable</a></p><h3 class=""><u>Computing marginals</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=hjkwp-eDwt8&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=22" rel="noopener noreferrer" target="_blank">Neural networks [3.5] : Conditional random fields - computing marginals</a>. For instance, computing the probability of a single label at a given position <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span></span> in the sequence. Uses <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.0037em;">α</span></span></span></span></span> and <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05278em;">β</span></span></span></span></span> values above.</p><h3 class=""><u><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=pQJvX9U-MyE&amp;index=23&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH" rel="noopener noreferrer" target="_blank">Performing classification</a></u></h3><p>At each position <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span></span>, pick label <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">y_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span> with highest marginal probability <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><msub><mi>y</mi><mi>k</mi></msub><mi mathvariant="normal">∣</mi><mrow><mi mathvariant="bold">X</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">p(y_k|\mathbf{X})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord textstyle uncramped"><span class="mord mathbf">X</span></span><span class="mclose">)</span></span></span></span></span>. It turns out that this choice is the one that minimizes the sum of classification errors (generalization error) over the whole sequence, assuming the CRF is the true distribution! See <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=pQJvX9U-MyE&amp;index=23&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH#t=3m20s" rel="noopener noreferrer" target="_blank">proof</a> (see also <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Bayesian%20statistics">Bayesian statistics</a>).</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=pQJvX9U-MyE&amp;index=23&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH#t=11m" rel="noopener noreferrer" target="_blank">The other option: the most probable sequence</a>, can be performed using <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=pQJvX9U-MyE&amp;index=23&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH#t=11m45s" rel="noopener noreferrer" target="_blank">Viterbi decoding algorithm</a></p><h2 class=""><u>Factors, sufficient statistics</u></h2><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=uXV2an9TdJY&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=24" rel="noopener noreferrer" target="_blank">Neural networks [3.7] : Conditional random fields - factors, sufficient statistics and linear CRF</a></p><p>Factors or compatibility function, which use <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Sufficient%20statistic">Sufficient statistic</a>s. Often represent a CRF as a product of factors.</p><h3 class=""><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=uXV2an9TdJY&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=24#t=7m2s" rel="noopener noreferrer" target="_blank">Linear CRF</a> (no hidden units in NNs). It's a log-linear model</h3><h2 class=""><u>Representations</u></h2><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Markov%20network">Markov network</a></u></h3><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Factor%20graph">Factor graph</a></u></h3><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Belief%20propagation">Belief propagation</a></u></h2><h1 class=""><u>Training CRFs</u></h1><h3 class=""><u>Loss function</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=6dpGB60Q1Ts&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=28" rel="noopener noreferrer" target="_blank">Neural networks [4.1] : Training CRFs - loss function</a></p>