<p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Attention%20in%20machine%20learning">Attention in machine learning</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=iyaktBk8AjU" rel="noopener noreferrer" target="_blank">The frontal and parietal cortex: Eye movements and attention</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Predictive%20coding">Predictive coding</a> is related to attention</p><p><a class="tc-tiddlylink-external" href="https://www.ncbi.nlm.nih.gov/pubmed/19186161" rel="noopener noreferrer" target="_blank">The normalization model of attention</a>. Model proposes attention is mostly accomplished by multiplying input by an <strong>attention field</strong>. Furthermore, the propose  a model of attention that incorporates <strong>divisive normalization</strong> (code on paper)</p><dl><dd>Some results are consistent with the appealingly simple proposal that attention increases neuronal responses multiplicatively by applying a fixed response gain factor (McAdams and Maunsell, 1999; Treue and Martinez-Trujillo, 1999), while others are more in keeping with a change in contrast gain (Li and Basso, 2008, Martinez-Trujillo and Treue, 2002; Reynolds et at., 2000) or with effects that are intermediate between response gain and contrast gain changes (Williford and Maunsell, 2006)</dd></dl><p><small>We propose that this computational principle endows the brain with the capacity to increase sensitivity to faint stimuli presented alone and to reduce the impact of task irrelevant distracters when multiple stimuli are presented. </small></p><p>The three basic components of the model are: the stimulation field, the suppressive field, and the attention field</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=nA5LVjAqkt8" rel="noopener noreferrer" target="_blank">https://www.youtube.com/watch?v=nA5LVjAqkt8</a></p>