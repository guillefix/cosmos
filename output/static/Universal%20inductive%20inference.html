<p><em>aka <strong>Universal prediction</strong></em></p><p><a class="tc-tiddlylink-external" href="http://philsci-archive.pitt.edu/14186/" rel="noopener noreferrer" target="_blank">Recent thesis anlyzing Solomonoff prediction, and criticizing it</a>. See also paper that Ard showed me about Solomonoff being equivalent to just the assumption of computability (see email I sent him)</p><p><a class="tc-tiddlylink-external" href="https://ieeexplore.ieee.org/document/720534/" rel="noopener noreferrer" target="_blank">Nice paper about the theory of Universal Prediction using computable measures of complexity!</a> (<small>use Kami to see comments, saved on Google Drive</small>)</p><p>It is based on performing <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Levin%20search">Levin search</a> over candidate answers, to preferentially search simple ones. However, we should combine this with probabilistic heuristics that are learned from past experience (an example of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Incremental%20learning">Incremental learning</a>).</p><p>Developed by <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Ray%20Solomonoff">Ray Solomonoff</a></p><p>See also <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Universal%20probability">Universal probability</a>, <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Universal%20gambling">Universal gambling</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Induction">Induction</a></p><p><a class="tc-tiddlylink-external" href="https://en.wikipedia.org/wiki/Solomonoff's_theory_of_inductive_inference" rel="noopener noreferrer" target="_blank">Solomonoff's theory of inductive inference</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yXkEpzexYIU" rel="noopener noreferrer" target="_blank">Jeff Eldred Introduces Solomonoff Induction</a>: Introduction about <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Induction">Induction</a>, and about <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Baye's%20theorem">Baye's theorem</a>. He then goes into <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Kolmogorov%20complexity">Kolmogorov complexity</a>, and <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yXkEpzexYIU#t=31m05s" rel="noopener noreferrer" target="_blank">its relation to Solomonoff induction</a>, and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Turing%20machine">Turing machine</a>s</p><h2 class=""><u>Principles</u></h2><p><sub> See <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=wMcRMO9ejeM#t=3m32s" rel="noopener noreferrer" target="_blank">Ray Solomonoff paper read by Marcus Hutter - Algorithmic Probability, Heuristic Programming &amp; AGI</a></sub> It is based on:</p><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Occam's%20razor">Occam's razor</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Principle%20of%20multiple%20explanations">Principle of multiple explanations</a></li></ul><p>–&gt;See section 5.2 in Li&amp;Vitanyi's book on Kolmogorov complexity.</p><h2 class=""><u>Convergence theorem</u></h2><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=wMcRMO9ejeM#t=7m30s" rel="noopener noreferrer" target="_blank">Description (vid)</a>. There are apparently problems with self-referentiality, when the true probability distribution governing the bit sequence, depends on the induction machine itself. There are problems with reinforcement learning.</p><hr><p><u><em>More notes</em></u>:</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Bayesian%20statistics">Bayes</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Probability%20theory">Probability theory</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Human%20learning">Human learning</a>, have similitudes.</p><p>&quot;Information packing problem&quot;, Shannon. See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Data%20compression">Data compression</a></p><p>Carnap's probability theory. <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Korzybski">Korzybski</a>-like way of avoiding the problem of verifying probabilistic theories?</p><p>Huffman, Minksy, McCarthy.</p><p>An inductive inference machine (paper where he introduced the idea).</p><p>See also <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Grammar%20learning">Grammar learning</a>.</p><h2 class=""><u>Practical induction</u></h2><p><a class="tc-tiddlylink-external" href="https://www.eng.tau.ac.il/~meir/articles/32%20Universal%20Prediction.pdf" rel="noopener noreferrer" target="_blank">Universal prediction</a></p><h3 class=""><u>Optimal approxiamations</u></h3><p><u>Main idea: Willis' computable schemes (<strong>Resource bounded algorithmic probability</strong>)</u></p><blockquote class="tc-quote"><p>In 1968 I was asked to review &quot;Computational Complexity and Probability
Constructions
a paper by David Willis. It was the first substantive response I'd
seen to my 1964 paper giving the four models. I found his paper to be excellent
Willis avoided the &quot;halting problem&quot; by defining computationally limited Turing
machines that had no halting problems. From these machines, he was able to
define probabilities in an exact manner. By raising the computational limits on
his machines, he approached the behavior of universal machines.</p><p>In addition to its value as a rigorous treatment of a subject that I had
treated relatively informally, I was able to use Willis' results to prove that these
induction methods satisfied my &quot;correctness&quot; criterion. The methods converged
surprisingly rapidly to the correct probability values</p></blockquote><p>&quot;While Willis' work seemed closer to practical realization, Levin's was a model of mathematical elegance. &quot;</p><p><u>Algorithm</u></p><p><img src="https://s14.postimg.org/uoc5hjmm9/Selection_593.png"></p><p>These kinds of approximations are used to defin <strong>practical probability</strong>. Our particular algorithm/implementation will depend on:</p><p>These methods are optimal approximations. (&quot;By getting as large a value of the sum as we can, we get as close as possible to Algorithmic Probability in the allotted time.&quot;)</p><h3 class=""><u>Suboptimal approximations</u></h3><p>An example is &quot;The VH method&quot;, by Van Herdeen, based on the description of Boolean functions that predict binary strings. This is like just taking the first term in the sum that defines algorithmic probability. Thus it is a rather crude approximation.</p><p>Other methods:</p><p>Minimal message length</p><p>Minimal description length. Stochastic complexity.</p><p>More.</p><h3 class=""><u>Computability vs completeness</u></h3><p><img src="https://s9.postimg.org/m4dtwi9sf/Selection_591.png"></p><h3 class=""><u>Algorithmic probability (i.e. universal inductive inference) as the &quot;gold standard&quot; to <strong>evaluate the utility of induction methods</strong>!</u> (see also <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Universal%20AI">Universal AI</a>!)</h3><p><img src="https://s14.postimg.org/9mbw04hq9/Selection_592.png"></p><p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Randomness">Randomness</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Complexity">Complexity</a></p><p><u>Prediction by <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Compression">Compression</a></u></p><p>It is not a
priori clear that the shortest program dominates in all cases—and in fact
it does not. However, we show that in the overwhelming majority of cases
the shortest program dominates sufficiently to validate the approach that
uses only shortest programs for prediction. (sec 5.2.4 in Li&amp;Vitanyi's book)</p>