<p>&lt;p&gt;A &lt;strong&gt;minimal sufficient statistic&lt;/strong&gt; is a function of every other sufficient statistic. &lt;/p&gt;&lt;p&gt;The notion of minimal sufficient statistics was introduced by Lehmann and Scheff´e (Lehmann and Scheff´e, 1950) as the simplest sufficient statistics, or the coarsest sufficient partition of the sample space which captures the relevant components of the sample with respect to the parameter.&lt;/p&gt;&lt;p&gt;Pitman-Koopman-Darmois theorem showed that exact sufficient statistics with bounded dimensionality exist only for distributions of exponential form (Koompan, 1936).&lt;/p&gt;&lt;p&gt;&lt;small&gt;Kullback and Leibler (Kullback and Leibler) related suf- ficiency to Shannon’s information theory, showing that suf- ficiency is equivalent to preserving mutual information on the parameter, while minimal sufficient statistics minimize the mutual information with the sample due to the dataprocessing inequality (Cover and Thomas, 1991)&lt;/small&gt;. The &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Information%20bottleneck&quot;&gt;Information bottleneck&lt;/a&gt; (IB) method, introduced in (Tishby, Pereira and Bialek, 1999), is an information theoretic generalization of the minimal-sufficient-statistic concept to general distributions of two variables, X and Y .&lt;/p&gt;</p>