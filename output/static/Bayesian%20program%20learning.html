<p>&lt;p&gt;A type of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Program%20induction&quot;&gt;Program induction&lt;/a&gt;, that can be used for &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#One-shot%20learning&quot;&gt;One-shot learning&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://cims.nyu.edu/~brenden/LakeEtAl2015Science.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Human-level concept learning through probabilistic program induction&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Concepts are represented as simple probabilistic programs—that is, probabilistic generative models expressed as structured procedures in an abstract description language (17,18). Our framework brings together three key ideas—compositionality, causality, and learning to learn—that have been separately influ-ential in cognitive science and machine learning over the past several decades (19–22).&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Causality&quot;&gt;Causality&lt;/a&gt;&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;naturally captures the abstract “causal” structure of the real-world processes that produce examples of a category. &lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Learning%20to%20learn&quot;&gt;Learning to learn&lt;/a&gt;&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;Learning proceeds by constructing programs that best explain the observations under a &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Bayesian%20statistics&quot;&gt;Bayesian&lt;/a&gt; criterion, and the model “learns to learn” (23,24)  by developing hierarchical priors that allow previous experience with related concepts to ease learning of new concepts (25,26). These priors represent a learned inductive bias (27) that abstracts the key regularities and dimensions of variation holding across both types of concepts and across instances (or tokens) of a concept in a given domain. &lt;/p&gt;&lt;p&gt;In short, BPL can construct new programs by reusing the pieces of existing ones, capturing the causal and compositional properties of real-world generative processes operating on multiple scales. &lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;BPL&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;BPL defines a generative model that can sam-ple new types of concepts (an“A,”“B,”etc.) by combining parts and subparts in new ways. Each new type is also represented as a genera-tive model, and this lower-level generative model produces new examples (or tokens) of the con-cept (Fig. 3A, v), making BPL a generative model for generative models. The final step renders the token-level variables in the format of the raw data&lt;/p&gt;&lt;p&gt;Could we decode representations structurally similar to those in BPL from brain imaging of premotor cortex (or otheraction-oriented regions) in humans perceiving and classifying new char-acters for the first time?&lt;/p&gt;</p>