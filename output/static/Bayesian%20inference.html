<p>&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://approximateinference.org/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;http://approximateinference.org/&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Introduction&lt;/u&gt;&lt;/h2&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Method&lt;/u&gt;&lt;/h2&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr class=&quot;evenRow&quot;&gt;&lt;td&gt;Likelihood + prior – &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Bayes'%20theorem&quot;&gt;Bayes' theorem&lt;/a&gt; –&amp;gt; posterior&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;ol&gt;&lt;li&gt;Define variables&lt;/li&gt;&lt;li&gt;Define &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Probabilistic%20model&quot;&gt;Probabilistic model&lt;/a&gt; that we are going to consider.&lt;/li&gt;&lt;li&gt;We first choose a &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Prior%20distribution&quot;&gt;Prior distribution&lt;/a&gt; over the set of hypotheses, for instance favouring simple ones (see regularization below), which defines the parametrized family of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Likelihood%20function&quot;&gt;Likelihood function&lt;/a&gt;s&lt;/li&gt;&lt;li&gt;We then &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;amp;list=PLA89DCFA6ADACE599&amp;amp;index=11#t=5m&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;calculate posterior from prior&lt;/a&gt; using &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Bayes'%20theorem&quot;&gt;Bayes' theorem&lt;/a&gt;&lt;/li&gt;&lt;li&gt;And we can then &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;amp;list=PLA89DCFA6ADACE599&amp;amp;index=11#t=6m07s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;make a new prediction&lt;/a&gt; by weighting over all hypothesis to calculate the &lt;strong&gt;expected value&lt;/strong&gt; of the output for a new input. I think one can show (see Elements of statistical learning book) that if we knew the real distribution of output given input, the expectation value is the prediction that minimizes the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Generalization%20error&quot;&gt;Generalization error&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The last two steps &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;amp;list=PLA89DCFA6ADACE599&amp;amp;index=11#t=8m20s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;are often computationally very difficult&lt;/a&gt;. So, &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;amp;list=PLA89DCFA6ADACE599&amp;amp;index=11#t=9m05s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;what's commonly done&lt;/a&gt; is maximizing the posterior distribution (MAP principle, above).&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;Posteriors summaries&lt;/u&gt;&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Point summaries.&lt;ul&gt;&lt;li&gt;Posterior mean (gives less expected error).&lt;/li&gt;&lt;li&gt;Posterior &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Median&quot;&gt;Median&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Posterior mode (&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Maximum%20a%20posteriori&quot;&gt;Maximum a posteriori&lt;/a&gt;)&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Interval summaries. Prefer estimates incorporating uncertainty over point estimates.&lt;ul&gt;&lt;li&gt;&lt;em&gt;Credible intervals&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Central%20posterior%20interval&quot;&gt;Central posterior interval&lt;/a&gt; (CPI)&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Highest%20density%20region&quot;&gt;Highest density region&lt;/a&gt;/interval (HDI). Useful if avoiding nonsensical (low density) regions is important&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Depending on the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Loss%20function&quot;&gt;Loss function&lt;/a&gt;, different choices may be optimal, as studied by &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Decision%20theory&quot;&gt;Decision theory&lt;/a&gt;. However, generally prefer posterior mean or median over MAP.&lt;/p&gt;&lt;p&gt;&lt;u&gt;Ways of dealing with the problem of integrating prior to find normalization&lt;/u&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Conjugate%20prior&quot;&gt;Conjugate prior&lt;/a&gt;s are particular choices of prior distributioj which give posterior distributions which are analytically integrable.&lt;/li&gt;&lt;li&gt;Discretize Baye's rule.&lt;/li&gt;&lt;li&gt;Sampling&lt;/li&gt;&lt;/ul&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Sampling&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://benlambertdotcom.files.wordpress.com/2016/05/bayesian-course-4-v1-handout.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;slides&lt;/a&gt;. Often, we can't calculate the posterior distritbution directly, and so we &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Sampling&quot;&gt;sample&lt;/a&gt;, using &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Monte%20Carlo&quot;&gt;Monte Carlo&lt;/a&gt; methods (basically just sampling methods).&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Rejection sampling&lt;/strong&gt;, creates independent samples, but it becomes increasingly inefficient as dimension increases (one example of the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Curse%20of%20dimensionality&quot;&gt;Curse of dimensionality&lt;/a&gt;).&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Dependent sampling&lt;/strong&gt;. A sampling algorithm where the next sample depends on the current value.&amp;quot;&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Markov%20chain%20Monte%20Carlo&quot;&gt;Markov chain Monte Carlo&lt;/a&gt;. Where to step next is determined via a distribution conditional on the current parameter value (1st order &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Markov%20chain&quot;&gt;Markov chain&lt;/a&gt;). We want to choose starting position, and conditional sampling distribution so that the distribution converges to the posterior.&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Metropolis%20algorithm&quot;&gt;Metropolis algorithm&lt;/a&gt;. Random walk Metropolis. Under quite general conditions the Random Walk Metropolis sampler converges asymptotically to the posterior. &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Ergodic%20theorem&quot;&gt;Ergodic theorem&lt;/a&gt;... We move based the ratio of the proposed un-normalised posterior to our current location =&amp;gt; no need to calculate troublesome denominator. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=pHsuIaPbNbY&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Efficient Bayesian inference with Hamiltonian Monte Carlo -- Michael Betancourt (Part 1)&lt;/a&gt;. To check for convergence, multiple walkers are used (Multiple chain convergence monitoring). Still the measure to use isn't clear. Gelman and Rubin (1992) had the idea of comparing within-chain to between-chain variability. Dependence &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo&gt;↑&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\uparrow&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.69444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:0.8888799999999999em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mrel&quot;&gt;↑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; =&amp;gt; Effective sample size &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo&gt;↓&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\downarrow&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.69444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:0.8888799999999999em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mrel&quot;&gt;↓&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;ul&gt;&lt;li&gt;Metropolis-Hastings. See &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://benlambertdotcom.files.wordpress.com/2016/05/bayesian-course-5-vfinal-v2-handout.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;. Help with uniform convergence near boundaries. For unconstrained parameters we are free to use symmetric jumping kernels. However for constrained parameters we are forced to break this symmetry.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Gibbs%20sampling&quot;&gt;Gibbs sampling&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Hamiltonian%20Monte%20Carlo&quot;&gt;Hamiltonian Monte Carlo&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Hierarchical models&lt;/u&gt;&lt;/h2&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Differential%20equations&quot;&gt;Differential equations&lt;/a&gt; models&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;Estimating ODE/PDE parameters. Add random noise around DE solution&lt;/p&gt;&lt;p&gt;Can use random walk &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Metropolis-Hastings%20algorithm&quot;&gt;Metropolis-Hastings algorithm&lt;/a&gt;..&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Approximate%20Bayesian%20computation&quot;&gt;Approximate Bayesian computation&lt;/a&gt;&lt;/u&gt;&lt;/h2&gt;&lt;hr&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Posterior predictive distribution&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;from &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\theta | X&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.75em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.02778em;&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;mord mathrm&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; to &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/mrow&gt;&lt;mo&gt;~&lt;/mo&gt;&lt;/mover&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\tilde{X}|X&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.9201900000000001em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:1.17019em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord accent&quot;&gt;&lt;span class=&quot;vlist&quot;&gt;&lt;span style=&quot;top:0em;&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:0em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord textstyle cramped&quot;&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-0.60233em;margin-left:0.16668em;&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:0em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;accent-body&quot;&gt;&lt;span&gt;~&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;baseline-fix&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:0em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;​&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord mathrm&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. Find probability distribution over new observations by marginalizing over posterior &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/mrow&gt;&lt;mo&gt;~&lt;/mo&gt;&lt;/mover&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/mrow&gt;&lt;mo&gt;~&lt;/mo&gt;&lt;/mover&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo separator=&quot;true&quot;&gt;,&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;P(\tilde{X}|X) = \sum_{\theta} P(\tilde{X}|\theta, X)P(\theta|X)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.9201900000000001em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:1.2202000000000002em;vertical-align:-0.30001em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord accent&quot;&gt;&lt;span class=&quot;vlist&quot;&gt;&lt;span style=&quot;top:0em;&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:0em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord textstyle cramped&quot;&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-0.60233em;margin-left:0.16668em;&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:0em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;accent-body&quot;&gt;&lt;span&gt;~&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;baseline-fix&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:0em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;​&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord mathrm&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mop&quot;&gt;&lt;span class=&quot;op-symbol small-op mop&quot; style=&quot;top:-0.0000050000000000050004em;&quot;&gt;∑&lt;/span&gt;&lt;span class=&quot;vlist&quot;&gt;&lt;span style=&quot;top:0.30001em;margin-right:0.05em;margin-left:0em;&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:0em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;reset-textstyle scriptstyle cramped&quot;&gt;&lt;span class=&quot;mord scriptstyle cramped&quot;&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.02778em;&quot;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;baseline-fix&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:0em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;​&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord accent&quot;&gt;&lt;span class=&quot;vlist&quot;&gt;&lt;span style=&quot;top:0em;&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:0em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord textstyle cramped&quot;&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-0.60233em;margin-left:0.16668em;&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:0em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;accent-body&quot;&gt;&lt;span&gt;~&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;baseline-fix&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:0em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;​&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord mathrm&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.02778em;&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;mpunct&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.02778em;&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;mord mathrm&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.07847em;&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://ben-lambert.com/bayesian-lecture-slides/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Lecture course&lt;/a&gt; - &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://benlambertdotcom.files.wordpress.com/2016/05/bayesian-course-1-vfinal-vfinal.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;notes pdf&lt;/a&gt;. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://ben-lambert.com/bayesian-lecture-slides/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;notes2&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.wikiwand.com/en/Bayesian_inference&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://www.wikiwand.com/en/Bayesian_inference&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Bayesian%20inference%20exercises&quot;&gt;Bayesian inference exercises&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;As exemplified by &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Gaussian%20process&quot;&gt;Gaussian process&lt;/a&gt;es, one can also apply &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Bayes'%20theorem&quot;&gt;Bayes' theorem&lt;/a&gt; by modeling the joint Data + parameter (or thing to be inferred) distribution, which appears in the numerator.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Bayes: what's the optimal predictor for a given prior. What is the optimal prior?
&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Learning%20theory&quot;&gt;Learning theory&lt;/a&gt;: is your prior good enough for the data you have? What is a good enough predictor?&lt;/p&gt;</p>