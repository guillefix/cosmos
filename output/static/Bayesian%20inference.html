<p><a class="tc-tiddlylink-external" href="http://approximateinference.org/" rel="noopener noreferrer" target="_blank">http://approximateinference.org/</a></p><h2 class=""><u>Introduction</u></h2><h2 class=""><u>Method</u></h2><table><tbody><tr class="evenRow"><td>Likelihood + prior – <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Bayes'%20theorem">Bayes' theorem</a> –&gt; posterior</td></tr></tbody></table><ol><li>Define variables</li><li>Define <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Probabilistic%20model">Probabilistic model</a> that we are going to consider.</li><li>We first choose a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Prior%20distribution">Prior distribution</a> over the set of hypotheses, for instance favouring simple ones (see regularization below), which defines the parametrized family of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Likelihood%20function">Likelihood function</a>s</li><li>We then <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=5m" rel="noopener noreferrer" target="_blank">calculate posterior from prior</a> using <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Bayes'%20theorem">Bayes' theorem</a></li><li>And we can then <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=6m07s" rel="noopener noreferrer" target="_blank">make a new prediction</a> by weighting over all hypothesis to calculate the <strong>expected value</strong> of the output for a new input. I think one can show (see Elements of statistical learning book) that if we knew the real distribution of output given input, the expectation value is the prediction that minimizes the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Generalization%20error">Generalization error</a></li></ol><p>The last two steps <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=8m20s" rel="noopener noreferrer" target="_blank">are often computationally very difficult</a>. So, <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;list=PLA89DCFA6ADACE599&amp;index=11#t=9m05s" rel="noopener noreferrer" target="_blank">what's commonly done</a> is maximizing the posterior distribution (MAP principle, above).</p><h3 class=""><u>Posteriors summaries</u></h3><ul><li>Point summaries.<ul><li>Posterior mean (gives less expected error).</li><li>Posterior <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Median">Median</a></li><li>Posterior mode (<a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Maximum%20a%20posteriori">Maximum a posteriori</a>)</li></ul></li><li>Interval summaries. Prefer estimates incorporating uncertainty over point estimates.<ul><li><em>Credible intervals</em></li><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Central%20posterior%20interval">Central posterior interval</a> (CPI)</li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Highest%20density%20region">Highest density region</a>/interval (HDI). Useful if avoiding nonsensical (low density) regions is important</li></ul></li></ul><p>Depending on the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Loss%20function">Loss function</a>, different choices may be optimal, as studied by <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Decision%20theory">Decision theory</a>. However, generally prefer posterior mean or median over MAP.</p><p><u>Ways of dealing with the problem of integrating prior to find normalization</u></p><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Conjugate%20prior">Conjugate prior</a>s are particular choices of prior distributioj which give posterior distributions which are analytically integrable.</li><li>Discretize Baye's rule.</li><li>Sampling</li></ul><h2 class=""><u>Sampling</u></h2><p><a class="tc-tiddlylink-external" href="https://benlambertdotcom.files.wordpress.com/2016/05/bayesian-course-4-v1-handout.pdf" rel="noopener noreferrer" target="_blank">slides</a>. Often, we can't calculate the posterior distritbution directly, and so we <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Sampling">sample</a>, using <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Monte%20Carlo">Monte Carlo</a> methods (basically just sampling methods).</p><ul><li><strong>Rejection sampling</strong>, creates independent samples, but it becomes increasingly inefficient as dimension increases (one example of the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Curse%20of%20dimensionality">Curse of dimensionality</a>).</li><li><strong>Dependent sampling</strong>. A sampling algorithm where the next sample depends on the current value.&quot;<ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Markov%20chain%20Monte%20Carlo">Markov chain Monte Carlo</a>. Where to step next is determined via a distribution conditional on the current parameter value (1st order <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Markov%20chain">Markov chain</a>). We want to choose starting position, and conditional sampling distribution so that the distribution converges to the posterior.<ul><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Metropolis%20algorithm">Metropolis algorithm</a>. Random walk Metropolis. Under quite general conditions the Random Walk Metropolis sampler converges asymptotically to the posterior. <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Ergodic%20theorem">Ergodic theorem</a>... We move based the ratio of the proposed un-normalised posterior to our current location =&gt; no need to calculate troublesome denominator. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=pHsuIaPbNbY" rel="noopener noreferrer" target="_blank">Efficient Bayesian inference with Hamiltonian Monte Carlo -- Michael Betancourt (Part 1)</a>. To check for convergence, multiple walkers are used (Multiple chain convergence monitoring). Still the measure to use isn't clear. Gelman and Rubin (1992) had the idea of comparing within-chain to between-chain variability. Dependence <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>↑</mo></mrow><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mrel">↑</span></span></span></span></span> =&gt; Effective sample size <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>↓</mo></mrow><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mrel">↓</span></span></span></span></span><ul><li>Metropolis-Hastings. See <a class="tc-tiddlylink-external" href="https://benlambertdotcom.files.wordpress.com/2016/05/bayesian-course-5-vfinal-v2-handout.pdf" rel="noopener noreferrer" target="_blank">here</a>. Help with uniform convergence near boundaries. For unconstrained parameters we are free to use symmetric jumping kernels. However for constrained parameters we are forced to break this symmetry.</li></ul></li></ul></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Gibbs%20sampling">Gibbs sampling</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Hamiltonian%20Monte%20Carlo">Hamiltonian Monte Carlo</a></li></ul></li></ul><h2 class=""><u>Hierarchical models</u></h2><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Differential%20equations">Differential equations</a> models</u></h2><p>Estimating ODE/PDE parameters. Add random noise around DE solution</p><p>Can use random walk <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Metropolis-Hastings%20algorithm">Metropolis-Hastings algorithm</a>..</p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Approximate%20Bayesian%20computation">Approximate Bayesian computation</a></u></h2><hr><h2 class=""><u>Posterior predictive distribution</u></h2><p>from <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi><mi mathvariant="normal">∣</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">\theta | X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span></span></span> to <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mrow><mi>X</mi></mrow><mo>~</mo></mover><mi mathvariant="normal">∣</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">\tilde{X}|X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.9201900000000001em;"></span><span class="strut bottom" style="height:1.17019em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span><span style="top:-0.60233em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>~</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span></span></span>. Find probability distribution over new observations by marginalizing over posterior <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mover accent="true"><mrow><mi>X</mi></mrow><mo>~</mo></mover><mi mathvariant="normal">∣</mi><mi>X</mi><mo>)</mo><mo>=</mo><msub><mo>∑</mo><mrow><mi>θ</mi></mrow></msub><mi>P</mi><mo>(</mo><mover accent="true"><mrow><mi>X</mi></mrow><mo>~</mo></mover><mi mathvariant="normal">∣</mi><mi>θ</mi><mo separator="true">,</mo><mi>X</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>θ</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(\tilde{X}|X) = \sum_{\theta} P(\tilde{X}|\theta, X)P(\theta|X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.9201900000000001em;"></span><span class="strut bottom" style="height:1.2202000000000002em;vertical-align:-0.30001em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span><span style="top:-0.60233em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>~</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mrel">=</span><span class="mop"><span class="op-symbol small-op mop" style="top:-0.0000050000000000050004em;">∑</span><span class="vlist"><span style="top:0.30001em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span><span style="top:-0.60233em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>~</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span></span>.</p><hr><p><a class="tc-tiddlylink-external" href="https://ben-lambert.com/bayesian-lecture-slides/" rel="noopener noreferrer" target="_blank">Lecture course</a> - <a class="tc-tiddlylink-external" href="https://benlambertdotcom.files.wordpress.com/2016/05/bayesian-course-1-vfinal-vfinal.pdf" rel="noopener noreferrer" target="_blank">notes pdf</a>. <a class="tc-tiddlylink-external" href="https://ben-lambert.com/bayesian-lecture-slides/" rel="noopener noreferrer" target="_blank">notes2</a></p><hr><p><a class="tc-tiddlylink-external" href="https://www.wikiwand.com/en/Bayesian_inference" rel="noopener noreferrer" target="_blank">https://www.wikiwand.com/en/Bayesian_inference</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Bayesian%20inference%20exercises">Bayesian inference exercises</a></p><hr><p>As exemplified by <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Gaussian%20process">Gaussian process</a>es, one can also apply <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Bayes'%20theorem">Bayes' theorem</a> by modeling the joint Data + parameter (or thing to be inferred) distribution, which appears in the numerator.</p><hr><p>Bayes: what's the optimal predictor for a given prior. What is the optimal prior?
<a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Learning%20theory">Learning theory</a>: is your prior good enough for the data you have? What is a good enough predictor?</p>