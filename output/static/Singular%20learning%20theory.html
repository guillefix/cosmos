<p>A statistical model or a learning machine is called regular if the map taking a parameter to a probability distribution is one-to-one and if its Fisher information matrix is always positive definite. If otherwise, it is called singular. In regular statistical models, the Bayes free energy, which is defined by the minus logarithm of Bayes marginal likelihood, can be asymptotically approximated by the Schwarz Bayes information criterion (BIC), whereas in singular models such approximation does not hold. Uses tools of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Algebraic%20geometry">Algebraic geometry</a></p><p><a class="tc-tiddlylink-external" href="http://www.jmlr.org/papers/v14/watanabe13a.html" rel="noopener noreferrer" target="_blank">A Widely Applicable Bayesian Information Criterion</a></p><p><a class="tc-tiddlylink-external" href="http://www.mitpressjournals.org/doi/pdf/10.1162/089976601300014402" rel="noopener noreferrer" target="_blank">Algebraic Analysis for Nonidentifiable Learning Machines</a>, <a class="tc-tiddlylink-external" href="https://link.springer.com/chapter/10.1007/3-540-46769-6_4" rel="noopener noreferrer" target="_blank">Algebraic Analysis for Singular Statistical Estimation</a></p><p><a class="tc-tiddlylink-external" href="http://www.jmlr.org/papers/v11/watanabe10a.html" rel="noopener noreferrer" target="_blank">Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory</a></p><p><a class="tc-tiddlylink-external" href="http://www.sciencedirect.com/science/article/pii/S0893608003000054?np=y&amp;npKey=142c3bf066ad1c365c9aa78437002039085af147ca0a88722344eda3e73f0bc4" rel="noopener noreferrer" target="_blank">Singularities in mixture models and upper bounds of stochastic complexity</a></p><p>Algorithm for singular models: <a class="tc-tiddlylink-external" href="http://sci-hub.cc/10.1007/s11063-013-9283-z" rel="noopener noreferrer" target="_blank">http://sci-hub.cc/10.1007/s11063-013-9283-z</a></p><p><a class="tc-tiddlylink-external" href="http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.2006.18.5.1007" rel="noopener noreferrer" target="_blank">Singularities Affect Dynamics of Learning in Neuromanifolds</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1506.05855" rel="noopener noreferrer" target="_blank">Information-based inference in sloppy and singular models</a></p><p><a class="tc-tiddlylink-external" href="http://projecteuclid.org/euclid.aos/1056562464" rel="noopener noreferrer" target="_blank">Likelihood ratio of unidentifiable models and multilayer neural networks</a></p><hr><p><a class="tc-tiddlylink-external" href="http://www.mitpressjournals.org/doi/abs/10.1162/neco.2007.12-06-414" rel="noopener noreferrer" target="_blank">Dynamics of Learning Near Singularities in Layered Networks</a></p><p><a class="tc-tiddlylink-external" href="http://www.sciencedirect.com/science/article/pii/0893608095001190" rel="noopener noreferrer" target="_blank">A Regularity Condition of the Information Matrix of a Multilayer Perceptron Network</a>. The Fisher information matrix of a multi-layer perceptron network can be singular at certain parameters, and in such cases many statistical techniques based on asymptotic theory cannot be applied properly. In this paper, we prove rigorously that the Fisher information matrix of a three-layer perceptron network is positive definite if and only if the network is irreducible; that is, if there is no hidden unit that makes no contribution to the output and there is no pair of hidden units that could be collapsed to a single unit without altering the input-output map. This implies that a network that has a singular Fisher information matrix can be reduced to a network with a positive definite Fisher information matrix by eliminating redundant hidden units. </p><p><a class="tc-tiddlylink-external" href="http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.2006.18.5.1007" rel="noopener noreferrer" target="_blank">Singularities Affect Dynamics of Learning in Neuromanifolds</a></p><p><a class="tc-tiddlylink-external" href="http://ieeexplore.ieee.org/abstract/document/6390480/" rel="noopener noreferrer" target="_blank">Application of the error function in analyzing the learning dynamics near singularities of the multilayer perceptrons</a></p><p><a class="tc-tiddlylink-external" href="http://ieeexplore.ieee.org/abstract/document/7502092/" rel="noopener noreferrer" target="_blank">Resolution of Singularities Introduced by Hierarchical Structure in Deep Neural Networks</a></p><p><a class="tc-tiddlylink-external" href="https://link.springer.com/chapter/10.1007/978-3-319-46681-1_47" rel="noopener noreferrer" target="_blank">On the Singularity in Deep Neural Networks</a></p><p><a class="tc-tiddlylink-external" href="http://www.mitpressjournals.org/doi/abs/10.1162/neco.1993.5.6.910" rel="noopener noreferrer" target="_blank">On the Geometry of Feedforward Neural Network Error Surfaces</a></p>