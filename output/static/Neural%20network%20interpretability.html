<p>&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://distill.pub/2018/building-blocks/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://distill.pub/2018/building-blocks/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Feature%20visualization&quot;&gt;Feature visualization&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Feature%20attribution&quot;&gt;Feature attribution&lt;/a&gt;, saliency map. We do attribution by linear approximation in all of our interfaces. That is, we estimate the effect of a neuron on the output is its activation times the rate at which increasing its activation increases the output. &lt;/p&gt;&lt;hr&gt;&lt;p&gt;There also may exist abstractions which are visually familiar, yet that we lack good natural language descriptions for: for example, take the particular column of shimmering light where sun hits rippling water. Moreover, the network may learn new abstractions that appear alien to us — here, natural language would fail us entirely!&lt;/p&gt;</p>