<p><a class="tc-tiddlylink-external" href="https://distill.pub/2018/building-blocks/" rel="noopener noreferrer" target="_blank">https://distill.pub/2018/building-blocks/</a></p><p><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Feature%20visualization">Feature visualization</a></p><p><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Feature%20attribution">Feature attribution</a>, saliency map. We do attribution by linear approximation in all of our interfaces. That is, we estimate the effect of a neuron on the output is its activation times the rate at which increasing its activation increases the output. </p><hr><p>There also may exist abstractions which are visually familiar, yet that we lack good natural language descriptions for: for example, take the particular column of shimmering light where sun hits rippling water. Moreover, the network may learn new abstractions that appear alien to us — here, natural language would fail us entirely!</p>