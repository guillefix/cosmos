<p>A type of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Unsupervised%20learning">Unsupervised learning</a> where we describe the data using less features (called latent factors) than the data was initially described with.</p><p><a class="tc-tiddlylink-external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.453.8815" rel="noopener noreferrer" target="_blank">Graph embedding and extensions: A general framework for dimensionality reduction</a>. Basically minimize <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mo>∑</mo><mrow><mi>i</mi><mo>≠</mo><mi>j</mi></mrow></msub><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>j</mi></msub><mi mathvariant="normal">∣</mi><msup><mi mathvariant="normal">∣</mi><mrow><mn>2</mn></mrow></msup><msub><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\sum_{i \neq j} ||y_i -y_j||^{2} W_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.264618em;vertical-align:-0.45050999999999997em;"></span><span class="base textstyle uncramped"><span class="mop"><span class="op-symbol small-op mop" style="top:-0.0000050000000000050004em;">∑</span><span class="vlist"><span style="top:0.30001em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mrel">≠</span><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord mathrm">∣</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord"><span class="mord mathrm">∣</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">2</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span></p><p>See also <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Feature%20learning">Feature learning</a>, which is very similar.</p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Factor%20analysis%20model">Factor analysis model</a></u></h3><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Linear%20discriminant%20analysis">Linear discriminant analysis</a></u></h3><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Principal%20component%20analysis">Principal component analysis</a></u></h3><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Multidimensional%20scaling">Multidimensional scaling</a></u></h3><p><a class="tc-tiddlylink-external" href="https://en.wikipedia.org/wiki/Multidimensional_scaling" rel="noopener noreferrer" target="_blank">https://en.wikipedia.org/wiki/Multidimensional_scaling</a></p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Locality%20preserving%20projection">Locality preserving projection</a></u></h3><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Manifold%20learning">Manifold learning</a></u></h3><p><a class="tc-tiddlylink-external" href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Laplacian_eigenmaps" rel="noopener noreferrer" target="_blank">https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Laplacian_eigenmaps</a></p><hr><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Nonparametric%20statistics">Non-parametric models</a> are suitable especially for a scenario that all the data points in the source space are known or available and the embedding task needs to be undertaken on a given data set without the need of extension to unseen data points during learning. This is a salient characteristic that distinguishes between parametric and non-parametric subspace learning. As a typical non-parametric subspace learning framework, multi-dimensional scaling (MDS) (Cox and Cox 2000) refers to a family of algorithms that learn embedding a set of given high-dimensional data points into a low-dimensional subspace by preserving the distance information between data points in the high-dimensional space. Sammon mapping (Sammon 1969) is an effective non-linear MDS algorithm.</p><p>The fact that it works is related to the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Sloppy%20systems">Sloppy systems</a> and the <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Manifold%20hypothesis">Manifold hypothesis</a>, and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Simplicity%20bias">Simplicity bias</a></p><hr><h3 class=""><u>Incremental algorithms (<a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Online%20learning">Online learning</a>)</u></h3><p><a class="tc-tiddlylink-external" href="http://www.sciencedirect.com/science/article/pii/S016786550900213X?np=y&amp;npKey=a7379a552798ed3cc17a1cfb6ef118a83b9304ec725b827b3449a6898cc8f8bc" rel="noopener noreferrer" target="_blank">Incremental Laplacian eigenmaps by preserving adjacent information between data points</a></p><p><a class="tc-tiddlylink-external" href="http://www.sciencedirect.com/science/article/pii/S0167865511001048?np=y&amp;npKey=a7379a552798ed3cd01dc0579657be210f27d523e4ac61e53b81eb388b94a047" rel="noopener noreferrer" target="_blank">Incremental manifold learning by spectral embedding methods</a></p><p><a class="tc-tiddlylink-external" href="http://www.sciencedirect.com/science/article/pii/S0031320313002732" rel="noopener noreferrer" target="_blank">Embedding new observations via sparse-coding for non-linear manifold learning</a></p><p><a class="tc-tiddlylink-external" href="https://link.springer.com/chapter/10.1007/978-3-319-46182-3_5" rel="noopener noreferrer" target="_blank">Incremental Construction of Low-Dimensional Data Representations</a></p><p>A New Manifold Learning Algorithm Based on Incremental Spectral Decomposition</p><hr><p><a class="tc-tiddlylink-external" href="https://link.springer.com/article/10.1007/s13735-015-0079-y" rel="noopener noreferrer" target="_blank">Learning to detect concepts with Approximate Laplacian Eigenmaps in large-scale and online settings</a></p>