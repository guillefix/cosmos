<p>Found in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Residual%20neural%20network">Residual neural network</a>s, and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Highway%20network">Highway network</a>s.</p><p>See connections to <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Spiking%20neural%20network">Spiking neural network</a>s <a class="tc-tiddlylink-external" href="https://gingkoapp.com/app#7887cb3f9b23d73d7600001f" rel="noopener noreferrer" target="_blank">here</a>. See also <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Time-delay%20neural%20network">Time-delay neural network</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1604.03640" rel="noopener noreferrer" target="_blank">Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=x1kf4Zojtb0#t=16m52.5s" rel="noopener noreferrer" target="_blank">Skip connections</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1701.09175" rel="noopener noreferrer" target="_blank">Skip Connections as Effective Symmetry-Breaking</a>. We argue that skip connections help break symmetries inherent in the loss landscapes of deep networks, leading to drastically simplified landscapes. We find, however, that skip connections confer additional benefits over and above symmetry-breaking, such as the ability to deal effectively with the vanishing gradients problem.</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1612.07771.pdf" rel="noopener noreferrer" target="_blank">HIGHWAY AND RESIDUAL NETWORKS LEARN UNROLLED ITERATIVE ESTIMATION</a></p><p><a class="tc-tiddlylink-external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.pdf" rel="noopener noreferrer" target="_blank">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</a> . Contextual information outside
the region of interest is integrated using spatial recurrent
neural networks. Inside, we use skip pooling to extract
information at multiple scales and levels of abstraction – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=-QNDu_8Q7_A&amp;feature=youtu.be" rel="noopener noreferrer" target="_blank">video</a> –<a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=-QNDu_8Q7_A&amp;feature=youtu.be#t=2m" rel="noopener noreferrer" target="_blank">SKIP CONNECTIONS -- what and where</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1611.05552" rel="noopener noreferrer" target="_blank">DelugeNets: Deep Networks with Massive and Flexible Cross-layer Information Inflows</a>
 – <a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1608.06993.pdf" rel="noopener noreferrer" target="_blank">Densely Connected Convolutional Networks</a>
 – <a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1411.5752.pdf" rel="noopener noreferrer" target="_blank">Hypercolumns for Object Segmentation and Fine-grained Localization</a> – </p><p>See applications in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Image%20segmentation">Image segmentation</a>, and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Object%20detection">Object detection</a></p><p>I think skip-connections can simulate <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Polychronization">Polychronization</a></p><p><a class="tc-tiddlylink-external" href="https://pdfs.semanticscholar.org/1a6b/8d832037f1565f6b81e3743fdacd227c6009.pdf" rel="noopener noreferrer" target="_blank">Recurrent Residual Learning for Sequence Classification</a> – We show that
for sequence classification tasks, incorporating
residual connections into recurrent structures
yields similar accuracy to Long Short
Term Memory (LSTM) RNN with much fewer
model parameters. – <a class="tc-tiddlylink-external" href="https://publish.illinois.edu/yirenwang/home/emnlp16source/" rel="noopener noreferrer" target="_blank">Code</a></p><p><a class="tc-tiddlylink-external" href="http://papers.nips.cc/paper/6303-architectural-complexity-measures-of-recurrent-neural-networks.pdf" rel="noopener noreferrer" target="_blank">Architectural complexity of RNNs</a></p><p>Deep transition RNN - <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1312.6026" rel="noopener noreferrer" target="_blank">How to Construct Deep Recurrent Neural Networks</a></p>