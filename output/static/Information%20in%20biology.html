<p><strong>Crick information</strong> and biology, see paper that Ard sent</p><p>Their view of bounded Kolmogorov complexity is about descriptions using the models they consider (instead of LZ as we use)</p><p>If the science corresponded to the popular image, we would expect to nd that instructions written in the genetic code are read by gene regulatory networks to make an organism. But the genetic code runs out of steam when it has partially specied the linear structure of proteins (Godfrey-Smith 2000). The <code>histone codes' and </code>splicing codes' that supplement the genetic code are not integrated with the genetic code through a shared measure of coded information. The additional <code>codes' are simply distinct mechanisms that interact with the </code>codical' mechanisms of transcription and translation only in the straightforward way that any two or more physical mechanisms can interact. Turning our attention to gene regulatory networks, these are productively modelled as computing Boolean functions and/or dierential equations, but these operations are not specied in any of the more basic codes to which we just referred. Instead, these operations are specied by the stereochemical anities of genomic regions and gene products. The science that connects the <code>codes' with the </code>computing networks' is a direct application of thermodynamics, quantum theory, etc., to understand how stereochemical properties emerge from the linear structure of biomolecules and the contexts in which those biomolecules mature and function. The same is true of the other molecular networks that are at the heart of our understanding of the cell when we model these networks as performing computations those formal operations do not take as inputs representations written in the genetic code.</p><p>Griths and Stotz (2013) suggested a bottom-up approach to biological information, which of the Shannon type, and which they call Crick information.</p><p>Crick information is mostly the info in the DNA, butore generally information that is an specific cause, as opposed to background causes. it also includes non coding DNA.</p><p>If a cause makes a specic dierence to the linear sequence of a biomolecule, it is a source of Crick information for that molecule. What about other non-primary structure causes?</p><ul><li>Causal specicity: information as choice between alternatives. Spec: the specicity of a causal variable is obtained by measuring how much mutual information interventions on that variable carry about the eect variable. (Griths et al. 2015, p. 538).</li></ul><p>–&gt; I would find it more useful to look at causes that vary most often (have highest entropy). oh it is included in their measure: How muchthecause species the eect will not just depend on the details of the causal connection between the two variables (which we represented as a mapping in Fig. 2), but also on which causal values are probable or not.</p><p>The Shannon entropy of a source emitting sequences of length l asymptotically tends towards the expected Kolmogorov complexity of the sequences as l → ∞. Speaking loosely, potential causal specicity and expected Kolmogorov complexity go hand in hand. We leave to future work to make this connection more explicit (see Grünwald and Vitányi 2003, p. 518; Balduzzi 2011; Li and Vitanyi 2013, p. 142)</p><p>how much information is there in a given biological object?, </p><p>and the second, closer to Crick's thinking, is: how much information about one object comes from another? We can use mutual algorithmic information to see how much stuff that is needed for the output is found on the input.</p><p>–randomness de novo creates information. In a sense, this is true by denition: if an event is not random, then information about this event pre-existed the event. In biological terms, this means that any random mutation, any error of transcription, any random splicing event, creates information in the Kolmogorov sense, with high probability.</p><p>Non-Turing-Universality of cell lamguage means that not all possible strings will be produced, and that there are asymmetries between cause and effect.</p><p>For instance, if dna translocation is frequent, then an algorithm that pays due attention to translocation should be more likely to compress a dna-sequence. –  It is highly unlikely that the cell will, by chance, produce a string which is compressible by other means than some of its own means of production (or the corresponding models of these means).</p><hr><p>Biomolecules can be assembled in a process of <code>molecular epigenesis' in which dierent sources of information are brought together in a regulated manner (Stotz 2006; Griths and Stotz 2013, 98).

Two complementary approaches: Shannon and Kolmogorov approaches to information are not reducible to one another. The former deals with the choice between a set of bare alternatives and is blind to the internal structure of those alternatives. The latter is concerned precisely with that internal structure, independent of any particular set of alternative structures.

Shannon info deals with sets in general,but Kolmogorov theory specifies some sets which are special, in that they are picked by Turing machines. can describe the sets with a TM that identifies membership. The size of the set to which a string belongs is asymptotically independent of the TM as the string size goes to infinity.

One can go the other way from Kolmogorov to Shannon. Shannon is like Kolmogorov but where we try to put as much Kolmogorov information on the decoder machine for a set of objects.

Descriptional complexity is about the size of things that produce other things (&quot;describe&quot; them). Size often refers to the number of some fundamental building blocks.
Kolmogorov theory is like effective (in the computing sense) Shannon into theory</code></p>