<p>&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/pdf/1306.0543.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Predicting Parameters in Deep Learning&lt;/a&gt; â€“ We demonstrate that there is significant redundancy in the parameterization of
several deep learning models. Given only a few weight values for each feature it
is possible to accurately predict the remaining values. Moreover, we show that not
only can the parameter values be predicted, but many of them need not be learned
at all. We train several different architectures by learning only a small number of
weights and predicting the rest. In the best case we are able to predict more than
95% of the weights of a network without any drop in accuracy.&lt;/p&gt;&lt;p&gt;They find that we can &lt;strong&gt;reparametrize&lt;/strong&gt; the network with fewer parameters, by writting the matrices of the network as a product of two lower rank factors. If this is done well, the network can keep the same performance.&lt;/p&gt;&lt;p&gt;The key to constructing a good first factor is exploiting
smoothness in the structure of the inputs. When we have
prior knowledge of the smoothness structure we expect to
see (e.g. in natural images), we can impose this structure
directly through the choice of factor. When no such prior
knowledge is available we show that it is still possible to
make a good data driven choice.&lt;/p&gt;</p>