<p>&lt;p&gt;See &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Deep%20learning%20theory&quot;&gt;Deep learning theory&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Generalization%20in%20deep%20learning&quot;&gt;Generalization in deep learning&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/pdf/1710.10345.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;THE IMPLICIT BIAS OF GRADIENT DESCENT ON SEPARABLE DATA&lt;/a&gt; â€“  in deep learning we often benefit from implicit bias even when optimizing the (unregularized) training error to convergence, using stochastic or batch methods. For loss functions with attainable, finite, minimizers, such as the squared loss, we have some understanding of this: In particular, when minimizing an underdetermined least squares problem using gradient descent starting from the origin, we know we will &lt;strong&gt;converge to the minimum Euclidean norm solution&lt;/strong&gt;. But the logistic loss, and its generalization the cross-entropy loss which is often used in deep learning, do not admit a finite minimizer on separable problems. Instead, to drive the loss toward zero and thus minimize it, the predictor must diverge toward infinity.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/pdf/1709.01953.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Implicit regularization in deep learning&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;But we didn't tell the network to minimize the path norm (complexity). &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=ACdjYP0-cMw#t=39m5s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;So where is the regularization coming from?&lt;/a&gt;. He thinks it's the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Optimization&quot;&gt;Optimization&lt;/a&gt; algorithm that is biasing us towards simple global optima (work on this for convex opti?), but couldn't it be a &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Genotype-phenotype%20map&quot;&gt;GP map&lt;/a&gt;-like &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Simplicity%20bias&quot;&gt;Simplicity bias&lt;/a&gt;. He approaches it from &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Geometry&quot;&gt;Geometry&lt;/a&gt;. I think he is right in that the algorithm plays a role in biasing. But GP bias probably does also, or it could at least be seized (as in &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Neuroevolution&quot;&gt;Neuroevolution&lt;/a&gt; with indirect encodings..)&lt;/p&gt;&lt;hr&gt;</p>