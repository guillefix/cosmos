<p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0kWZoyNRxTY&amp;index=10&amp;list=PLA89DCFA6ADACE599#t=54m28s" rel="noopener noreferrer" target="_blank">video</a></p><p>In feature selection, we would like to select a subset of the features (input variables to a supervised learning algo) that are the most <strong>relevant</strong> ones for a specific learning problem, so as to get a simpler hypothesis class, and reduce the risk of overfitting. This is useful mostly when we have many features.</p><p>What we do is use heuristics to search the <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0kWZoyNRxTY&amp;index=10&amp;list=PLA89DCFA6ADACE599#t=56m54s" rel="noopener noreferrer" target="_blank">huge space</a>.</p><p><u>&quot;Wrapper&quot; feature selection</u></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0kWZoyNRxTY&amp;index=10&amp;list=PLA89DCFA6ADACE599#t=1h01m47s" rel="noopener noreferrer" target="_blank">vid</a>. Feature selections that repeatedly call your learning algo. Work well, but are computationally expensive.</p><ul><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0kWZoyNRxTY&amp;index=10&amp;list=PLA89DCFA6ADACE599#t=57m30s" rel="noopener noreferrer" target="_blank">Forward search</a></li><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0kWZoyNRxTY&amp;index=10&amp;list=PLA89DCFA6ADACE599#t=1h02m30s" rel="noopener noreferrer" target="_blank">Backward search</a></li></ul><p>Number of features to include can be chosen by optimizing generalization error (estimated by cross-validation), or by chosen a plausible number..</p><p><u>&quot;Filter&quot; feature selection</u></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0kWZoyNRxTY&amp;index=10&amp;list=PLA89DCFA6ADACE599#t=1h05m47s" rel="noopener noreferrer" target="_blank">vid</a>. Less computationally expensive, but often less effective.  For each feature i, we'll compute some measure of how informative x<sub>i</sub> is about y, for instance by computing:</p><ul><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Correlation">Correlation</a> between x<sub>i</sub> and y, or</li><li>Empirical <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Mutual%20information">Mutual information</a></li></ul><h2 class=""><u>Learning meaningful <strong>representations</strong> of the data</u></h2><p>Can learn from <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Unsupervised%20learning">Unsupervised learning</a>, or <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Supervised%20learning">Supervised learning</a> algorithms!</p><p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Transfer%20learning">Transfer learning</a></p><p><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Restricted%20Boltzmann%20machine">Restricted Boltzmann machine</a> feature learning</u></p><p>See <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=S0kFFiHzR8M&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=41#t=0m50s" rel="noopener noreferrer" target="_blank">here</a></p>