<p>&lt;p&gt;The term &amp;quot;perceptron&amp;quot; was introduced in the 1950s to designate
a simple mechanism to achieve &amp;quot;perception.&amp;quot;&lt;/p&gt;&lt;p&gt;See &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=HZ4cvaztQEs&amp;amp;index=3&amp;amp;list=PLA89DCFA6ADACE599#t=1h10m&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;video&lt;/a&gt;. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=_q5twKE9Jeo#t=14m20s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Definition of perceptron&lt;/a&gt; &lt;/p&gt;&lt;p&gt;It is basically a &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Feedforward%20neural%20network&quot;&gt;Feedforward neural network&lt;/a&gt;, with 0 hidden layers, and &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Heaviside%20step%20function&quot;&gt;Heaviside step function&lt;/a&gt; activation functions. They are a simpler version of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Logistic%20regression&quot;&gt;Logistic regression&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.wikiwand.com/en/Perceptron&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://www.wikiwand.com/en/Perceptron&lt;/a&gt;&lt;/p&gt;&lt;p&gt;As most older models, the original perceptrons, had threshold activation functions (see &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Hopfield%20network&quot;&gt;Hopfield network&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;The hopes to build &amp;quot;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Computer%20vision&quot;&gt;seeing machines&lt;/a&gt;&amp;quot; vanished when Minsky and Papert published their
book Perceptrons [1969], in which they rigorously demonstrated that
perceptrons are quite limited in their ability to extract global features
from local information. They could only implement linearly separable classification.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=_q5twKE9Jeo#t=15m50s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Simple storing problem&lt;/a&gt;: is a certain training set linearly separable?&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Multilayer perceptron&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;However, the reaction was too drastic, because adding more layers to the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Feedforward%20neural%20network&quot;&gt;Feedforward neural network&lt;/a&gt; (giving so-called &amp;quot;multilayer perceptrons&amp;quot;), avoided the issues pointed out by Minksy and Papert.&lt;/p&gt;&lt;p&gt;Multilayered perceptrons work essentially in a manner similar
to the prevailing neurophysiological view. According to this view, on
arrival at the cortex, sensory information is subject to a hierarchy of
feature extractions. Further comparison with the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Cortex&quot;&gt;Cortex&lt;/a&gt;, is done in the Corticonics book (pages 200-203)&lt;/p&gt;</p>