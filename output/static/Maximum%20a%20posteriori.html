<p>&lt;p&gt;A learning principle that can be viewed as approximating the expected value of the output from a model, using &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Bayesian%20statistics&quot;&gt;Bayesian statistics&lt;/a&gt;, by only considering the hypothesis with maximum a posteriori probability.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=sQ8T9b-uGVE&amp;amp;list=PLA89DCFA6ADACE599&amp;amp;index=11#t=9m36s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;video&lt;/a&gt; â€“ 
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=bL9Fvz3fx4c&amp;amp;index=37&amp;amp;list=PL50E6E80E8525B59C&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;MAP on graphical models&lt;/a&gt;&lt;/p&gt;&lt;p&gt;It can be seen as formally equivalent to &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Maximum%20likelihood&quot;&gt;Maximum likelihood&lt;/a&gt; by multiplying the likelihood by the prior (adds the log of the prior to the log likelihood).
&lt;/p&gt;</p>