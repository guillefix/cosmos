<p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=8_ywDfr1FGU" rel="noopener noreferrer" target="_blank">Nice video about parallel computing</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=jMfVx4hFHVk" rel="noopener noreferrer" target="_blank">Why we cannot keep increasing CPU speed?</a> Power has emerged as one of the primary factors in processor design.</p><p>Often used in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Computer%20cluster">Computer cluster</a> and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#GPU%20computing">GPU computing</a>. Main application is for <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#High-performance%20computing">High-performance computing</a> (see more there)</p><p>Fundamental concept: <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?time_continue=81&amp;v=cQ--7XZs1ew" rel="noopener noreferrer" target="_blank">total time vs total work</a></p><p>We say that a parallel algorithm is <strong>work efficient</strong> if its <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?time_continue=88&amp;v=V8TTrUdfpIY" rel="noopener noreferrer" target="_blank">work complexity</a> is asymptotically the same as the equivalent serial algorithm</p><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Analysis%20of%20parallel%20algorithms">Analysis of parallel algorithms</a></h3><hr><h1 class="">Parallel programming</h1><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#CUDA">CUDA</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#OpenMP">OpenMP</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#MPI">MPI</a></li></ul><h3 class=""><u><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=LjWlZHqUG8A" rel="noopener noreferrer" target="_blank">Parallel communication patterns</a></u></h3><p>Tasks &lt;&gt; Memory</p><ul><li>Map. 1-to-1.. 1 thread on 1 part of memory, independently.</li><li>Scatter. 1-to-many. 1 thread, write to a potentially different and potentially more than 1 part of memory, independently.</li><li>Gather. many-to-1. Like scatter but for reading instead of writting.<ul><li>Stencil. Read from a fixed set of neighbours, and write to 1 part of  memory</li></ul></li><li>Transpose.1-to-1.  Any read and any write locations?</li><li>Reduce. all-to-1.</li><li>scan/sort. all-to-all.</li><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?time_continue=10&amp;v=Jo6RnEi6eHE" rel="noopener noreferrer" target="_blank">More methods</a><ul><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?time_continue=9&amp;v=N1eQowSCdlw" rel="noopener noreferrer" target="_blank">Reduce</a> –&gt; <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?time_continue=24&amp;v=prLb1MbAm8M" rel="noopener noreferrer" target="_blank">parallelizing reduce</a> for binary/associative operators. See more at <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Analysis%20of%20parallel%20algorithms">Analysis of parallel algorithms</a></li><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=We9j876CjtA" rel="noopener noreferrer" target="_blank">Scan</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?time_continue=11&amp;v=hS_uAPgXpzE" rel="noopener noreferrer" target="_blank">math</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?time_continue=142&amp;v=HfXkXUDlBqI" rel="noopener noreferrer" target="_blank">why do we care about parallel scan</a></li></ul></li></ul><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?time_continue=49&amp;v=8NiigEw_UIE" rel="noopener noreferrer" target="_blank">Thread diveregence</a></p><hr><p>Introduction to parallel programming by nvidia in Udacity: <a class="tc-tiddlylink-external" href="https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/671181630923" rel="noopener noreferrer" target="_blank">https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/671181630923</a></p><hr><h2 class=""><u>Latency vs throughput tradeoff</u></h2><p>Latency: time for a single unit operation to take place</p><p>Throughput: number of operations per second.</p><p>Latency has advanced more slowly than throughput in technologies: <a class="tc-tiddlylink-external" href="http://dl.acm.org.sci-hub.cc/citation.cfm?id=1022596" rel="noopener noreferrer" target="_blank">Latency lags throughput</a></p><h2 class="">Types of parallel computing</h2><ul><li>High-throughput computing, aka embarassingly parallel computing: lots of *independent* tasks.</li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#High-performance%20computing">High-performance computing</a> often refers to a big task divided into many parallel computing nodes, but they are not totally independent, and so issues of communication ened to be addressed.</li></ul><h2 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Memory">Memory</a> models</h2><p>distributed and shared memory parallel computing models </p><ul><li><strong>Share memory</strong>: all the cores can see the same memory. <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#OpenMP">OpenMP</a>. Limited to one node in a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Computer%20cluster">Computer cluster</a></li><li><strong>Distributed memory</strong>: each core has a separate memory they can access. <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#MPI">MPI</a>. Scales to many many thousdands of cores accross several nodes..</li></ul><p>Often use a combination of both, like <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#CUDA">CUDA</a></p><hr><p>– Clusters	and	job	managers.
– Jobs	vs Tasks.	
• Creating	and	submitting	them.
• Getting	the	results
– Code	portability.
– Callback	functions
• Advanced	parallelism.
– spmd mode,	message	passing.
– GPU	computing.</p><p><a class="tc-tiddlylink-external" href="https://uk.mathworks.com/help/distcomp/how-parallel-computing-products-run-a-job.html" rel="noopener noreferrer" target="_blank">https://uk.mathworks.com/help/distcomp/how-parallel-computing-products-run-a-job.html</a></p>