<p>&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=8_ywDfr1FGU&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Nice video about parallel computing&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=jMfVx4hFHVk&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Why we cannot keep increasing CPU speed?&lt;/a&gt; Power has emerged as one of the primary factors in processor design.&lt;/p&gt;&lt;p&gt;Often used in &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Computer%20cluster&quot;&gt;Computer cluster&lt;/a&gt; and &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#GPU%20computing&quot;&gt;GPU computing&lt;/a&gt;. Main application is for &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#High-performance%20computing&quot;&gt;High-performance computing&lt;/a&gt; (see more there)&lt;/p&gt;&lt;p&gt;Fundamental concept: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=81&amp;amp;v=cQ--7XZs1ew&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;total time vs total work&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We say that a parallel algorithm is &lt;strong&gt;work efficient&lt;/strong&gt; if its &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=88&amp;amp;v=V8TTrUdfpIY&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;work complexity&lt;/a&gt; is asymptotically the same as the equivalent serial algorithm&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Analysis%20of%20parallel%20algorithms&quot;&gt;Analysis of parallel algorithms&lt;/a&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h1 class=&quot;&quot;&gt;Parallel programming&lt;/h1&gt;&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#CUDA&quot;&gt;CUDA&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#OpenMP&quot;&gt;OpenMP&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#MPI&quot;&gt;MPI&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=LjWlZHqUG8A&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Parallel communication patterns&lt;/a&gt;&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;Tasks &amp;lt;&amp;gt; Memory&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Map. 1-to-1.. 1 thread on 1 part of memory, independently.&lt;/li&gt;&lt;li&gt;Scatter. 1-to-many. 1 thread, write to a potentially different and potentially more than 1 part of memory, independently.&lt;/li&gt;&lt;li&gt;Gather. many-to-1. Like scatter but for reading instead of writting.&lt;ul&gt;&lt;li&gt;Stencil. Read from a fixed set of neighbours, and write to 1 part of  memory&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Transpose.1-to-1.  Any read and any write locations?&lt;/li&gt;&lt;li&gt;Reduce. all-to-1.&lt;/li&gt;&lt;li&gt;scan/sort. all-to-all.&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=10&amp;amp;v=Jo6RnEi6eHE&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;More methods&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=9&amp;amp;v=N1eQowSCdlw&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Reduce&lt;/a&gt; –&amp;gt; &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=24&amp;amp;v=prLb1MbAm8M&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;parallelizing reduce&lt;/a&gt; for binary/associative operators. See more at &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Analysis%20of%20parallel%20algorithms&quot;&gt;Analysis of parallel algorithms&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=We9j876CjtA&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Scan&lt;/a&gt; – &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=11&amp;amp;v=hS_uAPgXpzE&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;math&lt;/a&gt; – &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=142&amp;amp;v=HfXkXUDlBqI&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;why do we care about parallel scan&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=49&amp;amp;v=8NiigEw_UIE&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Thread diveregence&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Introduction to parallel programming by nvidia in Udacity: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/671181630923&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/671181630923&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Latency vs throughput tradeoff&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;Latency: time for a single unit operation to take place&lt;/p&gt;&lt;p&gt;Throughput: number of operations per second.&lt;/p&gt;&lt;p&gt;Latency has advanced more slowly than throughput in technologies: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://dl.acm.org.sci-hub.cc/citation.cfm?id=1022596&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Latency lags throughput&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;Types of parallel computing&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;High-throughput computing, aka embarassingly parallel computing: lots of *independent* tasks.&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#High-performance%20computing&quot;&gt;High-performance computing&lt;/a&gt; often refers to a big task divided into many parallel computing nodes, but they are not totally independent, and so issues of communication ened to be addressed.&lt;/li&gt;&lt;/ul&gt;&lt;h2 class=&quot;&quot;&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Memory&quot;&gt;Memory&lt;/a&gt; models&lt;/h2&gt;&lt;p&gt;distributed and shared memory parallel computing models &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Share memory&lt;/strong&gt;: all the cores can see the same memory. &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#OpenMP&quot;&gt;OpenMP&lt;/a&gt;. Limited to one node in a &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Computer%20cluster&quot;&gt;Computer cluster&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Distributed memory&lt;/strong&gt;: each core has a separate memory they can access. &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#MPI&quot;&gt;MPI&lt;/a&gt;. Scales to many many thousdands of cores accross several nodes..&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Often use a combination of both, like &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#CUDA&quot;&gt;CUDA&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;– Clusters	and	job	managers.
– Jobs	vs Tasks.	
• Creating	and	submitting	them.
• Getting	the	results
– Code	portability.
– Callback	functions
• Advanced	parallelism.
– spmd mode,	message	passing.
– GPU	computing.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://uk.mathworks.com/help/distcomp/how-parallel-computing-products-run-a-job.html&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://uk.mathworks.com/help/distcomp/how-parallel-computing-products-run-a-job.html&lt;/a&gt;&lt;/p&gt;</p>