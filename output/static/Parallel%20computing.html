<p><code>&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=utf-8&quot; /&gt;
&lt;meta name=&quot;generator&quot; content=&quot;TiddlyWiki&quot; /&gt;
&lt;meta name=&quot;tiddlywiki-version&quot; content=&quot;</code>5.1.17<code>&quot; /&gt;
&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot; /&gt;
&lt;meta name=&quot;apple-mobile-web-app-capable&quot; content=&quot;yes&quot; /&gt;
&lt;meta name=&quot;apple-mobile-web-app-status-bar-style&quot; content=&quot;black-translucent&quot; /&gt;
&lt;meta name=&quot;mobile-web-app-capable&quot; content=&quot;yes&quot;/&gt;
&lt;meta name=&quot;format-detection&quot; content=&quot;telephone=no&quot;&gt;
&lt;link id=&quot;faviconLink&quot; rel=&quot;shortcut icon&quot; href=&quot;favicon.ico&quot;&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;static.css&quot;&gt;
&lt;title&gt;</code>Parallel computing: Cosmos — Everything there was, there is, <span class="subtitle-dark">and there will be</span><code>&lt;/title&gt;
&lt;/head&gt;
&lt;body class=&quot;tc-body&quot;&gt;
</code><code>
&lt;section class=&quot;tc-story-river&quot;&gt;
</code>
&lt;p&gt;&lt;div class=&quot;tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-High-performance%20computing &quot; data-tags=&quot;[[High-performance computing]]&quot; data-tiddler-title=&quot;Parallel computing&quot;&gt;&lt;div class=&quot;tc-tiddler-title&quot;&gt;
&lt;div class=&quot;tc-titlebar&quot;&gt;
&lt;span class=&quot;tc-tiddler-controls&quot;&gt;
&lt;span class=&quot; tc-reveal&quot;&gt;&lt;button aria-label=&quot;more&quot; class=&quot;tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions&quot; title=&quot;More actions&quot;&gt;&lt;/button&gt;&lt;div class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/div&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot;&gt;&lt;button aria-label=&quot;edit&quot; class=&quot;tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit&quot; title=&quot;Edit this tiddler&quot;&gt;&lt;/button&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot;&gt;&lt;button aria-label=&quot;close&quot; class=&quot;tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose&quot; title=&quot;Close this tiddler&quot;&gt;&lt;/button&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot;&gt;&lt;button aria-label=&quot;tiddlymap&quot; class=&quot;tc-btn-invisible tc-btn-%24%3A%2Fplugins%2Ffelixhayashi%2Ftiddlymap%2Fmisc%2FquickConnectButton &quot; title=&quot;Toggle TiddlyMap actions&quot;&gt;


&lt;/button&gt;&lt;/span&gt;
&lt;/span&gt;

&lt;span&gt;

&lt;span class=&quot;tc-tiddler-title-icon&quot; style=&quot;fill:;&quot;&gt;

&lt;/span&gt;



&lt;h2 class=&quot;tc-title&quot;&gt;
Parallel computing
&lt;/h2&gt;

&lt;/span&gt;

&lt;/div&gt;

&lt;div class=&quot;tc-tiddler-info tc-popup-handle tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/div&gt;
&lt;/div&gt;&lt;div class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/div&gt;
&lt;div class=&quot; tc-reveal&quot;&gt;
&lt;div class=&quot;tc-subtitle&quot;&gt;
&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;cosmos.html&quot;&gt;
cosmos
&lt;/a&gt; 4th April 2017 at 5:26am
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot; tc-reveal&quot;&gt;
&lt;div class=&quot;tc-tags-wrapper&quot;&gt;&lt;span class=&quot;tc-tag-list-item&quot;&gt;


&lt;span class=&quot;tc-tag-label tc-btn-invisible&quot; draggable=&quot;true&quot; style=&quot;background-color:;
fill:#333333;
color:#333333;&quot;&gt;
 High-performance computing
&lt;/span&gt;

&lt;span class=&quot;tc-drop-down tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;

&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;tc-tiddler-body tc-reveal&quot;&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=8_ywDfr1FGU&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Nice video about parallel computing&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=jMfVx4hFHVk&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Why we cannot keep increasing CPU speed?&lt;/a&gt; Power has emerged as one of the primary factors in processor design.&lt;/p&gt;&lt;p&gt;Often used in &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Computer%2520cluster.html&quot;&gt;Computer cluster&lt;/a&gt; and &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;GPU%2520computing.html&quot;&gt;GPU computing&lt;/a&gt;. Main application is for &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;High-performance%2520computing.html&quot;&gt;High-performance computing&lt;/a&gt; (see more there)&lt;/p&gt;&lt;p&gt;Fundamental concept: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=81&amp;amp;v=cQ--7XZs1ew&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;total time vs total work&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We say that a parallel algorithm is &lt;strong&gt;work efficient&lt;/strong&gt; if its &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=88&amp;amp;v=V8TTrUdfpIY&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;work complexity&lt;/a&gt; is asymptotically the same as the equivalent serial algorithm&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Analysis%2520of%2520parallel%2520algorithms.html&quot;&gt;Analysis of parallel algorithms&lt;/a&gt;&lt;/h3&gt;&lt;hr&gt;&lt;h1 class=&quot;&quot;&gt;Parallel programming&lt;/h1&gt;&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;CUDA.html&quot;&gt;CUDA&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;OpenMP.html&quot;&gt;OpenMP&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;MPI.html&quot;&gt;MPI&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=LjWlZHqUG8A&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Parallel communication patterns&lt;/a&gt;&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;Tasks &amp;lt;&amp;gt; Memory&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Map. 1-to-1.. 1 thread on 1 part of memory, independently.&lt;/li&gt;&lt;li&gt;Scatter. 1-to-many. 1 thread, write to a potentially different and potentially more than 1 part of memory, independently.&lt;/li&gt;&lt;li&gt;Gather. many-to-1. Like scatter but for reading instead of writting.&lt;ul&gt;&lt;li&gt;Stencil. Read from a fixed set of neighbours, and write to 1 part of  memory&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Transpose.1-to-1.  Any read and any write locations?&lt;/li&gt;&lt;li&gt;Reduce. all-to-1.&lt;/li&gt;&lt;li&gt;scan/sort. all-to-all.&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=10&amp;amp;v=Jo6RnEi6eHE&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;More methods&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=9&amp;amp;v=N1eQowSCdlw&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Reduce&lt;/a&gt; –&amp;gt; &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=24&amp;amp;v=prLb1MbAm8M&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;parallelizing reduce&lt;/a&gt; for binary/associative operators. See more at &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Analysis%2520of%2520parallel%2520algorithms.html&quot;&gt;Analysis of parallel algorithms&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=We9j876CjtA&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Scan&lt;/a&gt; – &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=11&amp;amp;v=hS_uAPgXpzE&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;math&lt;/a&gt; – &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=142&amp;amp;v=HfXkXUDlBqI&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;why do we care about parallel scan&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?time_continue=49&amp;amp;v=8NiigEw_UIE&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Thread diveregence&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Introduction to parallel programming by nvidia in Udacity: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/671181630923&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/671181630923&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Latency vs throughput tradeoff&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;Latency: time for a single unit operation to take place&lt;/p&gt;&lt;p&gt;Throughput: number of operations per second.&lt;/p&gt;&lt;p&gt;Latency has advanced more slowly than throughput in technologies: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://dl.acm.org.sci-hub.cc/citation.cfm?id=1022596&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Latency lags throughput&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;Types of parallel computing&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;High-throughput computing, aka embarassingly parallel computing: lots of *independent* tasks.&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;High-performance%2520computing.html&quot;&gt;High-performance computing&lt;/a&gt; often refers to a big task divided into many parallel computing nodes, but they are not totally independent, and so issues of communication ened to be addressed.&lt;/li&gt;&lt;/ul&gt;&lt;h2 class=&quot;&quot;&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Memory.html&quot;&gt;Memory&lt;/a&gt; models&lt;/h2&gt;&lt;p&gt;distributed and shared memory parallel computing models &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Share memory&lt;/strong&gt;: all the cores can see the same memory. &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;OpenMP.html&quot;&gt;OpenMP&lt;/a&gt;. Limited to one node in a &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Computer%2520cluster.html&quot;&gt;Computer cluster&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Distributed memory&lt;/strong&gt;: each core has a separate memory they can access. &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;MPI.html&quot;&gt;MPI&lt;/a&gt;. Scales to many many thousdands of cores accross several nodes..&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Often use a combination of both, like &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;CUDA.html&quot;&gt;CUDA&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;– Clusters	and	job	managers.
– Jobs	vs Tasks.	
• Creating	and	submitting	them.
• Getting	the	results
– Code	portability.
– Callback	functions
• Advanced	parallelism.
– spmd mode,	message	passing.
– GPU	computing.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://uk.mathworks.com/help/distcomp/how-parallel-computing-products-run-a-job.html&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://uk.mathworks.com/help/distcomp/how-parallel-computing-products-run-a-job.html&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;


&lt;/div&gt;

&lt;/p&gt;
<code>
&lt;/section&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></p>