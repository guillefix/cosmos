<p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Artificial%20intelligence">Artificial intelligence</a></p><p>Read about interesting models and framings for AI safety here: <a class="tc-tiddlylink-external" href="https://docs.google.com/document/d/145yJBoNTYHOJ_FMOO2hO-x2KnJQT45hxhtd0I84HVLE/edit" rel="noopener noreferrer" target="_blank">https://docs.google.com/document/d/145yJBoNTYHOJ_FMOO2hO-x2KnJQT45hxhtd0I84HVLE/edit</a></p><p><a class="tc-tiddlylink-external" href="https://blog.openai.com/debate/" rel="noopener noreferrer" target="_blank">https://blog.openai.com/debate/</a></p><p><a class="tc-tiddlylink-external" href="https://maliciousaireport.com" rel="noopener noreferrer" target="_blank">https://maliciousaireport.com</a> </p><p><a class="tc-tiddlylink-external" href="https://openai.com/blog/concrete-ai-safety-problems/" rel="noopener noreferrer" target="_blank">OpenAI - Concrete AI safety problems</a> <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1606.06565" rel="noopener noreferrer" target="_blank">paper</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#AI%20ethics">AI ethics</a></p><p>See here for David Dustch comment (linked to specific time): <a class="tc-tiddlylink-external" href="https://vimeo.com/22099396#t=2758s" rel="noopener noreferrer" target="_blank">https://vimeo.com/22099396#t=2758s</a></p><p><a class="tc-tiddlylink-external" href="https://maliciousaireport.com/" rel="noopener noreferrer" target="_blank">https://maliciousaireport.com/</a></p><p><a class="tc-tiddlylink-external" href="https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec" rel="noopener noreferrer" target="_blank">https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec</a> â€“ comments here: <a class="tc-tiddlylink-external" href="https://www.facebook.com/guillermovalleperez/posts/10156139287091223" rel="noopener noreferrer" target="_blank">https://www.facebook.com/guillermovalleperez/posts/10156139287091223</a></p><hr><p>Excellent article not only on what may be the most tangible current AI risk, but more importantly, its possible solution. The choice forks the future into a potential dystopia or a more humane and enritched society.</p><p><a class="tc-tiddlylink-external" href="https://medium.com/@francois.chollet/what-worries-me-about-ai-ed9df072b704" rel="noopener noreferrer" target="_blank">https://medium.com/@francois.chollet/what-worries-me-about-ai-ed9df072b704</a></p><hr><p><a class="tc-tiddlylink-external" href="https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/" rel="noopener noreferrer" target="_blank">https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/</a></p><hr><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Adversarial%20example">Adversarial example</a>s</u></h2><p>What is the simplest way to attack a model. Justin Glimer. Security of ML model is about test error basically. Defend against attackers trying random stuff to fool model. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=kgTocVLNvYI" rel="noopener noreferrer" target="_blank">VIDEO</a></p>