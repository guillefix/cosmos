<p>&lt;p&gt;This includes &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Model%20evaluation&quot;&gt;Model evaluation&lt;/a&gt;, which is the way models are selected...&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0kWZoyNRxTY&amp;amp;index=10&amp;amp;list=PLA89DCFA6ADACE599#t=36m40s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Introduction&lt;/a&gt;, see overfitting and underfitting below. Model selection algorithms provide methods to automatically choose optimal bias/variance tradeoffs. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0kWZoyNRxTY&amp;amp;index=10&amp;amp;list=PLA89DCFA6ADACE599#t=40m10s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Explanation&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Predictive%20posterior&quot;&gt;Predictive posterior&lt;/a&gt;&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;Predictive posterior checks. Likelihood of data, mostly on test data (see &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Cross-validation&quot;&gt;Cross-validation&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;Check distribution of extreme values&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Information criteria&lt;/u&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Akaike%20information%20criterion&quot;&gt;Akaike information criterion&lt;/a&gt; (AIC)&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Bayesian%20information%20criterion&quot;&gt;Bayesian information criterion&lt;/a&gt; (BIC)&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Widely%20applicable%20information%20criterion&quot;&gt;Widely applicable information criterion&lt;/a&gt; (WAIC)&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Approximate%20leave-one-out%20cross-validation&quot;&gt;Approximate leave-one-out cross-validation&lt;/a&gt; (LOO) using Pareto smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. (&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://cran.r-project.org/web/packages/loo/index.html&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;R package&lt;/a&gt;)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.stat.columbia.edu/~gelman/research/unpublished/loo_stan.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Cross-validation&quot;&gt;Cross-validation&lt;/a&gt;&lt;/u&gt;&lt;/h2&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Feature%20selection&quot;&gt;Feature selection&lt;/a&gt;&lt;/u&gt;&lt;/h2&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Structural%20risk%20minimization&quot;&gt;Structural risk minimization&lt;/a&gt;&lt;/u&gt;&lt;/h2&gt;&lt;hr&gt;&lt;p&gt;A lot of these methods are very much related to &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Regularization&quot;&gt;Regularization&lt;/a&gt; methods, as both try to make our model better. Often we want the model to be better at generalizing, and this is done by reducing model complexity.&lt;/p&gt;&lt;p&gt;Using cross-validation for regularization can be done using &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Early%20stopping&quot;&gt;Early stopping&lt;/a&gt; using the validation set&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;u&gt;Model selection for &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Artificial%20neural%20network&quot;&gt;Artificial neural network&lt;/a&gt;s&lt;/u&gt;: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=Fs-raHUnF2M&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;amp;index=16&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Neural networks [2.10] : Training neural networks - model selection&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sub&gt;&lt;em&gt;Old comment&lt;/em&gt;: One can show (maybe technical details I don't know..) that given the real distribution of the data, and a sample used for training, one is likely to underestimate the error. So I think cross-validation can be shown rigorously to be good for assessing a model's predictive power (i.e. probability of predicting rightly). See Elements of Statistical Learning book for all details..&lt;/sub&gt;&lt;/p&gt;&lt;p&gt;It is a way to find out if you are overfitting&lt;/p&gt;&lt;p&gt;Related: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data&lt;/a&gt;&lt;/p&gt;</p>