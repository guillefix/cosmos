<p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Learning%20theory">Learning theory</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Order">Order</a> and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Simplicity%20bias">Simplicity bias</a>. The simplicity and structure in signals in the real-world is often seized to make the learning problem easier to solve. Can be formalized via <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Learning%20theory">Learning theory</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#PAC-Bayesian%20learning">PAC-Bayesian learning</a></p><p>See my paper on <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Simplicity%20bias%20in%20the%20parameter-function%20map">Simplicity bias in the parameter-function map</a></p><p>Applications in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Inverse%20problem">Inverse problem</a>s. For instance, see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Convex%20optimization%20heuristics%20for%20linear%20inverse%20problems">Convex optimization heuristics for linear inverse problems</a> and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Linear%20inverse%20problem">Linear inverse problem</a></p><p>Applications in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Compressed%20sensing">Compressed sensing</a></p><h3 class=""><u>Simplicity and neural networks</u></h3><p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Neural%20network%20theory">Neural network theory</a>, <a class="tc-tiddlylink-external" href="http://arxiv.org/abs/1608.08225" rel="noopener noreferrer" target="_blank">Why does deep and cheap learning work so well?</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Deep%20learning%20theory">Deep learning theory</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Generalization%20in%20deep%20learning">Generalization in deep learning</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1111.3846" rel="noopener noreferrer" target="_blank">No Free Lunch versus Occam's Razor in Supervised Learning</a></p><p><b>Nature often results in functions that are polynomials with several simplifying features</b>:</p><p><em>1. Low polynomial order </em></p><p>For reasons that are still not fully understood, our uni-verse can be accurately described by polynomial <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Hamiltonian">Hamiltonian</a>s of low order <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">d</span></span></span></span></span>. </p><p>The <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Central%20limit%20theorem">Central limit theorem</a> gives rise to <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Probability%20distributions">Probability distributions</a> corresponding to quadratic Hamiltonians (see def in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Neural%20network%20theory">Neural network theory</a>). Similar results regarding maximum entropy distributions are also mentioned in the paper. Several common operations on image and sound are linear and thus order 1 polynomials on the input.</p><p><em>2. Locality</em></p><p>locality in a lattice manifests itself by allowing only nearest-neighbor interaction. In other words, almost all coeficients in the polynomial are forced to vanish, and the total number of non-zero coeficients grows only linearly with <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span></span>.</p><p>This can be stated more generally and precisely using the Markov network formalism</p><p><em>3. Symmetry</em></p><p>Whenever the Hamiltonian obeys some symmetry (is in-variant under some transformation), the number of independent parameters required to describe it is further reduced.</p><hr><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=oOB4evKlEmQ#t=35m" rel="noopener noreferrer" target="_blank">deep learning</a></p><p>&quot;If f is a truly random function then
it is highly unlikely that anyone will ever conceive of its existence and
would want to learn it.&quot; ~ Li&amp;Vitanyi's book</p><p>See also comments in <a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1611.00740.pdf" rel="noopener noreferrer" target="_blank">this paper</a></p>