<p>&lt;p&gt;See &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Learning%20theory&quot;&gt;Learning theory&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Order&quot;&gt;Order&lt;/a&gt; and &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Simplicity%20bias&quot;&gt;Simplicity bias&lt;/a&gt;. The simplicity and structure in signals in the real-world is often seized to make the learning problem easier to solve. Can be formalized via &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Learning%20theory&quot;&gt;Learning theory&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#PAC-Bayesian%20learning&quot;&gt;PAC-Bayesian learning&lt;/a&gt;&lt;/p&gt;&lt;p&gt;See my paper on &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Simplicity%20bias%20in%20the%20parameter-function%20map&quot;&gt;Simplicity bias in the parameter-function map&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Applications in &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Inverse%20problem&quot;&gt;Inverse problem&lt;/a&gt;s. For instance, see &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Convex%20optimization%20heuristics%20for%20linear%20inverse%20problems&quot;&gt;Convex optimization heuristics for linear inverse problems&lt;/a&gt; and &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Linear%20inverse%20problem&quot;&gt;Linear inverse problem&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Applications in &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Compressed%20sensing&quot;&gt;Compressed sensing&lt;/a&gt;&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;Simplicity and neural networks&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;See &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Neural%20network%20theory&quot;&gt;Neural network theory&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://arxiv.org/abs/1608.08225&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Why does deep and cheap learning work so well?&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Deep%20learning%20theory&quot;&gt;Deep learning theory&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Generalization%20in%20deep%20learning&quot;&gt;Generalization in deep learning&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1111.3846&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;No Free Lunch versus Occam's Razor in Supervised Learning&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Nature often results in functions that are polynomials with several simplifying features&lt;/b&gt;:&lt;/p&gt;&lt;p&gt;&lt;em&gt;1. Low polynomial order &lt;/em&gt;&lt;/p&gt;&lt;p&gt;For reasons that are still not fully understood, our uni-verse can be accurately described by polynomial &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Hamiltonian&quot;&gt;Hamiltonian&lt;/a&gt;s of low order &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;d&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.69444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:0.69444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot;&gt;d&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. &lt;/p&gt;&lt;p&gt;The &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Central%20limit%20theorem&quot;&gt;Central limit theorem&lt;/a&gt; gives rise to &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Probability%20distributions&quot;&gt;Probability distributions&lt;/a&gt; corresponding to quadratic Hamiltonians (see def in &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Neural%20network%20theory&quot;&gt;Neural network theory&lt;/a&gt;). Similar results regarding maximum entropy distributions are also mentioned in the paper. Several common operations on image and sound are linear and thus order 1 polynomials on the input.&lt;/p&gt;&lt;p&gt;&lt;em&gt;2. Locality&lt;/em&gt;&lt;/p&gt;&lt;p&gt;locality in a lattice manifests itself by allowing only nearest-neighbor interaction. In other words, almost all coeficients in the polynomial are forced to vanish, and the total number of non-zero coeficients grows only linearly with &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;n&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot;&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;&lt;p&gt;This can be stated more generally and precisely using the Markov network formalism&lt;/p&gt;&lt;p&gt;&lt;em&gt;3. Symmetry&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Whenever the Hamiltonian obeys some symmetry (is in-variant under some transformation), the number of independent parameters required to describe it is further reduced.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=oOB4evKlEmQ#t=35m&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;deep learning&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&amp;quot;If f is a truly random function then
it is highly unlikely that anyone will ever conceive of its existence and
would want to learn it.&amp;quot; ~ Li&amp;amp;Vitanyi's book&lt;/p&gt;&lt;p&gt;See also comments in &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/pdf/1611.00740.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;this paper&lt;/a&gt;&lt;/p&gt;</p>