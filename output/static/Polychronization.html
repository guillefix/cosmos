<p><code>&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=utf-8&quot; /&gt;
&lt;meta name=&quot;generator&quot; content=&quot;TiddlyWiki&quot; /&gt;
&lt;meta name=&quot;tiddlywiki-version&quot; content=&quot;</code>5.1.17<code>&quot; /&gt;
&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot; /&gt;
&lt;meta name=&quot;apple-mobile-web-app-capable&quot; content=&quot;yes&quot; /&gt;
&lt;meta name=&quot;apple-mobile-web-app-status-bar-style&quot; content=&quot;black-translucent&quot; /&gt;
&lt;meta name=&quot;mobile-web-app-capable&quot; content=&quot;yes&quot;/&gt;
&lt;meta name=&quot;format-detection&quot; content=&quot;telephone=no&quot;&gt;
&lt;link id=&quot;faviconLink&quot; rel=&quot;shortcut icon&quot; href=&quot;favicon.ico&quot;&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;static.css&quot;&gt;
&lt;title&gt;</code>Polychronization: Cosmos — Everything there was, there is, <span class="subtitle-dark">and there will be</span><code>&lt;/title&gt;
&lt;/head&gt;
&lt;body class=&quot;tc-body&quot;&gt;
</code><code>
&lt;section class=&quot;tc-story-river&quot;&gt;
</code>
&lt;p&gt;&lt;div class=&quot;tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Spiking%20neural%20network &quot; data-tags=&quot;[[Spiking neural network]]&quot; data-tiddler-title=&quot;Polychronization&quot;&gt;&lt;div class=&quot;tc-tiddler-title&quot;&gt;
&lt;div class=&quot;tc-titlebar&quot;&gt;
&lt;span class=&quot;tc-tiddler-controls&quot;&gt;
&lt;span class=&quot; tc-reveal&quot;&gt;&lt;button aria-label=&quot;more&quot; class=&quot;tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions&quot; title=&quot;More actions&quot;&gt;&lt;/button&gt;&lt;div class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/div&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot;&gt;&lt;button aria-label=&quot;edit&quot; class=&quot;tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit&quot; title=&quot;Edit this tiddler&quot;&gt;&lt;/button&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot;&gt;&lt;button aria-label=&quot;close&quot; class=&quot;tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose&quot; title=&quot;Close this tiddler&quot;&gt;&lt;/button&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot;&gt;&lt;button aria-label=&quot;tiddlymap&quot; class=&quot;tc-btn-invisible tc-btn-%24%3A%2Fplugins%2Ffelixhayashi%2Ftiddlymap%2Fmisc%2FquickConnectButton &quot; title=&quot;Toggle TiddlyMap actions&quot;&gt;


&lt;/button&gt;&lt;/span&gt;
&lt;/span&gt;

&lt;span&gt;

&lt;span class=&quot;tc-tiddler-title-icon&quot; style=&quot;fill:;&quot;&gt;

&lt;/span&gt;



&lt;h2 class=&quot;tc-title&quot;&gt;
Polychronization
&lt;/h2&gt;

&lt;/span&gt;

&lt;/div&gt;

&lt;div class=&quot;tc-tiddler-info tc-popup-handle tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/div&gt;
&lt;/div&gt;&lt;div class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/div&gt;
&lt;div class=&quot; tc-reveal&quot;&gt;
&lt;div class=&quot;tc-subtitle&quot;&gt;
&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;cosmos.html&quot;&gt;
cosmos
&lt;/a&gt; 25th March 2017 at 10:50am
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot; tc-reveal&quot;&gt;
&lt;div class=&quot;tc-tags-wrapper&quot;&gt;&lt;span class=&quot;tc-tag-list-item&quot;&gt;


&lt;span class=&quot;tc-tag-label tc-btn-invisible&quot; draggable=&quot;true&quot; style=&quot;background-color:;
fill:#333333;
color:#333333;&quot;&gt;
 Spiking neural network
&lt;/span&gt;

&lt;span class=&quot;tc-drop-down tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;

&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;tc-tiddler-body tc-reveal&quot;&gt;&lt;p&gt;Certain spatio-temporal patterns of firing (with time-locked firings) emerge in &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Spiking%2520neural%2520network.html&quot;&gt;Spiking neural network&lt;/a&gt;s with axonal delays, and STDP learning – &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.izhikevich.org/publications/spnet.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;paper&lt;/a&gt;. This gives rise to &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;Polychrnous%2520neural%2520groups.html&quot;&gt;Polychrnous neural groups&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Because of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Spike-timing-dependent%2520plasticity.html&quot;&gt;Spike-timing-dependent plasticity&lt;/a&gt;, spiking neural networks can give rise to radically different self-organisation of visual representations when trained on visual scenes (for example). Furthermore, if distributions of axonal delays between neurons are incorporated, then this can give rise to a phenomenon known as ‘polychronization’. This phenomenon involves the network learning many memory patterns, each of which takes the form of a repeating temporal loop of neuronal spike emissions. These temporal memory loops self-organise automatically when STDP is used to modify the strengths of synapses in a recurrently connected spiking network with randomised distributions of axonal conduction delays between neurons. Stringer et al. discuss how a form of polychronization may contribute to the development of highly selective &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Binding%2520problem.html&quot;&gt;binding neurons&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The firing patterns can be found in the firing sequences of single neurons or in the relative timing of spikes of multiple neurons, which are then said to form a functional &lt;strong&gt;neuronal group&lt;/strong&gt;. Activation of such a neuronal group can be triggered by stimuli or behavioral events. These findings have been widely used to support the hypothesis of &lt;strong&gt;temporal coding&lt;/strong&gt; in the brain.&lt;/p&gt;&lt;p&gt;Since the firings of these neurons are not synchronous but time-locked to each other, we refer to such groups as polychronous, wherepolymeans manyandchronousmeanstimeorclockin Greek. Polychrony should be dis-tinguished from asynchrony, since the latter does not imply a reproducible time-locking pattern, but usually describes noisy, random, nonsynchronous events. It is also different from the notion of clustering, partial synchrony (Hoppensteadt &amp;amp; Izhikevich, 1997), or polysynchrony (Stewart, Golubit-sky, &amp;amp; Pivato, 2003), in which some neurons oscillate synchronously while others do not. &lt;/p&gt;&lt;p&gt;&lt;em&gt;How many distinct polychronous groups can be stored in a given network?&lt;/em&gt; Experimentally, even more than the number of synapses. But theoretically?&lt;/p&gt;&lt;p&gt;We hypothesize that polychronous groups could represent &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Memory.html&quot;&gt;memories&lt;/a&gt; and experience. They show that the memory tends to learn to &amp;quot;recognize&amp;quot; input patterns, by developing polychronized groups activated by them.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Rate to Spike-Timing Conversion&lt;/strong&gt;. Neurons in the model use a spike-timing code to interact and form groups. However, the external input from sensory organs, such as retinal cells and hair cells in cochlear, arrives as a rate code, that is, encoded into the mean firing frequency of spiking. How can the network convert rates to precise spike timings? . It is easy to see how rate to spike-timing conversion could occur at the onset of stimulation. As the input volley arrives, the neurons getting stronger excitation fire first, and the neurons getting weaker excitation fire later or not at all. Spiking between layers in &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Feedforward%2520neural%2520network.html&quot;&gt;Feedforward neural network&lt;/a&gt;s?.   They hypothesize that &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Brain%2520waves.html&quot;&gt;intrinsic rhythmicity&lt;/a&gt; generates internal “saccades” that can parse the rate input into spike timing, and they discuss three possible mechanisms how this could be accomplished.&lt;/p&gt;&lt;p&gt;Related to &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Synfire%2520chain.html&quot;&gt;Synfire chain&lt;/a&gt;s, which are sometimes called &lt;em&gt;synfire braid&lt;/em&gt;s.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Polychronization simulation in rate-coded &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Artificial%2520neural%2520network.html&quot;&gt;Artificial neural network&lt;/a&gt;s via &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Skip-connection.html&quot;&gt;Skip-connection&lt;/a&gt;-axonal delays:&lt;/p&gt;&lt;p&gt;&amp;quot;In particular, our spiking models exploit the ability of real neurons to operate as 'coincidence detectors', whereby a postsynaptic neuron responds if and only if a number of incoming presynaptic spikes arrive simultaneously.&amp;quot; But I thought that &amp;quot;coincidence detection&amp;quot;, or more generally, polychrony detection, was a result of having axonal delays, and appropriate activation functions, which can thus be implemented in my unrolled network. In fact, I found some work that tries to stripe down polychronous groups to the minimal set of requirements, allowing for more efficient computation, and their networks are similar to what I have in mind: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/0806.1070&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/0806.1070&lt;/a&gt; and &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://ieeexplore.ieee.org.sci-hub.cc/document/6033533/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;http://ieeexplore.ieee.org.sci-hub.cc/document/6033533/&lt;/a&gt; . If you look at figure 1b in your paper, the idea is essentialy to model each neuron at each time step where at least one axon arrives, as an independent neuron in a deep feedforward network. You just need to make sure your activation function has some thresholding character to have coincidence detection. By unfolding time, we basically convert complex dynamics + simple topology (in an SNN) to complex topology with simple dynamics. This is useful for theoretical analysis (the reason you use it in your paper, and elsewhere), but I am claiming that it may be useful for simulation on the computer too, as one can probably exploit many of the tricks from deep learning. Just to make this more clear, the reason you achieve polychrony detection is because activating a group of neurons in two different temporal patterns, is translated into activating two different set of neurons. There is a subtlety with this unrolled network, which is we need to keep weight sharing to be a true image of the original net, but this is already done with RNNs anyway.&lt;/p&gt;&lt;p&gt;&amp;quot;Another important aspect of our models is the 'holographic principle', in which information about visuospatial features at every spatial scale is propagated upwards to the later (output) stages of visual processing for readout by later brain systems. This provides a holistic representation of the visuospatial world, in which features at every spatial scale are properly integrated.&amp;quot;. Interestingly, the literature on semantic image segmentation has been doing quite similar things. For instance: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1608.06993&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/1608.06993&lt;/a&gt; and &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/pdf/1611.09326.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/pdf/1611.09326.pdf&lt;/a&gt; . They justify it in several ways, but among them, they talk about the need to integrate high level features with low level features to produce high resolution semantic segmentation of the visual scene. As far as I know, however noone has noticed the connection between these ideas and spiking nets and polychrony, which I think gives some very promising new directions for improvement, by seizing the effects you talk about in your paper, implemented in the way described above.&lt;/p&gt;&lt;p&gt;Finally, regarding &amp;quot;For example, one way that we could develop applications would be to employ a biological spiking network as a preprocessing stage before a more traditional supervised engineering network such as backpropagation of error / deep learning&amp;quot;. I think this falls under the general area known as &amp;quot;reservoir computing&amp;quot;, where they have some recurrent network of some kind as a reservoir of nonlinear features, on top of which one trains a simpler network, by backpropagation for instance. Here is an interesting paper on this area: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://bengio.abracadoudou.com/cv/publications/pdf/paugam_2008_neurocomputing.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;http://bengio.abracadoudou.com/cv/publications/pdf/paugam_2008_neurocomputing.pdf&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;


&lt;/div&gt;

&lt;/p&gt;
<code>
&lt;/section&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></p>