<p><sub>aka noisy-channel coding theorem</sub></p><p>In information theory, the noisy-channel coding theorem (sometimes Shannon's theorem), establishes that for any given degree of noise contamination of a communication channel, it is possible to communicate discrete data (digital information) nearly error-free up to a computable maximum rate through the channel, called the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Channel%20capacity">Channel capacity</a>. </p><p>In other words, the theorem states that given a noisy channel with channel capacity C and information transmitted at a rate R, then if R&lt;C there exist codes that allow the probability of error at the receiver to be made arbitrarily small. This means that, theoretically, it is possible to transmit information nearly without error at any rate below a limiting rate, C.</p><p><sub>Long enough code blocks can achieve the channel capacity limits (similar to arguments for understanding entropy by many trials).</sub></p><p>See <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=OHCpECIFNxM" rel="noopener noreferrer" target="_blank">this video</a></p><p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Data%20transmission">Data transmission</a>. Can be done with <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Hamming%20code">Hamming code</a></p>