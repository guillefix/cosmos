<p>&lt;p&gt;The sensitivity of a function refers to a measure of how sensitive its outputs are to its inputs. In particular, &amp;quot;how much do the outputs change when we make a small change of the inputs&amp;quot;?&lt;/p&gt;&lt;p&gt;There are a variety of precise notions, mostly defined for &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Boolean%20function&quot;&gt;Boolean function&lt;/a&gt;s. See &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://theorycenter.cs.uchicago.edu/REU/2014/final-papers/tsang.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Average sensitivity&lt;/strong&gt;: the average {number of bits that change the output} over all inputs. This is the same as the &lt;em&gt;total influence&lt;/em&gt; which is the sum of the influences of input bits, where the &lt;strong&gt;influence&lt;/strong&gt; of input bit i, is the probability that changing that bit changes the output, when uniformly sampling inputs.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://youtu.be/HIKTt9vaElM?t=10m9s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Average sensitivity for linear threshold functions&lt;/a&gt; is &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;O(\sqrt{n})&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.8002800000000001em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:1.05028em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.02778em;&quot;&gt;O&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sqrt mord&quot;&gt;&lt;span class=&quot;sqrt-sign&quot; style=&quot;top:0.03971999999999998em;&quot;&gt;&lt;span class=&quot;style-wrap reset-textstyle textstyle uncramped&quot;&gt;√&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist&quot;&gt;&lt;span style=&quot;top:0em;&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:1em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord textstyle cramped&quot;&gt;&lt;span class=&quot;mord mathit&quot;&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-0.72028em;&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:1em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;reset-textstyle textstyle uncramped sqrt-line&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;baseline-fix&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:1em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;​&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; (can be seen I think by asking the question of when does changing bit i change the output? This happens when the weights for the other n-1 bits times the corresponding input bits sum to less than the weight of the ith bit. This sum can be analyzed using the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Central%20limit%20theorem&quot;&gt;Central limit theorem&lt;/a&gt;, and one can conclude that this occurs with probability &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;/&lt;/mi&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;O(1/\sqrt{n})&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.8002800000000001em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:1.05028em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.02778em;&quot;&gt;O&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathrm&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;mord mathrm&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;sqrt mord&quot;&gt;&lt;span class=&quot;sqrt-sign&quot; style=&quot;top:0.03971999999999998em;&quot;&gt;&lt;span class=&quot;style-wrap reset-textstyle textstyle uncramped&quot;&gt;√&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist&quot;&gt;&lt;span style=&quot;top:0em;&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:1em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord textstyle cramped&quot;&gt;&lt;span class=&quot;mord mathit&quot;&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-0.72028em;&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:1em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;reset-textstyle textstyle uncramped sqrt-line&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;baseline-fix&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:1em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;​&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. This was for one bit. Then the average sensitivity is the sum of this over bits, so we multiply by &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;n&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot;&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, giving the result) – &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=HIKTt9vaElM#t=11m53s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Average sensitivity for degree d polynomial threshold functions (PTF) is still open&lt;/a&gt;, but it was conjectured that it is &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;O(d\sqrt{n})&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.8002800000000001em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:1.05028em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot; style=&quot;margin-right:0.02778em;&quot;&gt;O&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathit&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;sqrt mord&quot;&gt;&lt;span class=&quot;sqrt-sign&quot; style=&quot;top:0.03971999999999998em;&quot;&gt;&lt;span class=&quot;style-wrap reset-textstyle textstyle uncramped&quot;&gt;√&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist&quot;&gt;&lt;span style=&quot;top:0em;&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:1em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord textstyle cramped&quot;&gt;&lt;span class=&quot;mord mathit&quot;&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-0.72028em;&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:1em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;reset-textstyle textstyle uncramped sqrt-line&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;baseline-fix&quot;&gt;&lt;span class=&quot;fontsize-ensurer reset-size5 size5&quot;&gt;&lt;span style=&quot;font-size:1em;&quot;&gt;​&lt;/span&gt;&lt;/span&gt;​&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. He has some proven bounds (which are not as good though). See &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/0909.5011&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;this paper&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Noise%20sensitivity&quot;&gt;Noise sensitivity&lt;/a&gt;. Probability that output changes for a uniformly random input, when changing each bit with probability &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; (called &lt;em&gt;noise rate&lt;/em&gt;). This is like the mutation model in the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Wright-Fisher%20model&quot;&gt;Wright-Fisher model&lt;/a&gt;. See &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://youtu.be/tT00oKP4lk4?t=10m29s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;this video&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=HIKTt9vaElM#t=14m55s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Upper bounds on noise sensitivity gives upper bounds on average sensitivity&lt;/a&gt;. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://youtu.be/HIKTt9vaElM?t=15m23s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;The converse is true for degree d PTFs&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=HIKTt9vaElM&amp;amp;feature=youtu.be&amp;amp;t=16m35s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;This has applications to learning&lt;/a&gt;. Upper bound on noise sensitivity implies Fourier concentration, kind of smoothness.. And this is related to learning algorithms. Their results implies that the class of degree d PTFs is &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Agnostic%20learning&quot;&gt;agnostic lernable&lt;/a&gt; in polynomial time.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=HIKTt9vaElM&amp;amp;feature=youtu.be&amp;amp;t=29m10s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Regularity lemma and pseudorandom generators for PTFs&lt;/a&gt; – &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://youtu.be/HIKTt9vaElM?t=34m48s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;PRG&lt;/a&gt;. version of central limit theorem still holds for weakly indpedenent random variables. They show that these weakly dependent distributions (generated from PRGs) fools the class of linear threshold functions. Meaning that the expectation of the linear threshold function when inputs are under this pseudorandom distribution, this expectation is epsilon-close to the expectation when inputs are truly unfiromly distributed (see &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://youtu.be/HIKTt9vaElM?t=38m12s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;). They then use this to show that CLT also holds for these weakly dependent PR distributions. Results of this kind are called &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Derandomization&quot;&gt;Derandomization&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;These PR distributions have small support (seeds are much smaller than output space).&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://youtu.be/tT00oKP4lk4?t=3m27s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Fourier formula for the total influence&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://youtu.be/42ofUBEozzQ?t=1h11m10s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Fourier formula for the total influence for monotone functions&lt;/a&gt;!&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Poincare%20inequalities&quot;&gt;Poincare inequalities&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;This notion can be seen as an extension of &amp;quot;smoothness&amp;quot; in &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Analysis&quot;&gt;Analysis&lt;/a&gt; to functions on discrete/finite domains&lt;/p&gt;&lt;p&gt;This is essentially the same notion &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Mutational%20robustness&quot;&gt;Mutational robustness&lt;/a&gt;&lt;/p&gt;&lt;p&gt;It is also the basis for the definition &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Generalization%20complexity&quot;&gt;Generalization complexity&lt;/a&gt; by L Franco. In fact the 0th complexity is just the average sensitivity.&lt;/p&gt;</p>