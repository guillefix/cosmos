<p><code>&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=utf-8&quot; /&gt;
&lt;meta name=&quot;generator&quot; content=&quot;TiddlyWiki&quot; /&gt;
&lt;meta name=&quot;tiddlywiki-version&quot; content=&quot;</code>5.1.17<code>&quot; /&gt;
&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot; /&gt;
&lt;meta name=&quot;apple-mobile-web-app-capable&quot; content=&quot;yes&quot; /&gt;
&lt;meta name=&quot;apple-mobile-web-app-status-bar-style&quot; content=&quot;black-translucent&quot; /&gt;
&lt;meta name=&quot;mobile-web-app-capable&quot; content=&quot;yes&quot;/&gt;
&lt;meta name=&quot;format-detection&quot; content=&quot;telephone=no&quot;&gt;
&lt;link id=&quot;faviconLink&quot; rel=&quot;shortcut icon&quot; href=&quot;favicon.ico&quot;&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;static.css&quot;&gt;
&lt;title&gt;</code>Temporal difference learning: Cosmos — Everything there was, there is, <span class="subtitle-dark">and there will be</span><code>&lt;/title&gt;
&lt;/head&gt;
&lt;body class=&quot;tc-body&quot;&gt;
</code><code>
&lt;section class=&quot;tc-story-river&quot;&gt;
</code>
&lt;p&gt;&lt;div class=&quot;tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Reinforcement%20learning &quot; data-tags=&quot;[[Reinforcement learning]]&quot; data-tiddler-title=&quot;Temporal difference learning&quot;&gt;&lt;div class=&quot;tc-tiddler-title&quot;&gt;
&lt;div class=&quot;tc-titlebar&quot;&gt;
&lt;span class=&quot;tc-tiddler-controls&quot;&gt;
&lt;span class=&quot; tc-reveal&quot;&gt;&lt;button aria-label=&quot;more&quot; class=&quot;tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions&quot; title=&quot;More actions&quot;&gt;&lt;/button&gt;&lt;div class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/div&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot;&gt;&lt;button aria-label=&quot;edit&quot; class=&quot;tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit&quot; title=&quot;Edit this tiddler&quot;&gt;&lt;/button&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot;&gt;&lt;button aria-label=&quot;close&quot; class=&quot;tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose&quot; title=&quot;Close this tiddler&quot;&gt;&lt;/button&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;&lt;span class=&quot; tc-reveal&quot;&gt;&lt;button aria-label=&quot;tiddlymap&quot; class=&quot;tc-btn-invisible tc-btn-%24%3A%2Fplugins%2Ffelixhayashi%2Ftiddlymap%2Fmisc%2FquickConnectButton &quot; title=&quot;Toggle TiddlyMap actions&quot;&gt;


&lt;/button&gt;&lt;/span&gt;
&lt;/span&gt;

&lt;span&gt;

&lt;span class=&quot;tc-tiddler-title-icon&quot; style=&quot;fill:;&quot;&gt;

&lt;/span&gt;



&lt;h2 class=&quot;tc-title&quot;&gt;
Temporal difference learning
&lt;/h2&gt;

&lt;/span&gt;

&lt;/div&gt;

&lt;div class=&quot;tc-tiddler-info tc-popup-handle tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/div&gt;
&lt;/div&gt;&lt;div class=&quot; tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/div&gt;
&lt;div class=&quot; tc-reveal&quot;&gt;
&lt;div class=&quot;tc-subtitle&quot;&gt;
&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;cosmos.html&quot;&gt;
cosmos
&lt;/a&gt; 9th April 2018 at 1:27pm
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot; tc-reveal&quot;&gt;
&lt;div class=&quot;tc-tags-wrapper&quot;&gt;&lt;span class=&quot;tc-tag-list-item&quot;&gt;


&lt;span class=&quot;tc-tag-label tc-btn-invisible&quot; draggable=&quot;true&quot; style=&quot;background-color:;
fill:#333333;
color:#333333;&quot;&gt;
 Reinforcement learning
&lt;/span&gt;

&lt;span class=&quot;tc-drop-down tc-reveal&quot; hidden=&quot;true&quot;&gt;&lt;/span&gt;

&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;tc-tiddler-body tc-reveal&quot;&gt;&lt;p&gt;An approach to &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Reinforcement%2520learning.html&quot;&gt;Reinforcement learning&lt;/a&gt; (particularly &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Model-free%2520reinforcement%2520learning.html&quot;&gt;Model-free reinforcement learning&lt;/a&gt;) – &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;amp;index=4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=34m10s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;video&lt;/a&gt;&lt;/p&gt;&lt;p&gt;TD learning is a combination of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Monte%2520Carlo.html&quot;&gt;Monte Carlo&lt;/a&gt; ideas and &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Dynamic%2520programming.html&quot;&gt;Dynamic programming&lt;/a&gt; (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment's dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap).&lt;/p&gt;&lt;p&gt;Here we discuss the basic &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;On-policy%2520learning.html&quot;&gt;On-policy learning&lt;/a&gt;s. See &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Off-policy%2520learning.html&quot;&gt;Off-policy learning&lt;/a&gt; for their extensions to off-policy learning.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://videolectures.net/deeplearning2017_sutton_td_learning/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Sutton -- TD learning&lt;/a&gt; – &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=EeMCEQa85tw&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;DeepMind's Richard Sutton - The Long-term of AI &amp;amp; Temporal-Difference Learning&lt;/a&gt; &lt;small&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://youtu.be/Qgd3OK5DZWI?t=23m30s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Demis Hassabis talks about it here&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;&lt;h1 class=&quot;&quot;&gt;&lt;u&gt;TD &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Policy%2520evaluation.html&quot;&gt;Policy evaluation&lt;/a&gt;&lt;/u&gt;&lt;/h1&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;amp;index=4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h13m&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;intuition for why we update the current value function assuming the value function at the state after one step, instead of updating it the other way&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;TD0&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;amp;index=4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=36m10s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;intro vid&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=dV80NAlEins&amp;amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;amp;index=16#t=19m30s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;vid&lt;/a&gt;&lt;/p&gt;&lt;p&gt;A kind of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Gradient%2520descent.html&quot;&gt;Gradient descent&lt;/a&gt; to converge to solution to V(s) that satisfies &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Bellman%2520equation.html&quot;&gt;Bellman equation&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node60.html&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node60.html&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Proven to work (converge to true value function, in the case of table-lookup representation. But in the case of representing value function in some other ways (parametric function approximation), &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;amp;index=4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=53m10s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;there are subtleties.&lt;/a&gt;&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;amp;index=4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=59m&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Simple example comparing monte carlo vs TD0&lt;/a&gt;.&lt;/h3&gt;&lt;p&gt;If you let TD0 converge on a limited sample (a limited set of episodes from an MDP), it will converge to the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Maximum%2520likelihood.html&quot;&gt;Maximum likelihood&lt;/a&gt; estimate &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Markov%2520reward%2520process.html&quot;&gt;MRP&lt;/a&gt; for that data. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;amp;index=4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h3m30s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;TD makes use of the Markov property&lt;/a&gt;&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;Optimality of TD(0)&lt;/h3&gt;&lt;p&gt;See section 6.3 of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Sutton-Barto.html&quot;&gt;Sutton-Barto&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;n-step look-ahead&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;amp;index=4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h12m5s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;intro vid&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;amp;index=4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h16m&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;video&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We can take &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;n&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot;&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; steps of the (unknown) MDP, instead of 1. Monte Carlo &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Model-free%2520reinforcement%2520learning.html&quot;&gt;Model-free reinforcement learning&lt;/a&gt; is when &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;→&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∞&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;n \rightarrow \infty&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;→&lt;/span&gt;&lt;span class=&quot;mord mathrm&quot;&gt;∞&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;TD(lambda).html&quot;&gt;TD(lambda)&lt;/a&gt;&lt;/u&gt;&lt;/h2&gt;&lt;h1 class=&quot;&quot;&gt;&lt;u&gt;TD control&lt;/u&gt;&lt;/h1&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;amp;index=5#t=38m45s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;introduction to TD learning for control&lt;/a&gt;&lt;/p&gt;&lt;p&gt;TD prediction + policy improvement (&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;Generalized%2520policy%2520iteration.html&quot;&gt;GPI&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Sarsa.html&quot;&gt;Sarsa&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Related with &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;Actor-critic%2520method.html&quot;&gt;Actor-critic method&lt;/a&gt;s&lt;/p&gt;&lt;/div&gt;


&lt;/div&gt;

&lt;/p&gt;
<code>
&lt;/section&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></p>