<p><em>aka TD(<span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>位</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">位</span></span></span></span></span>)</em></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h12m5s" rel="noopener noreferrer" target="_blank">intro vid</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h22m" rel="noopener noreferrer" target="_blank">Averaging n-step returns</a> is better than just one choice of <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span></span>. This is what The TD(<span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>位</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">位</span></span></span></span></span>) algorithm achieves, efficiently!</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h29m" rel="noopener noreferrer" target="_blank">TD lambda can be done by updating only at the end of the episde</a>, like for Monte Carlo. This isn't computationally efficient</p><p>To make computationally efficient, one can update by looking only to the past with  <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h30m25s" rel="noopener noreferrer" target="_blank">backward view of TD lambda algorithm</a>, combines frequency with recency to define <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Eligibility%20traces">Eligibility traces</a></p><p>See <a class="tc-tiddlylink-external" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" rel="noopener noreferrer" target="_blank">notes</a> for more details and proofs of equivalence of the two interpretations. </p><p>There are new exactly equivalent <strong>online</strong> TD(lambda) algorithms!</p>