<p><em>a.k.a. representation independent learning</em></p><p><strong>Improper learning</strong> refers to a learning algorithm which outputs hypotheses from a class different from the concept class</p><p>We define <em>concept class</em> as the set of possible <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Target%20function">Target function</a>s. We define <em>hypothesis class</em> as the set of possible outputs for the <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Learning%20algorithm">Learning algorithm</a>. </p><p><small>
(In <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Probably%20approximately%20correct">PAC learning</a>, one typically assumes that the hypothesis and concept classes are the same (although not always; technically one just assumes <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Realizability%20assumption">realizability</a>, see <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Understanding%20machine%20learning">UML</a>, <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Wolpert">Wolpert</a>'s <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#No%20free%20lunch%20theorem">No free lunch theorem</a> article),
 in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Agnostic%20learning">Agnostic learning</a>, one is trying to do as well as any hypothesis in a given hypothesis class, while one doesn't assume anything about the concept class). I suppose one can make an extension of agnostic PAC where one can output things from a hypothesis class, and then compare with things in a different class and/or assume something about the concept class.
</small></p><p>See <a class="tc-tiddlylink-external" href="http://www.cs.ox.ac.uk/people/varun.kanade/teaching/AML-HT2017/lectures/lecture01.pdf#page=11" rel="noopener noreferrer" target="_blank">here</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1311.2272" rel="noopener noreferrer" target="_blank">From average case complexity to improper learning complexity</a></p><p><a class="tc-tiddlylink-external" href="https://stats.stackexchange.com/questions/152181/what-does-improper-learning-mean-in-the-context-of-statistical-learning-theory-a" rel="noopener noreferrer" target="_blank">https://stats.stackexchange.com/questions/152181/what-does-improper-learning-mean-in-the-context-of-statistical-learning-theory-a</a></p><p>... One may wonder, whether this is always the case, i.e., can we always identify a larger
hypothesis class from which we can identify a consistent learner?
In <a class="tc-tiddlylink-external" href="http://www.cs.ox.ac.uk/people/varun.kanade/teaching/AML-HT2017/lectures/lecture05.pdf" rel="noopener noreferrer" target="_blank">this lecture</a>, weâ€™ll answer this question in the negative, provided a certain widely believed
assumption in cryptography holds. We will show that there are concept classes that cannot
be efficiently PAC-learnt, even in the case improper learning where the output hypothesis is
allowed to come from any polynomially evaluatable hypothesis class.</p>