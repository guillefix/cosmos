<p>&lt;p&gt;&lt;em&gt;a.k.a. representation independent learning&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Improper learning&lt;/strong&gt; refers to a learning algorithm which outputs hypotheses from a class different from the concept class&lt;/p&gt;&lt;p&gt;We define &lt;em&gt;concept class&lt;/em&gt; as the set of possible &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Target%20function&quot;&gt;Target function&lt;/a&gt;s. We define &lt;em&gt;hypothesis class&lt;/em&gt; as the set of possible outputs for the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Learning%20algorithm&quot;&gt;Learning algorithm&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;small&gt;
(In &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Probably%20approximately%20correct&quot;&gt;PAC learning&lt;/a&gt;, one typically assumes that the hypothesis and concept classes are the same (although not always; technically one just assumes &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Realizability%20assumption&quot;&gt;realizability&lt;/a&gt;, see &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Understanding%20machine%20learning&quot;&gt;UML&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Wolpert&quot;&gt;Wolpert&lt;/a&gt;'s &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#No%20free%20lunch%20theorem&quot;&gt;No free lunch theorem&lt;/a&gt; article),
 in &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Agnostic%20learning&quot;&gt;Agnostic learning&lt;/a&gt;, one is trying to do as well as any hypothesis in a given hypothesis class, while one doesn't assume anything about the concept class). I suppose one can make an extension of agnostic PAC where one can output things from a hypothesis class, and then compare with things in a different class and/or assume something about the concept class.
&lt;/small&gt;&lt;/p&gt;&lt;p&gt;See &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.cs.ox.ac.uk/people/varun.kanade/teaching/AML-HT2017/lectures/lecture01.pdf#page=11&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1311.2272&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;From average case complexity to improper learning complexity&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://stats.stackexchange.com/questions/152181/what-does-improper-learning-mean-in-the-context-of-statistical-learning-theory-a&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://stats.stackexchange.com/questions/152181/what-does-improper-learning-mean-in-the-context-of-statistical-learning-theory-a&lt;/a&gt;&lt;/p&gt;&lt;p&gt;... One may wonder, whether this is always the case, i.e., can we always identify a larger
hypothesis class from which we can identify a consistent learner?
In &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.cs.ox.ac.uk/people/varun.kanade/teaching/AML-HT2017/lectures/lecture05.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;this lecture&lt;/a&gt;, weâ€™ll answer this question in the negative, provided a certain widely believed
assumption in cryptography holds. We will show that there are concept classes that cannot
be efficiently PAC-learnt, even in the case improper learning where the output hypothesis is
allowed to come from any polynomially evaluatable hypothesis class.&lt;/p&gt;</p>