<p>See page 363 on Li&amp;Vitanyi kolmogorov compleixty book</p><p>starting with the f k ’s ordered by decreasing probability according to P (H k ) = 1/k(k + 1), as the list f 1 , f 2 , . . . . Similar to the classical Gold method, after receiv- ing each new example we eliminate all remaining f k ’s that are inconsis- tent from the beginning onward up to the position of the first consistent function. When we receive a new example e, set D := D, e, and repeat this process. Eventually, the remaining first function in the enumeration is a copy of f and it does not change any more. This algorithm is called learning by enumeration. One learns more and more about the unknown target function and approximates it until the correct identification has been achieved. This learning model is called learning in the limit. The learner eventually learns the concept exactly but never knows when that has happened. This deceptively simple idea has generated a large body of sophisticated literature.</p><p>One can use the universal prior (see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Universal%20inductive%20inference">Universal inductive inference</a>), as the prior over hypotheses in this case.</p>