<p>&lt;p&gt;An architecture to train generative neural networks, i.e. &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Artificial%20neural%20network&quot;&gt;neural networks&lt;/a&gt; which act as &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Generative%20model&quot;&gt;Generative model&lt;/a&gt;s, i.e. their inputs are &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Latent%20variable&quot;&gt;Latent variable&lt;/a&gt;s, and their output is the observed data.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://deephunt.in/the-gan-zoo-79597dc8c347&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://deephunt.in/the-gan-zoo-79597dc8c347&lt;/a&gt; â€“ &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://github.com/wiseodd/generative-models&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://github.com/wiseodd/generative-models&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/pdf/1701.00160.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;NIPS tutorial&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://reiinakano.github.io/gan-playground/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;GAN on browser&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Adversarial networks&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;The way the network is trained is by having the generative network produce images, while training a different discriminative network to discriminate between real and generated images. In this way, the discriminative network can be used as a very good cost function, which penalized generated images which are distinguishably different from the real images that the generative network is train to model.&lt;/p&gt;&lt;p&gt;We are in effect learning the cost function&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=QPkb5VcgXAM#t=14m30&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;video&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Trining GANs&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=X1mUN6dD8uE&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;NIPS 2016 Workshop on Adversarial Training - Soumith Chintala - How to train a GAN&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=QPkb5VcgXAM#t=18m05&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Trained&lt;/a&gt; via &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Gradient%20descent&quot;&gt;Gradient descent&lt;/a&gt; and &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Backpropagation&quot;&gt;Backpropagation&lt;/a&gt;&lt;/p&gt;&lt;p&gt;I think that the discriminator needs to have a separate cost function which measures the number of images the discriminator missclassified as being real or generated. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=QPkb5VcgXAM#t=19m47&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Discriminator is optimized to not be fooled by the generator&lt;/a&gt;&lt;/p&gt;&lt;p&gt;On the other hand, the generative network uses the discriminative network as cost. &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=QPkb5VcgXAM#t=19m20&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Generator to fool discriminator, i.e. it is trained to maximize the mistakes the the discriminator does&lt;/a&gt;, that's why they are called &lt;strong&gt;adversarial&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The discriminator is &lt;em&gt;teaching&lt;/em&gt; the generator, and it's adapting to the generator's knowledge and flaws. &lt;em&gt;Machine teaching&lt;/em&gt;, not just machine learning.&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;Alternating &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Optimization&quot;&gt;Optimization&lt;/a&gt;&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=QPkb5VcgXAM#t=19m59&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;video&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1606.03498&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Improved Techniques for Training GANs&lt;/a&gt; &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=QPkb5VcgXAM#t=45m30&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;vid&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Theoretical properties&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;If you have an optimal discriminator, the generator minimizes the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Jensen-Shanon%20divergence&quot;&gt;Jensen-Shanon divergence&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Variants&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=QPkb5VcgXAM#t=23m15&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Original GANs were diffucult to train&lt;/a&gt; &lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;Class-conditional GANs&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;They are &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Supervised%20learning&quot;&gt;supervised&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1506.05751&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks&lt;/a&gt; &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=QPkb5VcgXAM#t=24m&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;vid&lt;/a&gt;&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;Video-prediction GANs&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1511.05440&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Deep multi-scale video prediction beyond mean square error&lt;/a&gt;
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=QPkb5VcgXAM#t=25m45s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;vid&lt;/a&gt;&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;DCGANs&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1511.06434&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=QPkb5VcgXAM#t=30m30s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Latent space arithmetic&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;In-painting GANs&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1604.07379&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Context Encoders: Feature Learning by Inpainting&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=QPkb5VcgXAM#t=39m05&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;video&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Applications&lt;/u&gt;&lt;/h2&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Feature%20learning&quot;&gt;Feature learning&lt;/a&gt; for &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Semi-supervised%20learning&quot;&gt;Semi-supervised learning&lt;/a&gt;&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=QPkb5VcgXAM#t=33m25&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;GANs for feature learning&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=pqkpIfu36Os&amp;amp;list=PLujxSBD-JXgnqDD1n-V30pKtp6Q886x7e&amp;amp;index=108&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Two Minute Papers - Image Editing with Generative Adversarial Networks&lt;/a&gt;&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;Disentangling representations&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=QPkb5VcgXAM#t=41m50s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;vid&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1606.03657&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets&lt;/a&gt; (from &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#OpenAI&quot;&gt;OpenAI&lt;/a&gt;)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.inference.vc/how-to-train-your-generative-models-why-generative-adversarial-networks-work-so-well-2/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;http://www.inference.vc/how-to-train-your-generative-models-why-generative-adversarial-networks-work-so-well-2/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;They are similar to &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Autoencoder&quot;&gt;Autoencoder&lt;/a&gt;s but we learn the cost function, instead of just using l2 loss  (&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=QPkb5VcgXAM#t=58m50&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;vid&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=deyOX6Mt_As&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;vid&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1406.2661&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Generative Adversarial Networks&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.wikiwand.com/en/Generative_adversarial_networks&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://www.wikiwand.com/en/Generative_adversarial_networks&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=QPkb5VcgXAM#t=51m20&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;the future&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1610.01945&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Connecting Generative Adversarial Networks and Actor-Critic Methods&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;More variations and others&lt;/p&gt;&lt;p&gt;URL list from Sunday, May. 21 2017 16:41 PM&lt;/p&gt;&lt;p&gt;To copy this list, type [Ctrl] A, then type [Ctrl] C. &lt;/p&gt;&lt;p&gt;1603.08155.pdf
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/pdf/1603.08155.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/pdf/1603.08155.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;1703.10593.pdf
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/pdf/1703.10593.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/pdf/1703.10593.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;1610.09003.pdf
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/pdf/1610.09003.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/pdf/1610.09003.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;RL Course by David Silver - Lecture 5: Model Free Control - YouTube
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;index=5&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;amp;index=5&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Teaching
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Lecture 1a - Introduction [Phil Blunsom] - YouTube
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=RP3tZFcC2e8&amp;amp;list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://www.youtube.com/watch?v=RP3tZFcC2e8&amp;amp;list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[1612.03242] StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1612.03242&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/1612.03242&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[1703.06412] TAC-GAN - Text Conditioned Auxiliary Classifier Generative Adversarial Network
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1703.06412&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/1703.06412&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[1703.06676] I2T2I: Learning Text to Image Synthesis with Textual Data Augmentation
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1703.06676&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/1703.06676&lt;/a&gt;&lt;/p&gt;&lt;p&gt;I2T2I: LEARNING TEXT TO IMAGE SYNTHESIS WITH TEXTUAL DATA AUGMENTATION
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/pdf/1703.06676.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/pdf/1703.06676.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Generative Adversarial Text to Image Synthesis
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/pdf/1605.05396.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/pdf/1605.05396.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Reed: Generative adversarial text to image synthesis - Google Scholar
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://scholar.google.co.uk/scholar?start=70&amp;amp;hl=en&amp;amp;as_sdt=0,5&amp;amp;sciodt=0,5&amp;amp;cites=8255440757806230750&amp;amp;scipsc&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://scholar.google.co.uk/scholar?start=70&amp;amp;hl=en&amp;amp;as_sdt=0,5&amp;amp;sciodt=0,5&amp;amp;cites=8255440757806230750&amp;amp;scipsc&lt;/a&gt;=&lt;/p&gt;&lt;p&gt;Learning What and Where to Draw
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://papers.nips.cc/paper/6111-learning-what-and-where-to-draw&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;http://papers.nips.cc/paper/6111-learning-what-and-where-to-draw&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Generating Visual Explanations | SpringerLink
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://link.springer.com/chapter/10.1007/978-3-319-46493-0_1&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://link.springer.com/chapter/10.1007/978-3-319-46493-0_1&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[1609.09444] Contextual RNN-GANs for Abstract Reasoning Diagram Generation
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1609.09444&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/1609.09444&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[1702.03431] Crossing Nets: Dual Generative Models with a Shared Latent Space for Hand Pose Estimation
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1702.03431&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/1702.03431&lt;/a&gt;&lt;/p&gt;&lt;p&gt;DISCO Nets : DISsimilarity COefficients Networks
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://papers.nips.cc/paper/6143-disco-nets-dissimilarity-coefficients-networks&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;http://papers.nips.cc/paper/6143-disco-nets-dissimilarity-coefficients-networks&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[1704.06933] Adversarial Neural Machine Translation
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1704.06933&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/1704.06933&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[1702.04125] One-Step Time-Dependent Future Video Frame Prediction with a Convolutional Encoder-Decoder Neural Network
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1702.04125&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/1702.04125&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[1703.06029] Towards Diverse and Natural Image Descriptions via a Conditional GAN
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://arxiv.org/abs/1703.06029&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/1703.06029&lt;/a&gt;&lt;/p&gt;&lt;p&gt;cvpr17_summarization.pdf
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://web.engr.oregonstate.edu/~sinisa/research/publications/cvpr17_summarization.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;http://web.engr.oregonstate.edu/~sinisa/research/publications/cvpr17_summarization.pdf&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://make.girls.moe/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;http://make.girls.moe/&lt;/a&gt;
&lt;/p&gt;</p>