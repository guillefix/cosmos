<p>&lt;p&gt;The architecture of the GAN used in this study was inspired by recent work in this field [18, 41]. According to original studies, the adversarial network and the autoencoder are trained jointly with SGD in two phases – the reconstruction phase and the regularization phase – executed on each mini-batch. In the reconstruction phase, the autoencoder updates the encoder and the decoder to minimize the reconstruction error of the inputs. In the regularization phase, the adversarial network first updates its &lt;strong&gt;discriminative network to tell apart the true samples (generated using the prior) from the generated samples (the hidden codes computed by the autoencoder)&lt;/strong&gt;. So the objective is like that of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Variational%20autoencoder&quot;&gt;Variational autoencoder&lt;/a&gt;. The adversarial network then updates its generator (which is also the encoder of the autoencoder) to confuse the discriminative network. Once the training procedure is done, the decoder of the autoencoder will define a generative model that maps the imposed prior of p(z) to the data distribution.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.impactjournals.com/oncotarget/index.php?journal=oncotarget&amp;amp;page=article&amp;amp;op=view&amp;amp;path%5B0%5D=14073&amp;amp;path%5B1%5D=44886&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;The cornucopia of meaningful leads: Applying deep adversarial autoencoders for new molecule development in oncology&lt;/a&gt;&lt;/p&gt;</p>