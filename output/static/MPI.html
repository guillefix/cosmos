<p>&lt;p&gt;Message programming interface&lt;/p&gt;&lt;p&gt;Distributed memory &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Parallel%20programming&quot;&gt;Parallel programming&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Multiple processes&lt;/p&gt;&lt;p&gt;&lt;code&gt;MPI_Init()&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;code&gt;MPI_Comm_rank()&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;code&gt;MPI_Finalize()&lt;/code&gt;&lt;/p&gt;&lt;p&gt;MPI_Comm_size
reports the size of the group of 
processes associated with the specified &lt;strong&gt;communicator&lt;/strong&gt; (a group of processes which communicate with each other).&lt;/p&gt;&lt;p&gt;&lt;code&gt;MPI_Send&lt;/code&gt; &amp;lt;â€“&amp;gt; &lt;code&gt;MPI_Recv&lt;/code&gt;&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;Deadlocks&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;When a process makes a call to MPI_Recv , it will wait patiently until a matching send is posted.&lt;/li&gt;&lt;li&gt;Similarly you must assume that when a process makes a call to MPI_Send it will wait until a matching recv is posted&lt;/li&gt;&lt;/ul&gt;&lt;h3 class=&quot;&quot;&gt;Reduces&lt;/h3&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;code&gt;MPI_Reduce&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;code&gt;MPI_Allreduce&lt;/code&gt;, MPI_Allreduce(const void *sendbuf, void *recvbuf, int count,
                  MPI_Datatype datatype, MPI_Op op, MPI_Comm comm)&lt;/p&gt;</p>