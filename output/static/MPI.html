<p>Message programming interface</p><p>Distributed memory <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Parallel%20programming">Parallel programming</a></p><p>Multiple processes</p><p><code>MPI_Init()</code></p><p><code>MPI_Comm_rank()</code></p><p><code>MPI_Finalize()</code></p><p>MPI_Comm_size
reports the size of the group of 
processes associated with the specified <strong>communicator</strong> (a group of processes which communicate with each other).</p><p><code>MPI_Send</code> &lt;â€“&gt; <code>MPI_Recv</code></p><h3 class="">Deadlocks</h3><ul><li>When a process makes a call to MPI_Recv , it will wait patiently until a matching send is posted.</li><li>Similarly you must assume that when a process makes a call to MPI_Send it will wait until a matching recv is posted</li></ul><h3 class="">Reduces</h3><p><a class="tc-tiddlylink-external" href="http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/" rel="noopener noreferrer" target="_blank">http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/</a></p><p><code>MPI_Reduce</code></p><p><code>MPI_Allreduce</code>, MPI_Allreduce(const void *sendbuf, void *recvbuf, int count,
                  MPI_Datatype datatype, MPI_Op op, MPI_Comm comm)</p>