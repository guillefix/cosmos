<p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Regression%20analysis">Regression analysis</a>.</p><h2 class=""><strong>Least mean squares</strong></h2><p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Least-squares">Least-squares</a></p><p>Use <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Matrix%20calculus">Matrix calculus</a> for optimization: leads to <strong>normal equations</strong> (analytical solution to least squares), etc.</p><p>See <a class="tc-tiddlylink-external" href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" rel="noopener noreferrer" target="_blank">here</a></p><h3 class=""><u>Probabilitistic interpretation</u></h3><p>See <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=HZ4cvaztQEs&amp;index=3&amp;list=PLA89DCFA6ADACE599#t=28m40s" rel="noopener noreferrer" target="_blank">here</a>. We assume the errors between model values and actual values follow a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Normal%20distribution">Normal distribution</a>. This implies the data would be distributed as a Gaussian with mean <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi mathvariant="normal">Θ</mi><mi>T</mi></msup><mi>x</mi></mrow><annotation encoding="application/x-tex">\Theta^T x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathrm">Θ</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit">x</span></span></span></span></span>, and a certain <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Variance">Variance</a>. See <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=HZ4cvaztQEs&amp;index=3&amp;list=PLA89DCFA6ADACE599#t=33m" rel="noopener noreferrer" target="_blank">here</a>. With this we <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=HZ4cvaztQEs&amp;index=3&amp;list=PLA89DCFA6ADACE599#t=41m20s" rel="noopener noreferrer" target="_blank">define</a> the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Likelihood%20function">Likelihood function</a>. One can then derive that <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Maximum%20likelihood">maximizing likelihood</a> is the same as mimimizing mean squares.</p><p><u>Doing linear regression in Python with <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Sklearn">Sklearn</a></u></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=JcI5Vnw0b2c&amp;index=2&amp;list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v" rel="noopener noreferrer" target="_blank">Regression Intro - Practical Machine Learning Tutorial with Python p.2</a></p><p><sub><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=V59bYfIomVk&amp;list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&amp;index=7" rel="noopener noreferrer" target="_blank">Regression How it Works - Practical Machine Learning Tutorial with Python p.7</a></sub></p><hr><p>There exists a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Nonparametric%20statistics">nonparametric</a> generalization of linear regression: <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Locally-weighted%20linear%20regression">Locally-weighted linear regression</a></p><p>The different features should not be linearly dependent in linear models, as parameters would be badly behaved</p>