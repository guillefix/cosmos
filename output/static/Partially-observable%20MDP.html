<p><a name="Partially-observable MDP">
<div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Markov%20decision%20process " data-tags="[[Markov decision process]]" data-tiddler-title="Partially-observable MDP"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class=" tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class=" tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class=" tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="tiddlymap" class=" tc-btn-%24%3A%2Fplugins%2Ffelixhayashi%2Ftiddlymap%2Fmisc%2FquickConnectButton " title="Toggle TiddlyMap actions">


</button></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Partially-observable MDP
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="#cosmos">
cosmos
</a> 4th November 2016 at 9:43am
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p>A partially-observabe <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Markov%20decision%20process">Markov decision process</a> is a Markov decision process where the state is only partially observable by the actor, so that the policy can only depend on a function of the state, which looses some of the state's <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Information">Information</a></p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Kalman%20filter">Kalman filter</a>s and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#LQG%20control">LQG control</a></u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=46m50" rel="noopener noreferrer" target="_blank">video</a></p><p>A type of reinforcement learning, where we don't observe the state explicitly!</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=53m40s" rel="noopener noreferrer" target="_blank">Want to estimate actual state, given the noisy and incomplete measurements of the state</a>. Can use the method of marginalization, as used in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Factor%20analysis%20model">Factor analysis model</a>s. However, it is very computationally expensive. Instead we use a <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=55m52s" rel="noopener noreferrer" target="_blank">Kalman filter</a> model, which turns out to be a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Hidden%20Markov%20model">Hidden Markov model</a> with continuous states.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=57m35s" rel="noopener noreferrer" target="_blank">Outline of Kalman filter</a></p><ul><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=1h22s" rel="noopener noreferrer" target="_blank">Predict step</a></li><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=1h03m30s" rel="noopener noreferrer" target="_blank">Update step</a></li></ul><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=1h7m40s" rel="noopener noreferrer" target="_blank">Intuition</a>. I think this can be seen through the lens of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Sufficient%20statistic">Sufficient statistic</a>s</p><p>Kalman filter + LQR = <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#LQG%20control">LQG control</a> &lt;- <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=1h8m55s" rel="noopener noreferrer" target="_blank">video</a> &lt;â€“ <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;list=PLA89DCFA6ADACE599&amp;index#t=1h9m50s" rel="noopener noreferrer" target="_blank">how to solve</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;list=PLA89DCFA6ADACE599&amp;index#t=1h14m32s" rel="noopener noreferrer" target="_blank">Separation principle</a> of LQG control</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=0m50s" rel="noopener noreferrer" target="_blank">recap</a></p><h3 class=""><u>Other POMDP</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=3m55s" rel="noopener noreferrer" target="_blank">In general finding optimal policies of POMDPs is NP hard</a></p></div>


</div>


</a></p>