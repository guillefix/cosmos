<p>&lt;p&gt;A partially-observabe &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Markov%20decision%20process&quot;&gt;Markov decision process&lt;/a&gt; is a Markov decision process where the state is only partially observable by the actor, so that the policy can only depend on a function of the state, which looses some of the state's &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Information&quot;&gt;Information&lt;/a&gt;&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Kalman%20filter&quot;&gt;Kalman filter&lt;/a&gt;s and &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#LQG%20control&quot;&gt;LQG control&lt;/a&gt;&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;amp;index=19&amp;amp;list=PLA89DCFA6ADACE599#t=46m50&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;video&lt;/a&gt;&lt;/p&gt;&lt;p&gt;A type of reinforcement learning, where we don't observe the state explicitly!&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;amp;index=19&amp;amp;list=PLA89DCFA6ADACE599#t=53m40s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Want to estimate actual state, given the noisy and incomplete measurements of the state&lt;/a&gt;. Can use the method of marginalization, as used in &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Factor%20analysis%20model&quot;&gt;Factor analysis model&lt;/a&gt;s. However, it is very computationally expensive. Instead we use a &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;amp;index=19&amp;amp;list=PLA89DCFA6ADACE599#t=55m52s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Kalman filter&lt;/a&gt; model, which turns out to be a &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Hidden%20Markov%20model&quot;&gt;Hidden Markov model&lt;/a&gt; with continuous states.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;amp;index=19&amp;amp;list=PLA89DCFA6ADACE599#t=57m35s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Outline of Kalman filter&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;amp;index=19&amp;amp;list=PLA89DCFA6ADACE599#t=1h22s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Predict step&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;amp;index=19&amp;amp;list=PLA89DCFA6ADACE599#t=1h03m30s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Update step&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;amp;index=19&amp;amp;list=PLA89DCFA6ADACE599#t=1h7m40s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Intuition&lt;/a&gt;. I think this can be seen through the lens of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Sufficient%20statistic&quot;&gt;Sufficient statistic&lt;/a&gt;s&lt;/p&gt;&lt;p&gt;Kalman filter + LQR = &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#LQG%20control&quot;&gt;LQG control&lt;/a&gt; &amp;lt;- &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;amp;index=19&amp;amp;list=PLA89DCFA6ADACE599#t=1h8m55s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;video&lt;/a&gt; &amp;lt;â€“ &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;amp;list=PLA89DCFA6ADACE599&amp;amp;index#t=1h9m50s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;how to solve&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;amp;list=PLA89DCFA6ADACE599&amp;amp;index#t=1h14m32s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Separation principle&lt;/a&gt; of LQG control&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=yCqPMD6coO8&amp;amp;index=20&amp;amp;list=PLA89DCFA6ADACE599#t=0m50s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;recap&lt;/a&gt;&lt;/p&gt;&lt;h3 class=&quot;&quot;&gt;&lt;u&gt;Other POMDP&lt;/u&gt;&lt;/h3&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=yCqPMD6coO8&amp;amp;index=20&amp;amp;list=PLA89DCFA6ADACE599#t=3m55s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;In general finding optimal policies of POMDPs is NP hard&lt;/a&gt;&lt;/p&gt;</p>