<p>&lt;p&gt;See post: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://medium.com/@guillefix/learn-in-theory-3c88086ab388&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Learn in theory&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Mostly studies ways of ensuring good generalization for learning algorithms. See more at &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Computational%20learning%20theory&quot;&gt;Computational learning theory&lt;/a&gt; and &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Probably%20approximately%20correct&quot;&gt;Probably approximately correct&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We have several approaches for &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Generalization%20error%20bound&quot;&gt;Generalization error bound&lt;/a&gt;s (see more detail &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://gingkoapp.com/app#5bb786ea4176d5035eafbfb9&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;). See below.&lt;/p&gt;&lt;p&gt;General principle: The more that the bound depends on, the tighter it can be&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr class=&quot;evenRow&quot;&gt;&lt;td&gt;Note that bounds dependent on training sample are basically bounds that depend on the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Data%20distribution&quot;&gt;Data distribution&lt;/a&gt;, but where we estimate the data-distribution-dependent bound from the sample, using some &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Concentration%20inequality&quot;&gt;Concentration inequality&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;strong&gt;Realizable learning&lt;/strong&gt;&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;where we assume the hypothesis class and data distribution (e.g. target function) are such that we can reach &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;0&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.64444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:0.64444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathrm&quot;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Empirical%20risk&quot;&gt;Empirical risk&lt;/a&gt;. This leads to &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Probably%20approximately%20correct&quot;&gt;PAC learning&lt;/a&gt;
&lt;u&gt;&lt;strong&gt;Data-dependent realizable learning bounds&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Depends on data distribution. (Deterministc case: Depends on target function. )&lt;ul&gt;&lt;li&gt;Analysis of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Occam%20algorithm&quot;&gt;Occam algorithm&lt;/a&gt;s. (for deterministic case)&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Rademacher%20complexity&quot;&gt;Rademacher complexity&lt;/a&gt; analysis (average version)&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Depends on distribution over data distributions. See analysis when assuming distribution overt target functions follows the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Universal%20probability&quot;&gt;Universal probability&lt;/a&gt; distribution. Can also do &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Bayesian%20statistics&quot;&gt;Bayesian&lt;/a&gt; analysis in this case.&lt;/li&gt;&lt;li&gt;Dependent on training sample, but not on algorithm&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Rademacher%20complexity&quot;&gt;Rademacher complexity&lt;/a&gt; analysis. Depends on hypothesis class and training sample S&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Depends on training sample, and on algorithm&lt;ul&gt;&lt;li&gt;Elementary &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#PAC-Bayes&quot;&gt;PAC-Bayes&lt;/a&gt; bound. Depends on the output of the learning algorithm, &lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;h&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.69444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:0.69444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot;&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/li&gt;&lt;li&gt;Realizable &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#PAC-Bayes&quot;&gt;PAC-Bayes&lt;/a&gt; bound. Depends on likelihood of training sample&lt;/li&gt;&lt;li&gt;Realizable &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Structural%20risk%20minimization&quot;&gt;Structural risk minimization&lt;/a&gt;. Depends on the class to which output belongs. Occam algorithms can be analyzed like this also I think.&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Minimum%20description%20length&quot;&gt;Minimum description length&lt;/a&gt; is a special case.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;strong&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Agnostic%20learning&quot;&gt;Agnostic learning&lt;/a&gt;&lt;/strong&gt;&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;where we don't assume anything about the data distribution, and just ask: how well can I do relative to the function that does best (has lowest true risk) in a given hypothesis class. It has a related formulation, where we instead ask how much does the true risk differ from the empirical risk. The error in the former can be shown to be at most twice the error in the later.&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;strong&gt;Data-dependent agnostic learning&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Depends on data distribution (Deterministic case: Depends on target function.)&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Rademacher%20complexity&quot;&gt;Rademacher complexity&lt;/a&gt; analysis (average version))&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Depends on distribution over data distributions.&lt;/li&gt;&lt;li&gt;Dependent on training sample, but not on algorithm&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Rademacher%20complexity&quot;&gt;Rademacher complexity&lt;/a&gt; analysis. Depends on hypothesis class and training sample S&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Depends on the training sample and the algorithm&lt;ul&gt;&lt;li&gt;Agnostic &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#PAC-Bayes&quot;&gt;PAC-Bayes&lt;/a&gt; bounds&lt;/li&gt;&lt;li&gt;Agnostic &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Structural%20risk%20minimization&quot;&gt;Structural risk minimization&lt;/a&gt;. Depends on the class to which output belongs.&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Minimum%20description%20length&quot;&gt;Minimum description length&lt;/a&gt; is a special case.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Sensitivity&quot;&gt;Sensitivity&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Algorithmic%20robustness&quot;&gt;Algorithmic robustness&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;(well technically, what's called &amp;quot;realizability&amp;quot; tends to be that the true risk is 0, while here I am using it as meaning that the empirical risk is 0)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Structural%20risk%20minimization&quot;&gt;Structural risk minimization&lt;/a&gt; is interesting because it only depends on the data distribution and algorithm kind of implicitly (well kinda how sample-dependent bounds depend on the data distribution implicitly.)&lt;/p&gt;</p>