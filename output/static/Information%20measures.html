<p>&lt;ul&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Entropy&quot;&gt;Entropy&lt;/a&gt;  &lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Joint%20entropy&quot;&gt;Joint entropy&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Conditional%20entropy&quot;&gt;Conditional entropy&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Mutual%20information&quot;&gt;Mutual information&lt;/a&gt; (the difference between the entropy and the conditional entropy. I.e the decrease in uncertainty on a random variable when you learn about another random variable. I.e. the information you gain on a random variable from another RV) Measure of dependence.&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Conditional%20mutual%20information&quot;&gt;Conditional mutual information&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Relative%20entropy&quot;&gt;Relative entropy&lt;/a&gt;. Mututal information is a special case. Defines a measure of &amp;quot;distance&amp;quot; between probabiliy distributions. Applications in estimating hypothesis testing errors and in large deviation theory.&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Cross%20entropy&quot;&gt;Cross entropy&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=6RJP5m3m1XA&amp;amp;index=7&amp;amp;list=PLJfu_xpF92pvTfcJAILr5Kg1ptMvHUnft&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Shannon's Information Measures&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/lFj7iId.png&quot;&gt;
&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=6RJP5m3m1XA&amp;amp;index=7&amp;amp;list=PLJfu_xpF92pvTfcJAILr5Kg1ptMvHUnft#t=16m13s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;explanation&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=L6S2LO7S-7U&amp;amp;list=PLJfu_xpF92pvTfcJAILr5Kg1ptMvHUnft&amp;amp;index=8&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Continuity of Shannon's Information Measures &lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=tIt4pJAbCDQ&amp;amp;list=PLJfu_xpF92pvTfcJAILr5Kg1ptMvHUnft&amp;amp;index=13&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Some Useful Information Inequalities&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.tandfonline.com/doi/abs/10.1080/00207166808803030?journalCode=gcom20&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Three approaches to the quantitative definition of information&lt;/a&gt;&lt;/p&gt;</p>