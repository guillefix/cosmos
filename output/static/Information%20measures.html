<ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Entropy">Entropy</a>  </li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Joint%20entropy">Joint entropy</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Conditional%20entropy">Conditional entropy</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Mutual%20information">Mutual information</a> (the difference between the entropy and the conditional entropy. I.e the decrease in uncertainty on a random variable when you learn about another random variable. I.e. the information you gain on a random variable from another RV) Measure of dependence.</li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Conditional%20mutual%20information">Conditional mutual information</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Relative%20entropy">Relative entropy</a>. Mututal information is a special case. Defines a measure of &quot;distance&quot; between probabiliy distributions. Applications in estimating hypothesis testing errors and in large deviation theory.</li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Cross%20entropy">Cross entropy</a></li></ul><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=6RJP5m3m1XA&amp;index=7&amp;list=PLJfu_xpF92pvTfcJAILr5Kg1ptMvHUnft" rel="noopener noreferrer" target="_blank">Shannon's Information Measures</a></p><p><img src="http://i.imgur.com/lFj7iId.png">
<a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=6RJP5m3m1XA&amp;index=7&amp;list=PLJfu_xpF92pvTfcJAILr5Kg1ptMvHUnft#t=16m13s" rel="noopener noreferrer" target="_blank">explanation</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=L6S2LO7S-7U&amp;list=PLJfu_xpF92pvTfcJAILr5Kg1ptMvHUnft&amp;index=8" rel="noopener noreferrer" target="_blank">Continuity of Shannon's Information Measures </a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=tIt4pJAbCDQ&amp;list=PLJfu_xpF92pvTfcJAILr5Kg1ptMvHUnft&amp;index=13" rel="noopener noreferrer" target="_blank">Some Useful Information Inequalities</a></p><p><a class="tc-tiddlylink-external" href="http://www.tandfonline.com/doi/abs/10.1080/00207166808803030?journalCode=gcom20" rel="noopener noreferrer" target="_blank">Three approaches to the quantitative definition of information</a></p>