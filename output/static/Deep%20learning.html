<p>Deep learning <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Machine%20learning">Machine learning</a> in a modular way using <strong>layers</strong>, like in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Torch%20(Deep%20learning%20framework)">Torch</a>. <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Artificial%20neural%20network">Artificial neural network</a>s, with many layers..</p><p><a class="tc-tiddlylink-external" href="http://www.abigailsee.com/2018/02/21/deep-learning-structure-and-innate-priors.html" rel="noopener noreferrer" target="_blank">http://www.abigailsee.com/2018/02/21/deep-learning-structure-and-innate-priors.html</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=He4t7Zekob0&amp;index=5&amp;list=PLujxSBD-JXglGL3ERdDOhthD3jTlfudC2" rel="noopener noreferrer" target="_blank">Two+ Minute Papers - How Does Deep Learning Work?</a>
– 
<a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=g-dKXOlsf98" rel="noopener noreferrer" target="_blank">The computer that mastered Go</a></p><p><a class="tc-tiddlylink-external" href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/" rel="noopener noreferrer" target="_blank">Oxford course (with video)</a> on lecture 12</p><p><a class="tc-tiddlylink-external" href="https://uk.mathworks.com/discovery/deep-learning.html" rel="noopener noreferrer" target="_blank">matlab</a></p><p>The idea is also that layers are <em>recursive</em>, i.e. layers can be made up of layers.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=x1kf4Zojtb0#t=43m05" rel="noopener noreferrer" target="_blank">future</a></p><p>Concepts as programs; programs as networks</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Probabilistic%20programming">Probabilistic programming</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Program%20induction">Program induction</a></p><p>trainning models based on demonstration</p><p>Multi-agents, and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Communication">Communication</a></p><p>Generating programs is not that different from generating <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Explainable%20artificial%20intelligence">explanations</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Augmented%20RNN">Augmented RNN</a>s</p><p><a class="tc-tiddlylink-external" href="http://www.thespermwhale.com/jaseweston/" rel="noopener noreferrer" target="_blank">http://www.thespermwhale.com/jaseweston/</a></p><p>NIPS2016</p><hr><h2 class="">Deep learning methods</h2><h3 class=""><u>Neural networks for spatially structured data</u></h3><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Convolutional%20neural%20network">Convolutional neural network</a></p><p><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Deep%20multi-scale%20video%20prediction%20beyond%20mean%20square%20error%7Chttp%3A%2F%2Farxiv.org%2Fabs%2F1511.05440">Multi-scale networks</a> and <a class="tc-tiddlylink-external" href="http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf" rel="noopener noreferrer" target="_blank">an application</a>.</p><p><a class="tc-tiddlylink-external" href="http://arxiv.org/abs/1202.2160" rel="noopener noreferrer" target="_blank">Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers</a></p><p><a class="tc-tiddlylink-external" href="http://www.clement.farabet.net/research.html#parsing" rel="noopener noreferrer" target="_blank">http://www.clement.farabet.net/research.html#parsing</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Computer%20vision">Computer vision</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Residual%20neural%20network">Residual neural network</a></p><h3 class=""><u>Neural networks for <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Sequence">sequential</a> data</u></h3><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Recurrent%20neural%20network">Recurrent neural network</a></p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Transfer%20learning">Transfer learning</a></u></h3><p>good for generalizing models, <strong>transfer learning</strong>, <strong>multi-task learning</strong>. Good when don't have much supervision data.</p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Neural%20networks%20with%20memory">Neural networks with memory</a></u></h3><p><strong>Memory</strong> is good for recognizing time sequence data. See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Long%20short-term%20memory">Long short-term memory</a>.</p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Attention%20in%20machine%20learning">Attention in machine learning</a></u></h3><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Integrating%20symbols%20into%20deep%20learning">Integrating symbols into deep learning</a></u></h3><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Neural%20Turing%20machine">Neural Turing machine</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Neural%20programmer-interpreter">Neural programmer-interpreter</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Deep%20symbolic%20reinforcement%20learning">Deep symbolic reinforcement learning</a></li></ul><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Deep%20reinforcement%20learning">Deep reinforcement learning</a></u></h3><p><em>more...</em></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=x1kf4Zojtb0#t=20m" rel="noopener noreferrer" target="_blank">Structured learning</a> – Learning to learn and compositionality with deep recurrent neural networks</p><hr><h3 class="">Some techniques for deep learning</h3><p><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Layers%20for%20deep%20learning">Layers for deep learning</a></u></p><h2 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#New%20advances%20in%20deep%20learning">New advances in deep learning</a></h2><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Dropout">Dropout</a>. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=NUKp0c4xb8w&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;index=9#t=50m20s" rel="noopener noreferrer" target="_blank">usefulness of dropout</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Batch%20normalization">Batch normalization</a></p><p><a class="tc-tiddlylink-external" href="file:///home/guillefix/Dropbox/Oxford/Systems%20Biology%20DPhil/Research/DeFreitas-NIPS2013_5025.pdf" rel="noopener noreferrer" target="_blank">Predicting Parameters in Deep Learning</a> The intuition motivating the techniques in this paper is the well known observation that the first layer features of a neural network trained on natural image patches tend to be globally smooth with local edge features, similar to local Gabor features [6, 13]. I.e. they are seizing the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Simplicity">Simplicity</a> often found in real-world structures. Given this structure, representing the value of each pixel in the feature separately is redundant, since it is highly likely that the value of a pixel will be equal to a weighted average of its neighbours.  </p><p>The core of the technique is based on representing the weight matrix as a low rank product of two smaller matrices. </p><hr><h2 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Deep%20learning%20theory">Deep learning theory</a></h2><h2 class="">Deep learning applications</h2><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Deep%20art">Deep art</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Applications%20of%20AI">Applications of AI</a></p><hr><p><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Hardware%20for%20deep%20learning">Hardware for deep learning</a></u></p><p><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Software%20for%20deep%20learning">Software for deep learning</a></u></p><hr><p><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#People%20in%20deep%20learning">People in deep learning</a></u></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#History%20of%20deep%20learning">History of deep learning</a></p><hr><p><u>Books and reources</u></p><p><a class="tc-tiddlylink-external" href="http://www.sciencedirect.com/science/article/pii/S0893608014002135" rel="noopener noreferrer" target="_blank">Deep learning in neural networks: An overview</a></p><p><a class="tc-tiddlylink-external" href="https://deepmind.com/publications.html" rel="noopener noreferrer" target="_blank">https://deepmind.com/publications.html</a></p><p><a class="tc-tiddlylink-external" href="http://www.deeplearningbook.org/" rel="noopener noreferrer" target="_blank">http://www.deeplearningbook.org/</a></p><p><a class="tc-tiddlylink-external" href="http://carpedm20.github.io/" rel="noopener noreferrer" target="_blank">http://carpedm20.github.io/</a></p><hr><p>Work on giving prior knowledge to deep learning: <a class="tc-tiddlylink-external" href="https://yani.io/annou/thesis_online.pdf" rel="noopener noreferrer" target="_blank">https://yani.io/annou/thesis_online.pdf</a></p>