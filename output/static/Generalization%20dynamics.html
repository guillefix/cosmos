<p>&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.people.fas.harvard.edu/~asaxe/papers/Advani,%20Saxe%20-%202017%20-%20High-dimensional%20dynamics%20of%20generalization%20error%20in%20neural%20networks.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;High-dimensional dynamics of generalization error in neural networks&lt;/a&gt; (see &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Statistical%20mechanics%20of%20neural%20networks&quot;&gt;Statistical mechanics of neural networks&lt;/a&gt; also) Using random matrix theory and exact solutions in deep linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem.  &lt;/p&gt;&lt;p&gt;We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks.&lt;/p&gt;&lt;p&gt;Overtraining is worst at intermediate network sizes. For both linear and nonlinear nets, catastrophic overtraining is a symp- tom of a model whose complexity is exactly matched to the size of the training set, and can be combated either by making the model smaller or larger .&lt;/p&gt;&lt;p&gt;We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Singular%20learning%20theory&quot;&gt;Singular learning theory&lt;/a&gt;, &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Sloppy%20model&quot;&gt;Sloppy model&lt;/a&gt;s. These subspaces, are spaces are the &lt;strong&gt;neutral spaces&lt;/strong&gt; of the parameterâ€“function GP map!&lt;/li&gt;&lt;li&gt;and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;derive a generalization error bound
which incorporates the frozen subspace and conditioning effects and qualitatively matches
the behavior observed in simulation.&lt;/p&gt;&lt;p&gt;the optimal stopping time (for minimizing generalization error) in these
systems growths with the signal to noise ratio (SNR), and we provide arguments that this
growth is approximately logarithmic.  When the quality of the data is higher (high SNR),
we expect the algorithm to weigh the data more heavily and thus run gradient descent for
longer. &lt;/p&gt;&lt;p&gt;&amp;quot;while a large
deep network can indeed fit random labels, gradient-trained DNNs initialized with small-
norm weights learn simpler functions first and hence generalize well if there is a consistent
rule to be learned.&amp;quot;&lt;/p&gt;&lt;p&gt;Remarkably,  even  without  early  stopping,  generalization  can  still  be  better  in  larger
networks.  To understand this, we derive an alternative bound on the Rademacher complex-
ity of a two layer non-linear neural network with fixed first layer weights that incorporates
the dynamics of gradient descent.  We show that complexity is limited because of a frozen
subspace in which no learning occurs, and overtraining is prevented by a larger gap in the
eigenspectrum of the data in the hidden layer in overcomplete models &lt;/p&gt;</p>