<p>&lt;p&gt;ReLUs don't really suffer from the vanishing gradient problem (at least not in its standard form), as their gradient is either 0 or 1. See discussion here: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://stats.stackexchange.com/questions/176794/how-does-rectilinear-activation-function-solve-the-vanishing-gradient-problem-in&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://stats.stackexchange.com/questions/176794/how-does-rectilinear-activation-function-solve-the-vanishing-gradient-problem-in&lt;/a&gt;&lt;/p&gt;</p>