<h3 class=""><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1709.01953.pdf" rel="noopener noreferrer" target="_blank">Implicit regularization in deep learning</a></h3><p>Regularization caused by optimization algorithm.. &quot;we investigate the transformations under which the function computed by a network remains the same and therefore argue for complexity measures and optimization algorithms that have similar invariances. We find complexity measures that have similar invariances to neural networks and optimization algorithms that implicitly regularize them&quot;. <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Path-norm">Path-norm</a> a metric in parameter space, that is closer to model distance? <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Sloppy%20systems">Sloppy systems</a>?</p><ul><li>We prove generalization bounds for the class of fully connected feedforward networks with the bounded norm. We further show that for some norms, this bound is independent of the number of hidden units.</li><li>We show how PAC-Bayes framework can be employed to obtain generalization bounds for neural networks by making a connection between sharpness and PAC-Bayes theory.</li><li>Implicit Regularization by SGD (Chapter 6): We show that networks learned by SGD satisfy several conditions that lead to flat minima. (Hmm, doesn't seem to be in chapter 6)</li></ul><p>[28] proved that the Rademacher complexity of fully connected feedforward networks on set S can be bounded based on the 1 norm of the weights of hidden units in each layer  In <a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1709.01953.pdf#page=31" rel="noopener noreferrer" target="_blank">Chapter 5</a> we show
how the capacity can be controlled for a large family of norms. See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Learning%20real-valued%20functions">Learning real-valued functions</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Algorithmic%20robustness">Algorithmic robustness</a>, similar to <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Algorithmic%20stability">Algorithmic stability</a></p><p><small><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1709.01953.pdf#page=22" rel="noopener noreferrer" target="_blank">Sharpness</a>. we advocate viewing a related notion of expected sharpness in the context of the PAC-Bayesian framework. Viewed this way, it becomes clear that sharpness controls only one of two relevant terms, and must be balanced with some other measure such as norm. Together, sharpness and norm do provide capacity control and can explain many of the observed phenomena. This connection between sharpness and the PAC-Bayes framework was also recently noted by Dziugaite and Roy [32].</small></p><p><small><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1709.01953.pdf#page=25" rel="noopener noreferrer" target="_blank">On the Role of Implicit Regularization in Generalization</a> we draw an analogy to matrix
factorization and understand dimensionality versus norm control there. Based on this analogy we suggest that implicit norm regularization might be central also for deep learning, and also there we should think of bounded-norm models with capacity independent of number of hidden units. See also <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Deep%20learning%20theory">Deep learning theory</a></small></p><p>Focusing on networks with RELU activations in this section, we observe that scaling down the incoming
edges to a hidden unit and scaling up the outgoing edges by the same factor yields an equivalent network
computing the same function. Since predictions are invariant to such rescalings, it is natural to seek a
geometry, and corresponding optimization method, that is similarly invariant. In this chapter, we study
invariances in feedforward networks with shared weights. — <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Sloppy%20systems">Sloppy systems</a>!</p><p><small>Revisiting the choice of gradient descent, we recall that optimization is also inherently tied to a choice
of geometry or measure of distance, norm or divergence. Gradient descent for example is tied to the l2
norm as it is the steepest descent with respect to l2 norm in the parameter space, while coordinate descent
corresponds to steepest descent with respect to the l1 norm and exp-gradient (<strong>multiplicative weight</strong>)
updates is tied to an entropic divergence.</small></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Sensitivity">Sensitivity</a>-based bounds</p><hr><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1711.04623" rel="noopener noreferrer" target="_blank">Three Factors Influencing Minima in SGD</a> – Characterizing the relation between learning rate, batch size and the properties of the final minima, such as width or generalization</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1810.10702" rel="noopener noreferrer" target="_blank">Subgradient Descent Learns Orthogonal Dictionaries</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Stochastic%20gradient%20descent">Stochastic gradient descent</a></p><p><a class="tc-tiddlylink-external" href="https://openreview.net/forum?id=BJij4yg0Z" rel="noopener noreferrer" target="_blank">A Bayesian Perspective on Generalization and Stochastic Gradient Descent</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1710.10174" rel="noopener noreferrer" target="_blank">SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1808.01204" rel="noopener noreferrer" target="_blank">Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data</a></p><p><a class="tc-tiddlylink-external" href="http://proceedings.mlr.press/v80/bartlett18a.html" rel="noopener noreferrer" target="_blank">Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks</a> – <a class="tc-tiddlylink-external" href="http://proceedings.mlr.press/v75/li18a.html" rel="noopener noreferrer" target="_blank">Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations</a> – <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1808.04685" rel="noopener noreferrer" target="_blank">Learning ReLU Networks on Linearly Separable Data: Algorithm, Optimality, and Generalization</a> – <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1803.00195" rel="noopener noreferrer" target="_blank">The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Minima and Regularization Effects</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1802.04420" rel="noopener noreferrer" target="_blank">Towards Understanding the Generalization Bias of Two Layer Convolutional Linear Classifiers with Gradient Descent</a></p>