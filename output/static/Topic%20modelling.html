<p>&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Latent%20Dirichlet%20allocation&quot;&gt;Latent Dirichlet allocation&lt;/a&gt;&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;given number of topics&lt;/p&gt;&lt;ul&gt;&lt;li&gt;documents have multiple topics&lt;/li&gt;&lt;li&gt;uses &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Generative%20model&quot;&gt;Generative model&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Topics: distribution of terms over a fixed vocabulary&lt;/p&gt;&lt;p&gt;tm,  for preprocessing data. reduce stock words (commonly occuring, not useful). snowballC. stemming software.&lt;/p&gt;&lt;p&gt;Algotihm: &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Clustering&quot;&gt;Clustering&lt;/a&gt; algo.&lt;/p&gt;&lt;p&gt;Self-consistency. each word assigned topics based on other words which are assigned topics.&lt;/p&gt;&lt;p&gt;&lt;u&gt;Generative model&lt;/u&gt;: Each document has a probability over topics (prior of parameter is Dirichlet). Then each word is drawn from the probability distribution represented by the model, each independently.&lt;/p&gt;</p>