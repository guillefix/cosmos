<p>I had this idea! Well, I think there's still more work to do on artificial parietal cortices: neural networks for somato-sensory integration, i.e. combining the info from many different senses. I myself think that our semantic latent space in our minds is somewhere in the parietal lobe..</p><p>An architecture based on these ideas is what I hope could make a NN that can begin to generate realistic stories, which I think is a crucial part for AI. Things to include: has to be recurrent, multi-sensory integration into and from semantic latent space, attention. The training may be adversarial, though I think it may need to have some more advanced type of multi-modal context-dependent training (attention in training?). Also, metalearning/introspection (which I think in the brain resides in the frontal lobe) will be crucial. Finally, more advanced memory, like DNCs, will be necessary at some point (in the brain, hippocampus), but I don't yet know well how to do that.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Q6nYgeH8Krs" rel="noopener noreferrer" target="_blank">Unsupervised Learning of Spoken Language with Visual Context</a></p><p><a class="tc-tiddlylink-external" href="http://groups.csail.mit.edu/sls/publications/2016/FelixSun_SLT_2016.pdf" rel="noopener noreferrer" target="_blank">http://groups.csail.mit.edu/sls/publications/2016/FelixSun_SLT_2016.pdf</a></p><p>See facebook posts. CycleGAN.</p><p>multimodal learning, days fusion.</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1704.03152" rel="noopener noreferrer" target="_blank">Deep Multimodal Representation Learning from Temporal Data</a></p><p><a class="tc-tiddlylink-external" href="http://people.csail.mit.edu/yusuf/see-hear-read//" rel="noopener noreferrer" target="_blank">See, Hear, and Read: Deep Aligned Representations</a></p>