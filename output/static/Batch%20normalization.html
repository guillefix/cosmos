<p>&lt;p&gt;A method to increase learning rate for &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Deep%20learning&quot;&gt;deep&lt;/a&gt; &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Artificial%20neural%20network&quot;&gt;neural networks&lt;/a&gt;, as well as &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Regularization&quot;&gt;regularize&lt;/a&gt; them.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://arxiv.org/abs/1502.03167&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://standardfrancis.wordpress.com/2015/04/16/batch-normalization/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;an explanation&lt;/a&gt;&lt;/p&gt;&lt;p&gt;They suggest that a change in the distribution of activations because of parameter updates slows learning.  They call this change the internal covariate shift. They propose to normalize the activations (to have zero mean and unit variance) to avoid this.&lt;/p&gt;&lt;p&gt;These have been found to work better for &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Convolutional%20neural%20network&quot;&gt;Convolutional neural network&lt;/a&gt;s for &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Image%20classification&quot;&gt;Image classification&lt;/a&gt;. I think this may be because for images, you care more about relative differences between pixel values, and so normalizing helps remove unimportant information.&lt;/p&gt;</p>