<p>&lt;p&gt;Overfitting and underfitting refer to ways of misstraining a model, i.e., making it have poor generalization error, compared to the optimal model.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=tojaGtMPo5U&amp;amp;list=PLA89DCFA6ADACE599&amp;amp;index=9#t=4m1.5s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Bias-variance tradeoff&lt;/a&gt;, see also &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=tojaGtMPo5U&amp;amp;list=PLA89DCFA6ADACE599&amp;amp;index=9#t=1h03m40s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Relation to bias/variance tradeoff&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=tojaGtMPo5U&amp;amp;list=PLA89DCFA6ADACE599&amp;amp;index=9#t=1h08m20s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;training error/generalization error&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Underfitting&quot;&gt;Underfitting&lt;/a&gt;. A learning algorithm with a lot of &lt;strong&gt;bias&lt;/strong&gt;, meaning that they impose a lot of a priori structure/assumptions to the fitted functions. These, however, tend to have low &lt;strong&gt;variance&lt;/strong&gt;, meaning that the fitted function doesn't tend to vary much when different training data sampled from the same process are used, they are &lt;em&gt;stable&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Overfitting&quot;&gt;Overfitting&lt;/a&gt;. A learning algorithm with low bias (it is more &lt;em&gt;flexible&lt;/em&gt;), which however has a lot of variance, as it fits the idiosyncrasies of the data; it fits the noise.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=JfkbyODyujw&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;amp;index=14#t=6m45s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;See explanation here&lt;/a&gt; and &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=Fs-raHUnF2M&amp;amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;amp;index=16#t=9m05s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/1vNipHvUofw7v2XurbRcoKWdrha4XwW2Wg6iv_CjnPJ7yaJeLuOGPHEXP5r0bHiXDa5jXmi3gXWzRs-rnOmoWT2qdrpDlhqoPaINOW1e8wCnkcMmsfjL5I7MAnuysZNkA0ZS-AduSU6My_vj8QjrLwgU7PtqeOxEmOHYOzJMm1COtI55peywxXwYc4Ot0XMg3WSk4ctE620Fg-kQuA8Zw86ejVU0wPx4C6f-yYJYDol4KmH_zV43EJREoK0ZJaU0v4j54Luq0_enrS9FA4oPcWX5v4h6hTCXJq3aubFRI-HBAP0Az3Js3cA9ZxPQV0U-1MZBCEdfI-0b87bEVSEAvZ7vsWTfyadsG43bfwc8ZGr4XRhXWYVlGj48WxrpQyTPFhPQMXNoiRURzx5bm4ZHukhomdEE98JJ4c5XqhybUHdIk6qJbUS7BXjcYaBlm3z8bGiBlPtDSdt61a59mbotPi7DS3N-LdHrHUd3PXtG59t_5fHfKi3WpqNS_dJOefgRukPJ0OAK4fE579XHNw_8l0Fi2mAqsP7Y8WNm1lg8yXQI2c6hrlGzWt2jO_4it_Zef_2r=w1269-h675-no&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=qz9bKfOqd0Y&amp;amp;index=5&amp;amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=25m43s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Deep Learning Lecture 5: Regularization, model complexity and data complexity (part 2)&lt;/a&gt;&lt;/p&gt;&lt;p&gt;So the simplest model that works seems to work best most of the time. Seems like an example of Occam's razor, and thus related to Solomonoff's ideas on inference (see &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Algorithmic%20information%20theory&quot;&gt;Algorithmic information theory&lt;/a&gt;). Epicurus principle also related to Bayesian inference, because we give a distribution over models, but we keep all of them.&lt;/p&gt;&lt;p&gt;Hmm, also your error can't be smaller than the fundamental noise in the data. Well it can, but your model will at best be wasteful then.&lt;/p&gt;</p>