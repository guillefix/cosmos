<p>Overfitting and underfitting refer to ways of misstraining a model, i.e., making it have poor generalization error, compared to the optimal model.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=tojaGtMPo5U&amp;list=PLA89DCFA6ADACE599&amp;index=9#t=4m1.5s" rel="noopener noreferrer" target="_blank">Bias-variance tradeoff</a>, see also <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=tojaGtMPo5U&amp;list=PLA89DCFA6ADACE599&amp;index=9#t=1h03m40s" rel="noopener noreferrer" target="_blank">Relation to bias/variance tradeoff</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=tojaGtMPo5U&amp;list=PLA89DCFA6ADACE599&amp;index=9#t=1h08m20s" rel="noopener noreferrer" target="_blank">training error/generalization error</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Underfitting">Underfitting</a>. A learning algorithm with a lot of <strong>bias</strong>, meaning that they impose a lot of a priori structure/assumptions to the fitted functions. These, however, tend to have low <strong>variance</strong>, meaning that the fitted function doesn't tend to vary much when different training data sampled from the same process are used, they are <em>stable</em></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Overfitting">Overfitting</a>. A learning algorithm with low bias (it is more <em>flexible</em>), which however has a lot of variance, as it fits the idiosyncrasies of the data; it fits the noise.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=JfkbyODyujw&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=14#t=6m45s" rel="noopener noreferrer" target="_blank">See explanation here</a> and <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Fs-raHUnF2M&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=16#t=9m05s" rel="noopener noreferrer" target="_blank">here</a></p><p><img src="https://lh3.googleusercontent.com/1vNipHvUofw7v2XurbRcoKWdrha4XwW2Wg6iv_CjnPJ7yaJeLuOGPHEXP5r0bHiXDa5jXmi3gXWzRs-rnOmoWT2qdrpDlhqoPaINOW1e8wCnkcMmsfjL5I7MAnuysZNkA0ZS-AduSU6My_vj8QjrLwgU7PtqeOxEmOHYOzJMm1COtI55peywxXwYc4Ot0XMg3WSk4ctE620Fg-kQuA8Zw86ejVU0wPx4C6f-yYJYDol4KmH_zV43EJREoK0ZJaU0v4j54Luq0_enrS9FA4oPcWX5v4h6hTCXJq3aubFRI-HBAP0Az3Js3cA9ZxPQV0U-1MZBCEdfI-0b87bEVSEAvZ7vsWTfyadsG43bfwc8ZGr4XRhXWYVlGj48WxrpQyTPFhPQMXNoiRURzx5bm4ZHukhomdEE98JJ4c5XqhybUHdIk6qJbUS7BXjcYaBlm3z8bGiBlPtDSdt61a59mbotPi7DS3N-LdHrHUd3PXtG59t_5fHfKi3WpqNS_dJOefgRukPJ0OAK4fE579XHNw_8l0Fi2mAqsP7Y8WNm1lg8yXQI2c6hrlGzWt2jO_4it_Zef_2r=w1269-h675-no"></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=qz9bKfOqd0Y&amp;index=5&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=25m43s" rel="noopener noreferrer" target="_blank">Deep Learning Lecture 5: Regularization, model complexity and data complexity (part 2)</a></p><p>So the simplest model that works seems to work best most of the time. Seems like an example of Occam's razor, and thus related to Solomonoff's ideas on inference (see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Algorithmic%20information%20theory">Algorithmic information theory</a>). Epicurus principle also related to Bayesian inference, because we give a distribution over models, but we keep all of them.</p><p>Hmm, also your error can't be smaller than the fundamental noise in the data. Well it can, but your model will at best be wasteful then.</p>