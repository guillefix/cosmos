<h2 class=""><u>Basic RNNs</u></h2><p><strong><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=56TYLaQN4N8&amp;index=12&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=7m48s" rel="noopener noreferrer" target="_blank">Recurrent neural nets</a></strong>. Vanishing gradient problem, naively, RNNs don't give you long term memory.. so you have <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Long%20short-term%20memory">Long short-term memory</a> networks</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=mF5-tr7qAF4#t=21m25s" rel="noopener noreferrer" target="_blank">Recurrent neural networks -- Schmidhuber</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Jkkjy7dVdaY" rel="noopener noreferrer" target="_blank">Recurrent Neural Network Writes Music and Shakespeare Novels - Two Minute Papers</a></p><h3 class=""><u>The vanishing gradients problem</u></h3><p><a class="tc-tiddlylink-external" href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" rel="noopener noreferrer" target="_blank">Hochreiter1991</a> â€“ <a class="tc-tiddlylink-external" href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" rel="noopener noreferrer" target="_blank">Bengio1994</a></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Long%20short-term%20memory">Long short-term memory</a></u></h2><p>Proposed to solve the vanishing gradients problem</p><p><a class="tc-tiddlylink-external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener noreferrer" target="_blank">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><p>See also <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Neural%20networks%20with%20memory">Neural networks with memory</a>,  <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Deep%20learning">Deep learning</a></p><h2 class=""><u>Gated recurrent unit</u></h2><p>A variation of the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Long%20short-term%20memory">Long short-term memory</a> network</p><h2 class=""><u>More <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Augmented%20RNN">Augmented RNN</a>s</u></h2><hr><p>Nice curated list of RNNs: <a class="tc-tiddlylink-external" href="https://github.com/kjw0612/awesome-rnn" rel="noopener noreferrer" target="_blank">https://github.com/kjw0612/awesome-rnn</a></p><hr><p><u><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1605.00064.pdf" rel="noopener noreferrer" target="_blank">Higher Order Recurrent Neural Networks</a></u> . We propose to use more memory
units to keep track of more preceding states
in recurrent neural networks (RNNs), which are
all recurrently fed to the hidden layers as feedback
through different weighted path</p>