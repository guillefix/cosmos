<p>&lt;p&gt;See sec 8.7 of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Sutton-Barto&quot;&gt;Sutton-Barto&lt;/a&gt;&lt;/p&gt;&lt;p&gt;A minimization problem which can be formulated as a &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Reinforcement%20learning&quot;&gt;Reinforcement learning&lt;/a&gt; problem which satisfies these conditions: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;The inital value of every goal state is zero&lt;/li&gt;&lt;li&gt;there exists at least one policy that guarantees that a goal state will be reached with probability one from any start state&lt;/li&gt;&lt;li&gt;all rewards for transitions from non-goal states are strictly negative &lt;/li&gt;&lt;li&gt;all the initial values are equal to, or greater than, their optimal values (which can be satisfied by simply setting the initial values to zero)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Real-time%20dynamic%20programming&quot;&gt;Real-time dynamic programming&lt;/a&gt; converges for these problems&lt;/p&gt;</p>