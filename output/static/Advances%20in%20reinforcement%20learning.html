<p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Deep%20reinforcement%20learning">Deep reinforcement learning</a></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Curiosity-driven%20RL">Curiosity-driven RL</a></u></h2><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Model-based%20reinforcement%20learning">Model-based reinforcement learning</a></u></h2><p><a class="tc-tiddlylink-external" href="https://buff.ly/2KOUbFO" rel="noopener noreferrer" target="_blank">https://buff.ly/2KOUbFO</a> – MCTS approaches really seem like the best we have for AI problems beyond perception.</p><p><a class="tc-tiddlylink-external" href="https://yobibyte.github.io/starcraft-research.html#Stabilising-Experience-Replay-for-Deep-Multi-Agent-Reinforcement-Learning,-2017-[21" rel="noopener noreferrer" target="_blank">Adances in StarCraft RL</a></p><h2 class=""><u>Theory of RL</u></h2><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Ol0-c9OE3VQ" rel="noopener noreferrer" target="_blank">Comparing humans with the best Reinforcement Learning algorithms</a> – this demonstrates that our RL algorithms don't have the same priors that we humans use for learning</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1807.09647" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1807.09647</a></p><hr><p>AlphaZero</p><p><a class="tc-tiddlylink-external" href="https://machinethoughts.wordpress.com/2018/04/14/rl-and-the-game-of-mathematics/" rel="noopener noreferrer" target="_blank">https://machinethoughts.wordpress.com/2018/04/14/rl-and-the-game-of-mathematics/</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1806.02426" rel="noopener noreferrer" target="_blank">https://arxiv.org/abs/1806.02426</a></p><p><a class="tc-tiddlylink-external" href="https://medium.com/@awjuliani/on-solving-montezumas-revenge-2146d83f0bc3" rel="noopener noreferrer" target="_blank">&quot;Solving&quot; Montezuma's revenge</a> – They make the RL problem easier by mapping it to a supervised learning problem. However, that way they allow the algorithm to just solve by memorization, rather than having it generalize because of better priors. Learning from demonstrations is interesting though because it could allow the algorithms, for instance, to learn the priors from humans which already have them! However, the algorithms presented in the research discussed in this paper don't seem to do this, and just learn the particular trajectory/policy for solving this game..</p><hr><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Ol0-c9OE3VQ" rel="noopener noreferrer" target="_blank">https://www.youtube.com/watch?v=Ol0-c9OE3VQ</a> Interesting. This research quite undeniably demonstrates that our RL algorithms lack the priors that we use for learning. And that is bad when learning human-related tasks, but can be good in other cases (No Free Lunch!!). Most future AGIs will have rather different priors than us, and will specialize at things like quantum mechanics, relativity, algebraic geometry. They will dream in those things, and they will find our world very unintuitive :) Perhaps there will also be superintelligences which while being slightly worse at some things than us, will be much better at most relevant things than us.
Also, nice channel</p><hr><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1807.03392" rel="noopener noreferrer" target="_blank">Evolving Multimodal Robot Behavior via Many Stepping Stones with the Combinatorial Multi-Objective Evolutionary Algorithm</a></p><p><a class="tc-tiddlylink-external" href="https://blog.openai.com/openai-five/" rel="noopener noreferrer" target="_blank">https://blog.openai.com/openai-five/</a></p><p>This is quite impressive. Could they beat the top pro players by this August?</p><p>&quot;OpenAI Five plays 180 years worth of games against itself every day, learning via self-play. It trains using a scaled-up version of Proximal Policy Optimization running on 256 GPUs and 128,000 CPU cores&quot; 
２５６ ＧＰＵｓ ａｎｄ １２８，０００ ＣＰＵ ｃｏｒｅｓ</p><p>This indicates that reinforcement learning can yield long-term planning with large but achievable scale — without fundamental advances</p><p>Each of OpenAI Five’s networks contain a single-layer, 1024-unit LSTM that sees the current game state (extracted from Valve’s Bot API) and emits actions through several possible action heads. Each head has semantic meaning, for example, the number of ticks to delay this action, which action to select, the X or Y coordinate of this action in a grid around the unit, etc. Action heads are computed independently.</p><p>&quot;To avoid “strategy collapse”, the agent trains 80% of its games against itself and the other 20% against its past selves.&quot; This sounds similar to the &quot;double learning&quot; used in DQN, which enhances exploration.</p><p>To force exploration in strategy space, during training (and only during training) we randomized the properties (health, speed, start level, etc.) of the units, and it began beating humans/</p><p>We postprocess each agent’s reward by subtracting the other team’s average reward to prevent the agents from finding positive-sum situations.</p><p>Teamwork is controlled by a hyperparameter we dubbed “team spirit”. Team spirit ranges from 0 to 1, putting a weight on how much each of OpenAI Five’s heroes should care about its individual reward function versus the average of the team’s reward functions. We anneal its value from 0 to 1 over training.</p><p>Our system is implemented as a general-purpose RL training system called Rapid</p>