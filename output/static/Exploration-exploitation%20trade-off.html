<p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h20m40" rel="noopener noreferrer" target="_blank">Exploration vs exploitationn</a>, in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Model-free%20reinforcement%20learning">Model-free reinforcement learning</a></p><hr><p><u>Methods to ensure exploration</u></p><ul><li><span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">ϵ</span></span></span></span></span>-greediness</li><li></li><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Exploration%20bonus">Exploration bonus</a> (increasing the reward of transitions which are not commonly visited). Like UCB</li><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Optimistic%20initialization">Optimistic initialization</a>: initalize the expected <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Value%20function">value</a> of states to be considerably higher than what one expects, so that the greedy policy tries to visit them, before learning the true values.</li></ul>