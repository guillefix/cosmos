<p>&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h20m40&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Exploration vs exploitationn&lt;/a&gt;, in &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Model-free%20reinforcement%20learning&quot;&gt;Model-free reinforcement learning&lt;/a&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;u&gt;Methods to ensure exploration&lt;/u&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;span&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;strut bottom&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base textstyle uncramped&quot;&gt;&lt;span class=&quot;mord mathit&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;-greediness&lt;/li&gt;&lt;li&gt;&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Exploration%20bonus&quot;&gt;Exploration bonus&lt;/a&gt; (increasing the reward of transitions which are not commonly visited). Like UCB&lt;/li&gt;&lt;li&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-missing&quot; href=&quot;#Optimistic%20initialization&quot;&gt;Optimistic initialization&lt;/a&gt;: initalize the expected &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Value%20function&quot;&gt;value&lt;/a&gt; of states to be considerably higher than what one expects, so that the greedy policy tries to visit them, before learning the true values.&lt;/li&gt;&lt;/ul&gt;</p>