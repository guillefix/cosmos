<p>&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=LKdFTsM3hl4&amp;amp;list=PLA89DCFA6ADACE599&amp;amp;index=17#t=27m06s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;video&lt;/a&gt; â€“ discretization scales poorly to high dimensional state spaces.&lt;/p&gt;&lt;p&gt;See Elements of Statistical learning&lt;/p&gt;&lt;p&gt;As can be seen by applying &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Nearest-neighbour%20classification&quot;&gt;Nearest-neighbour classification&lt;/a&gt;, the size of the neighbourhood to consider a certain fraction of the total populations for the choice, grows with dimension (linear size needs to be larger to have same fraction of total volume). The problem then is that we are making the choice based on points which are quite far in terms of linear distance, and thus need not be good predictors any more. This is the &lt;strong&gt;curse of dimensionality&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;See page 18 in Murphy's book&lt;/p&gt;&lt;p&gt;The main way to combat the curse of dimensionality is to make some assumptions about
the nature of the data distribution (either p(y|x) for a supervised problem or p(x) for an
unsupervised problem). These assumptions, known as inductive bias, are often embodied in
the form of a parametric model, which is a statistical model with a fixed number of parameters.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.dbs.ifi.lmu.de/~zimek/publications/SSDBM2010/SNN-SSDBM2010-preprint.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;http://www.dbs.ifi.lmu.de/~zimek/publications/SSDBM2010/SNN-SSDBM2010-preprint.pdf&lt;/a&gt;. See page 5 for some interesting comments (c.f. &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Sloppy%20systems&quot;&gt;Sloppy systems&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;In many cases (for many distance measures) distances between pairs of points (distributed in some way) tend to their mean (i.e. their variance decreases) as we increase dimension. These well-known studies generally
assumed that the full data set followed a single data distribution, subject to
certain restrictions. In fact, when the data follows a mixture of distributions,
the concentration effect is not always observed; in such cases, distances between
members of different distributions may not necessarily tend to the global mean
as the dimensionality increases.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://youtu.be/i2bt4vt908g?t=25m43s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Curse of dimensionality in k-NN&lt;/a&gt;&lt;/p&gt;</p>