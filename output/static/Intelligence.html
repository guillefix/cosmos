<p><a name="Intelligence">
<div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists   tc-tagged-Cognitive%20science " data-tags="[[Cognitive science]]" data-tiddler-title="Intelligence"><div class="tc-tiddler-title">
<div class="tc-titlebar">
<span class="tc-tiddler-controls">
<span class=" tc-reveal"><button aria-label="more" class=" tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="edit" class=" tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="close" class=" tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="tiddlymap" class=" tc-btn-%24%3A%2Fplugins%2Ffelixhayashi%2Ftiddlymap%2Fmisc%2FquickConnectButton " title="Toggle TiddlyMap actions">


</button></span>
</span>

<span>

<span class="tc-tiddler-title-icon" style="fill:;">

</span>



<h2 class="tc-title">
Intelligence
</h2>

</span>

</div>

<div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div>
</div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal">
<div class="tc-subtitle">
<a class="tc-tiddlylink tc-tiddlylink-missing" href="#cosmos">
cosmos
</a> 12th August 2017 at 12:47pm
</div>
</div>
<div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">


<span class="tc-drop-down tc-reveal" hidden="true"></span>

</span>
</div>
</div>
<div class="tc-tiddler-body tc-reveal"><p><strong>Intelligence</strong> may be defined as the ability of an agent to achieve goals in many different environments.</p><h2 class=""><u>Features of intelligence</u></h2><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Learning">Learning</a><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#One-shot%20learning">One-shot learning</a></li><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Learning%20to%20learn">Learning to learn</a></li></ul></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Inference">Inference</a>. <sub><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Active%20inference">Active inference</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Hierarchical%20inference">Hierarchical inference</a></sub><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Induction">Induction</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Deduction">Deduction</a> – <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Logic">Logic</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Relational%20reasoning">Relational reasoning</a></li></ul></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Generalization">Generalization</a> – <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Abstraction">Abstraction</a> – <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Concept">Concept</a> – <ul><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Compositionality">Compositionality</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Causality">Causality</a></li></ul></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Memory">Memory</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Attention">Attention</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Perception">Perception</a> – <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Emotion">Emotion</a> – <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Multisensory%20integration">Multisensory integration</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Imagination">Imagination</a> – <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Creativity">Creativity</a> – <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Fun">Fun</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Play">Play</a> – <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Exploration">Exploration</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Communication">Communication</a></li></ul><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Dual%20process%20theory">Dual process theory</a> – <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Understanding">Understanding</a> –&gt; <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Reasoning">Reasoning</a></p><h2 class=""><u>Types of intelligence</u></h2><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Artificial%20intelligence">Artificial intelligence</a> (aka machine intelligence..)<ul><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="#General%20artificial%20intelligence">General artificial intelligence</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Machine%20learning">Machine learning</a>, <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Deep%20learning">Deep learning</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Logic">Logic</a></li></ul></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Natural%20intelligence">Natural intelligence</a><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Human%20intelligence">Human intelligence</a></li></ul></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Intelligence%20augmentation">Intelligence augmentation</a></li><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Hybrid%20intelligence">Hybrid intelligence</a></li></ul><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Neuroscience">Neuroscience</a> and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Artificial%20intelligence">Artificial intelligence</a></u></h2><p><a class="tc-tiddlylink-external" href="https://www.technologyreview.com/s/602627/searching-rat-brains-for-clues-on-how-to-make-smarter-machines/" rel="noopener noreferrer" target="_blank">Searching Rat Brains for Clues on How to Make Smarter Machines</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Toward%20an%20Integration%20of%20Deep%20Learning%20and%20Neuroscience">Toward an Integration of Deep Learning and Neuroscience</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Neuromorphic%20computing">Neuromorphic computing</a></p><p><a class="tc-tiddlylink-external" href="https://www.quora.com/What-are-the-advantages-and-disadvantages-I-should-have-to-be-aware-of-if-I-would-like-to-to-use-Spiking-Neural-Networks-SNN-as-machine-learning-tools" rel="noopener noreferrer" target="_blank">What are the advantages and disadvantages I should have to be aware of if I would like to to use Spiking Neural Networks (SNN) as machine learning tools?</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Spiking%20neural%20network">Spiking neural network</a></p><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Binding%20problem">Binding problem</a></h3><p><a class="tc-tiddlylink-external" href="http://slides.com/guillermovalle/deck-3/fullscreen#/" rel="noopener noreferrer" target="_blank">AI for science</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/channel/UCGoxKRfTs0jQP52cfHCyyRQ" rel="noopener noreferrer" target="_blank">Center for Brains, Minds and Machines (CBMM)</a></p><hr><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Cognitive%20science">Cognitive science</a></h3><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Free%20energy%20principle">Free energy principle</a></h3><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Philosophy%20of%20mind">Philosophy of mind</a></h3><hr><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Godel%2C%20Escher%2C%20Bach">Godel, Escher, Bach</a></p><hr><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Relational%20reasoning">Relational reasoning</a></p><p>See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Integrating%20symbols%20into%20deep%20learning">Integrating symbols into deep learning</a></p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1706.06383.pdf" rel="noopener noreferrer" target="_blank">Programmable agents</a> – prior is on the fact that objects are described by a set of properties that are then acted with a logical language (which encodes the task that the agent has to perform) upon.</p><h3 class=""><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/1604.00289.pdf" rel="noopener noreferrer" target="_blank">Building Machines That Learn and Think Like People</a></h3><p>recursive neural nets!</p><p>This richness and flexibility suggests that learning as model building is a better metaphor than
learning as pattern recognition. </p><p>Regarding Bayesian Program Learning, Structure
sharing across concepts is accomplished by the compositional reuse of stochastic primitives that
can combine in new ways to create new concepts</p><p>When comparing people and the current best algorithms in AI and machine learning, people learn
from less data and generalize in richer and more flexible ways.</p><hr><p>Even with just a few examples, people can learn remarkably rich conceptual models. One indicator
of richness is the variety of functions that these models support (A. B. Markman &amp; Ross, 2003;
Solomon, Medin, &amp; Lynch, 1999). Beyond classification, concepts support prediction (Murphy &amp;
Ross, 1994; Rips, 1975), action (Barsalou, 1983), communication (A. B. Markman &amp; Makin,
1998), imagination (Jern &amp; Kemp, 2013; Ward, 1994), explanation (Lombrozo, 2009; Williams &amp; Lombrozo, 2010), and composition (Murphy, 1988; Osherson &amp; Smith, 1981)
--------—</p><p>People can learn to recognize a new
handwritten character from a single example (Figure 1A-i), allowing them to discriminate between
novel instances drawn by other people and similar looking non-instances (Lake, Salakhutdinov, &amp;
Tenenbaum, 2015; E. G. Miller, Matsakis, &amp; Viola, 2000). Moreover, people learn more than
how to do pattern recognition: they learn a concept – that is, a model of the class that allows their
acquired knowledge to be flexibly applied in new ways. In addition to recognizing new examples,
people can also generate new examples (Figure 1A-ii), parse a character into its most important
parts and relations (Figure 1A-iii; Lake, Salakhutdinov, and Tenenbaum (2012)), and generate new
characters given a small set of related characters (Figure 1A-iv). These additional abilities come
for free along with the acquisition of the underlying concept.</p><p>Characters Challenge. Frostbite challenge</p><hr><p>They may be better seen as solving different tasks. Human learners – unlike DQN and
many other deep learning systems – approach new problems armed with extensive prior experience.
The human is encountering one in a years-long string of problems, with rich overlapping structure.
Humans as a result often have important domain-specific knowledge for these tasks, even before
they ‘begin.’ The DQN is starting completely from scratch.” We agree, and indeed this is another
way of putting our point here. Human learners fundamentally take on different learning tasks than
today’s neural networks, and if we want to build machines that learn and think like people, our
machines need to confront the kinds of tasks that human learners do, not shy away from them.
People never start completely from scratch, or even close to “from scratch,” and that is the secret
to their success. The challenge of building models of human learning and thinking then becomes:
How do we bring to bear rich prior knowledge to learn new tasks and solve new problems so quickly?
What form does that prior knowledge take, and how is it constructed, from some combination of
inbuilt capacities and previous experience? The core ingredients we propose in the next section
offer one route to meeting this challenge.</p><p><u>Developmental start-up software</u></p><p>“child as scientist”</p><p><u> Intuitive physics</u></p><p>A promising recent approach sees intuitive physical reasoning
as similar to inference over a physics software engine, the kind of simulators that power
modern-day animations and games (Bates, Yildirim, Tenenbaum, &amp; Battaglia, 2015; Battaglia,
Hamrick, &amp; Tenenbaum, 2013; Gerstenberg, Goodman, Lagnado, &amp; Tenenbaum, 2015; Sanborn,
Mansinghka, &amp; Griffiths, 2013).</p><p>Could deep learning systems such as PhysNet capture
this flexibility, without explicitly simulating the causal interactions between objects in three
dimensions? We are not sure, but we hope this is a challenge they will take on.</p><p><u>intuitive psychology</u></p><p>However, it seems to us that any full formal account of intuitive psychological reasoning needs to
include representations of agency, goals, efficiency, and reciprocal relations.</p><p>utility calculus, MDPs etc</p><p><u>Model building</u></p><p>Compositionality</p><p>Causality – “Analysis-by-synthesis”</p><p>Learning-to-learn. related to “transfer learning”</p><p><u>Thinking fast</u></p><p>This section discusses possible paths towards resolving the conflict between fast inference
and structured representations, including Helmholtz-machine-style approximate inference in generative
models (Dayan, Hinton, Neal, &amp; Zemel, 1995; Hinton et al., 1995) and cooperation between
model-free and model-based reinforcement learning systems.</p><p>Approximate inference in structured models –  “learning to do inference,”</p><p>Popular algorithms for approximate inference in probabilistic machine learning have been proposed
as psychological models (see Griffiths, Vul, &amp; Sanborn, 2012, for a review). Most prominently, it
has been proposed that humans can approximate Bayesian inference using Monte Carlo methods,
which stochastically sample the space of possible hypotheses and evaluate these samples according
to their consistency with the data and prior knowledge (Bonawitz, Denison, Griffiths, &amp; Gopnik,
2014; Gershman, Vul, &amp; Tenenbaum, 2012; T. D. Ullman, Goodman, &amp; Tenenbaum, 2012; Vul et
al., 2014). Monte Carlo sampling has been invoked to explain behavioral phenomena ranging from
children’s response variability (Bonawitz et al., 2014) to garden-path effects in sentence processing
(Levy, Reali, &amp; Griffiths, 2009) and perceptual multistability (Gershman et al., 2012; MorenoBote,
Knill, &amp; Pouget, 2011). Moreover, we are beginning to understand how such methods could
be implemented in neural circuits (Buesing, Bill, Nessler, &amp; Maass, 2011; Huang &amp; Rao, 2014;
Pecevski, Buesing, &amp; Maass, 2011).9</p><p>How might efficient mappings from questions to a plausible subset of answers be learned? Recent
work in AI spanning both deep learning and graphical models has attempted to tackle this challenge
by “amortizing” probabilistic inference computations into an efficient feed-forward mapping
(Eslami, Tarlow, Kohli, &amp; Winn, 2014; Heess, Tarlow, &amp; Winn, 2013; A. Mnih &amp; Gregor,
2014; Stuhlm¨uller, Taylor, &amp; Goodman, 2013). We can also think of this as “learning to do
inference,” which is independent from the ideas of learning as model building discussed in the
previous section. These feed-forward mappings can be learned in various ways, for example, using
paired generative/recognition networks (Dayan et al., 1995; Hinton et al., 1995) and variational
optimization (Gregor et al., 2015; A. Mnih &amp; Gregor, 2014; Rezende, Mohamed, &amp; Wierstra,
2014) or nearest-neighbor density estimation (Kulkarni, Kohli, Tenenbaum, &amp; Mansinghka, 2015;
Stuhlm¨uller et al., 2013). One implication of amortization is that solutions to different problems
will become correlated due to the sharing of amortized computations; some evidence for inferential
correlations in humans was reported by Gershman and Goodman (2014). This trend is an avenue
of potential integration of deep learning models with probabilistic models and probabilistic programming:
training neural networks to help perform probabilistic inference in a generative model
or a probabilistic program (Eslami et al., 2016; Kulkarni, Whitney, Kohli, &amp; Tenenbaum, 2015;
Yildirim, Kulkarni, Freiwald, &amp; Te, 2015). Another avenue for potential integration is through
differentiable programming (Dalrmple, 2016) – by ensuring that the program-like hypotheses are
differentiable and thus</p><p>Model-based and model-free reinforcement learning</p><p>The DQN introduced by V. Mnih et al. (2015) used a simple form of model-free reinforcement
learning in a deep neural network that allows for fast selection of actions. There is indeed substantial
evidence that the brain uses similar model-free learning algorithms in simple associative
learning or discrimination learning tasks (see Niv, 2009, for a review). In particular, the phasic
firing of midbrain dopaminergic neurons is qualitatively (Schultz, Dayan, &amp; Montague, 1997) and
quantitatively (Bayer &amp; Glimcher, 2005) consistent with the reward prediction error that drives
updating of model-free value estimates.</p><p>Model-free learning is not, however, the whole story. Considerable evidence suggests that the
brain also has a model-based learning system, responsible for building a “cognitive map” of the
environment and using it to plan action sequences for more complex tasks (Daw, Niv, &amp; Dayan,
2005; Dolan &amp; Dayan, 2013). Model-based planning is an essential ingredient of human intelligence,
enabling flexible adaptation to new tasks and goals; it is where all of the rich model-building
abilities discussed in the previous sections earn their value as guides to action.</p><p>One boundary condition on this flexibility is the fact that the skills become “habitized” with
routine application, possibly reflecting a shift from model-based to model-free control. This shift
may arise from a rational arbitration between learning systems to balance the trade-off between
flexibility and speed (Daw et al., 2005; Keramati, Dezfouli, &amp; Piray, 2011).</p><p>Similarly to how probabilistic computations can be amortized for efficiency (see previous section),
plans can be amortized into cached values by allowing the model-based system to simulate training
data for the model-free system (Sutton, 1990). This process might occur offline (e.g., in dreaming
or quiet wakefulness), suggesting a form of consolidation in reinforcement learning (Gershman,
Markman, &amp; Otto, 2014). Consistent with the idea of cooperation between learning systems, a
recent experiment demonstrated that model-based behavior becomes automatic over the course of
training (Economides, Kurth-Nelson, L¨ubbert, Guitart-Masip, &amp; Dolan, 2015). Thus, a marriage
of flexibility and efficiency might be achievable if we use the human reinforcement learning systems
as guidance.</p><p>Intrinsic motivation also plays an important role</p><hr><p>Although deep learning researchers do explore many such architectural
variations, and have been devising increasingly clever and powerful ones recently, it is the researchers
who are driving and directing this process. Exploration and creative innovation in the space of
network architectures have not yet been made algorithmic. Perhaps they could, using genetic
programming methods (Koza, 1992) or other structure-search algorithms (Yamins et al., 2014).
We think this would be a fascinating and promising direction to explore, but we may have to
acquire more patience than machine learning researchers typically express with their algorithms:
the dynamics of structure-search may look much more like the slow random hill-climbing of evolution
than the smooth, methodical progress of stochastic gradient-descent.</p><p>This is now being explored with AutoML</p><p>An alternative strategy is to build in appropriate infant-like knowledge representations and core ingredients as the starting point
for our learning-based AI systems, or to build learning systems with strong inductive biases that
guide them in this direction.</p><p>In the long run, we
are optimistic that neuroscience will eventually place more constraints on theories of intelligence.
For now, we believe cognitive plausibility offers a surer foundation.</p><p>All these ingredients are probably essential for language.</p><p>There has been recent interest in integrating psychological ingredients with deep neural networks,
especially selective attention (Bahdanau et al., 2015; V. Mnih, Heess, Graves, &amp; Kavukcuoglu,
2014; K. Xu et al., 2015), augmented working memory (Graves et al., 2014, 2016; Grefenstette
et al., 2015; Sukhbaatar et al., 2015; Weston et al., 2015), and experience replay (McClelland,
McNaughton, &amp; O’Reilly, 1995; V. Mnih et al., 2015). These ingredients are lower-level than
the key cognitive ingredients discussed in this paper, yet they suggest a promising trend of using
insights from cognitive psychology to improve deep learning, one that may be even furthered by
incorporating higher-level cognitive ingredients.</p><p>developments are also part of a broader trend towards
“differentiable programming,” the incorporation of classic data structures such a random access
memory, stacks, and queues, into gradient-based learning systems (Dalrmple, 2016). For example,
the Neural Turing Machine (NTM; Graves et al., 2014) and its successor the Differentiable Neural
Computer (DNC; Graves et al., 2016) are neural networks augmented with a random access
external memory with read and write operations that maintains end-to-end differentiability. </p><p>Neural programmers-interpreters
</p></div>


</div>


</a></p>