<p>&lt;p&gt;See &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#MMathPhys%20oral%20presentation&quot;&gt;MMathPhys oral presentation&lt;/a&gt;&lt;/p&gt;&lt;p&gt;~*Different definition of finite-state complexity here: &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://web.mit.edu/cocosci/Papers/complex.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;http://web.mit.edu/cocosci/Papers/complex.pdf&lt;/a&gt; (though still not the one we need below)&lt;/p&gt;&lt;p&gt;Using &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://arxiv.org/pdf/1008.1667.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;finite-state complexity&lt;/a&gt; we can define the complexity of a string produced by a finite transducer. It is effectively the length of the smallest program that describes both a transducer (according to some encoding) and the string itself. This definition is not universal as for Turing machines, because of the non-universality of finite transducers.&lt;/p&gt;&lt;p&gt;This is not what I want, because they don't fix the transducer, while a given GPM would fix the transducer. Assuming we can use the same idea as above, if the length of the input is much larger than the length of the transducer, we are effectively inputing random fixed-length strings to a Turing machine (that we know halt hm..), and by Levins coding theorem (applied here to not asymptotic case..) we expect that &amp;quot;strings with many long descriptions to have a short description too&amp;quot;. Furthermore, if we assume that the map is many to one, then each strings would have many long descriptions, so each will have a short description. But if there are many such strings, not all of them can have short descriptions. Thus, the only consistent situation is for a few strings with simple descriptions having many long descriptions too, and a many strings with few long descriptions.&lt;/p&gt;&lt;p&gt;This assumed that the finite transducer is simple (defined by the condition above that the input string to the finite state transducer (FST) be much larger than the FST description). If it isn't, the bias argument above still holds I think, but because the transducer is complex, it's outputs will all be complex, with a complexity dominated by the transducer's.&lt;/p&gt;&lt;p&gt;It seems like the Levin's coding theorem holds for strings of all inputs, which means it works for argument above! However, I don't understand it fully, in particular it's derivation, so I'm not too confident on this. See &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://books.google.co.uk/books?id=LKEmB_GQ53QC&amp;amp;pg=PA283&amp;amp;lpg=PA283&amp;amp;dq=levin+coding+theorem+finite+state&amp;amp;source=bl&amp;amp;ots=4MYNGplLKI&amp;amp;sig=2oKf7zcSUuX9cfq1nGSSdYbKDug&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ved=0ahUKEwjKgoKwh7LLAhUBQBoKHVlWBFoQ6AEISzAF#v=onepage&amp;amp;q=4.3.4%20the%20coding%20theorem&amp;amp;f=false&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;this book&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;mark&gt;SEE EMAIL CONVERSATION FOR FOLLOW UP ON THIS. Reasoning above doesn't hold&lt;/mark&gt;. Kamal's answer:&lt;/p&gt;&lt;blockquote class=&quot;tc-quote&quot;&gt;&lt;p&gt;I read the three papers - thanks for those.&lt;/p&gt;&lt;p&gt;Shallit and Wang (2001) was not super interesting, though obviously relevant in the sense that focus on computable complexities.&lt;/p&gt;&lt;p&gt;Calude (2011) is more interesting. The most interesting result being that a kind of Invariance Theorem holds for finite state transducers (which are weak/weakest type of computation, UTMs being the most powerful because they can compute any algorithm). The Invariance theorm in AIT comes from the fact that any UTM can simulate any other UTM, while their Inv Thm for finite state machines does not invoke this property. Assuming prefix free descriptions of the transducers, this implies a kind of coding theorem for finite state transducers. This is nice because finite state transducers do not have the mystical air that UTMs do (uncomputable). I think it is worth citing this Calude article as a comment, but maybe not making too much of a deal about it.&lt;/p&gt;&lt;p&gt;I also looked Guillermo’s link – just a comment on some reasonsing in there (I know it is just notes):&lt;/p&gt;&lt;p&gt;Furthermore, if we assume that the map is many to one, then each strings would have many long descriptions, so each will have a short [shorter] description. But if there are many such strings, not all of them can have short descriptions[true, but they can all have shorter descriptions]. Thus, the only consistent situation is for a few strings with simple descriptions having many long descriptions too, and a many strings with few long descriptions[hence bias in the map].&lt;/p&gt;&lt;p&gt;The reasoning here is a little rushed – if the map is many to one, then all outputs strings have shorter descriptions. But this does not explain why some outputs have short descriptions and some long (which leads to bias). The central thing to explain in bias is why some outputs will have shorter descriptions than others. The statement &amp;quot;strings with many long descriptions to have a short description too&amp;quot;&lt;/p&gt;&lt;p&gt;does not say anything about how long these short descriptions are, whereas the argument presented assumes that these are short enough to be a problem, in the sense of &amp;quot;not all of them can have short descriptions&amp;quot;.&lt;/p&gt;&lt;p&gt;As a trivial but illustrative example, consider the many-to-one map from binary string of length 10 to binary strings of length 5. We can easily construct a uniform distribution for this system. According to the argument above, this system should show bias….but it does not (even though the map is simple).&lt;/p&gt;&lt;/blockquote&gt;</p>