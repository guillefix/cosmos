<p>&lt;p&gt;A type of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Classification&quot;&gt;Classification&lt;/a&gt; method.&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=FYgsztDxSvE&amp;amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;amp;index=7&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Logistic regression&lt;/a&gt; &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=FYgsztDxSvE&amp;amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;amp;index=7#t=14m42s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Definition&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.gaussianprocess.org/gpml/chapters/RW3.pdf#page=5&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;See &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=HZ4cvaztQEs&amp;amp;index=3&amp;amp;list=PLA89DCFA6ADACE599#t=50m&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;also here&lt;/a&gt; for intro&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://www.youtube.com/watch?v=HZ4cvaztQEs&amp;amp;index=3&amp;amp;list=PLA89DCFA6ADACE599#t=57m17s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Probabilistic interpretation&lt;/a&gt;&lt;/u&gt;. Logitistic regression can also be interpretedas a &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Maximum%20likelihood&quot;&gt;Maximum likelihood&lt;/a&gt; estimate assuming the Logistic funciton (sigmoid) gives the probability of belonging to a given class. We can then use &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Gradient%20descent&quot;&gt;Gradient descent&lt;/a&gt;. More theoretical understanding can be acquired from the theory of &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Generalized%20linear%20model&quot;&gt;Generalized linear model&lt;/a&gt;s&lt;/p&gt;&lt;p&gt;A simpler version: &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Perceptron&quot;&gt;Perceptron&lt;/a&gt;&lt;/p&gt;&lt;p&gt;See &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.cs.ox.ac.uk/people/varun.kanade/teaching/ML-MT2016/slides/slides08.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://mathgotchas.blogspot.co.uk/2011/10/why-is-error-function-minimized-in.html&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Why is it convex&lt;/a&gt;&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;Training&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;Maximum likelihood. Log likelihood turns out to be cross-entropy.&lt;/p&gt;&lt;p&gt;To see if problem is convex, we test if the &lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Hessian&quot;&gt;Hessian&lt;/a&gt; is positive semi-definite, and it is. See &lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;http://www.cs.ox.ac.uk/people/varun.kanade/teaching/ML-MT2016/slides/slides08.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt; for form of Hessian&lt;/p&gt;&lt;h2 class=&quot;&quot;&gt;&lt;u&gt;&lt;a class=&quot;tc-tiddlylink tc-tiddlylink-resolves&quot; href=&quot;#Kernel%20method&quot;&gt;Kernel&lt;/a&gt; logistic regression&lt;/u&gt;&lt;/h2&gt;&lt;p&gt;&lt;a class=&quot;tc-tiddlylink-external&quot; href=&quot;https://youtu.be/Vm5QE54y6mw?t=26m57s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Video&lt;/a&gt;&lt;/p&gt;</p>