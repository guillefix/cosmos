<p>We test whether distributionspandqare different on the basis of samples drawn from each ofthem, by finding a well behaved (e.g., smooth) function which is large on the points drawn fromp,and small (as negative as possible) on the points fromq.  We use as our test statistic the differencebetween the mean function values on the two samples; when this is large, the samples are likelyfrom different distributions. We call this test statistic the <strong>Maximum Mean Discrepancy (MMD)</strong>.</p><p>paper: <a class="tc-tiddlylink-external" href="http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf" rel="noopener noreferrer" target="_blank">A kernel two-sample test</a></p><p>Clearly the quality of the MMD as a statistic depends on the classFof smooth functions thatdefine it. On one hand,Fmust be “rich enough” so that the population MMD vanishes if and onlyifp=q. On the other hand, for the test to be consistent in power,Fneeds to be “restrictive” enoughfor the empirical estimate of the MMD to converge quickly to its expectation as the sample sizeincreases. We will use the unit balls in characteristic <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Reproducing%20kernel%20Hilbert%20space">Reproducing kernel Hilbert space</a>s (Fukumizuet al., 2008; Sriperumbudur et al., 2010b) as our function classes, since these will be shown to satisfyboth  of  the  foregoing  properties. </p><p>Looks very similar to the definition of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Rademacher%20complexity">Rademacher complexity</a> (see Understanding machine learning)</p><p>I think probably the way they calculate the sup, is by optimizing, and this is tractable because the the <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Representer%20theorem">Representer theorem</a></p>