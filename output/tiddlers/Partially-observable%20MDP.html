<p>A partially-observabe <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Markov%20decision%20process">Markov decision process</a> is a Markov decision process where the state is only partially observable by the actor, so that the policy can only depend on a function of the state, which looses some of the state's <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Information">Information</a></p><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Kalman%20filter">Kalman filter</a>s and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#LQG%20control">LQG control</a></u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=46m50" rel="noopener noreferrer" target="_blank">video</a></p><p>A type of reinforcement learning, where we don't observe the state explicitly!</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=53m40s" rel="noopener noreferrer" target="_blank">Want to estimate actual state, given the noisy and incomplete measurements of the state</a>. Can use the method of marginalization, as used in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Factor%20analysis%20model">Factor analysis model</a>s. However, it is very computationally expensive. Instead we use a <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=55m52s" rel="noopener noreferrer" target="_blank">Kalman filter</a> model, which turns out to be a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Hidden%20Markov%20model">Hidden Markov model</a> with continuous states.</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=57m35s" rel="noopener noreferrer" target="_blank">Outline of Kalman filter</a></p><ul><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=1h22s" rel="noopener noreferrer" target="_blank">Predict step</a></li><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=1h03m30s" rel="noopener noreferrer" target="_blank">Update step</a></li></ul><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=1h7m40s" rel="noopener noreferrer" target="_blank">Intuition</a>. I think this can be seen through the lens of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Sufficient%20statistic">Sufficient statistic</a>s</p><p>Kalman filter + LQR = <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#LQG%20control">LQG control</a> &lt;- <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;index=19&amp;list=PLA89DCFA6ADACE599#t=1h8m55s" rel="noopener noreferrer" target="_blank">video</a> &lt;â€“ <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;list=PLA89DCFA6ADACE599&amp;index#t=1h9m50s" rel="noopener noreferrer" target="_blank">how to solve</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=UFH5ibWnA7g&amp;list=PLA89DCFA6ADACE599&amp;index#t=1h14m32s" rel="noopener noreferrer" target="_blank">Separation principle</a> of LQG control</p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=0m50s" rel="noopener noreferrer" target="_blank">recap</a></p><h3 class=""><u>Other POMDP</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=3m55s" rel="noopener noreferrer" target="_blank">In general finding optimal policies of POMDPs is NP hard</a></p>