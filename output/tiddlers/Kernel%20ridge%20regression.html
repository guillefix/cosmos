<p>The version of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Ridge%20regression">Ridge regression</a> (regularized <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Least-squares">Least-squares</a> for a linear model), which uses <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Feature%20map">Feature map</a>s / <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Kernel">Kernel</a>s. The regularization is typically <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=bBRX3OqNC9c&amp;index=6&amp;list=PLyGKBDfnk-iCXhuP9W-BQ9q2RkEIA5I5f#t=1h1m" rel="noopener noreferrer" target="_blank">the norm of the function in the</a> <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Reproducing%20kernel%20Hilbert%20space">Reproducing kernel Hilbert space</a>, but it can be other things I think</p><p>If treated <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Bayesian%20inference">probabilistically</a>, and the weights have a Gaussian prior, it is equivalent to a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Gaussian%20process">Gaussian process</a>, as the values of the function at a collection of points are distributed with a joint <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Gaussian">Gaussian</a> (and so is the posterior, and the predictive distribution, when using a Gaussian model for the error (difference between observed y, and the value of the function f(x)) ).</p><p>See <a class="tc-tiddlylink-external" href="http://www.gaussianprocess.org/gpml/chapters/RW2.pdf" rel="noopener noreferrer" target="_blank">here</a></p><p>By <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Mercer's%20theorem">Mercer's theorem</a>, any symmetric PSD <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Kernel">Kernel</a> can be expressed as a (possibly infinite) sum of product of <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Eigenfunction">Eigenfunction</a>s. One can then see that the vector space spanned by the eigenfunctions is the same as that spanned by the functions <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mo>⋅</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">k(x,\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span></span>, where <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span></span> is the kernel (although the latter is an overcomplete basis). Furthermore, it turns out that randomly sampling weights for the kernel regression case (where the weights are the coefficients in the linear combination of (a linear transformation of) the vector of eigenfunctions –  the linear transformation depending on the distribution over weights, see <a class="tc-tiddlylink-external" href="http://www.gaussianprocess.org/gpml/chapters/RW2.pdf#page=6" rel="noopener noreferrer" target="_blank">here</a>), with a Gaussian distribution, gives the same distribution over functions, as a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Gaussian%20process">GP</a> with kernel <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span></span>.. See <a class="tc-tiddlylink-external" href="http://www.gaussianprocess.org/gpml/chapters/RW2.pdf" rel="noopener noreferrer" target="_blank">here</a>. This is why kernel ridge regression (which just finds the MAP estimate), when treated in fully Bayesian manner is the same as a Gaussian process.. Note that, as mentioned above, the feature / basis functions in the kernel regression (which just takes linear combinations of these functions), are not the same as the eigenfunctions of the kernel, but rather are linear combinations of them, which depend on the distribution over weights. The reason is that a kernel determines the distribution over functions for a GP. And it also determines the egienfunctions of the kernel. If we take the eigenfunctions of the kerne as basis functions for the kernel regressor, then the kernel regressor has an extra degree of freedom that affects the distribution over functions: the distribution over weights. To make them equivalent, therefore, we &quot;cancel&quot; this degree of freedom in this way: no matter what distribution over weights we choose, we can transform the feature vector in a way that the resulting distribution over functions is that same as the GP. This is what gives the 1-to-1 equivalence..</p>