<p>also <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Vanishing%20gradients%20problem">Vanishing gradients problem</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=5v41xjKauY0" rel="noopener noreferrer" target="_blank">video</a> – <a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1712.05577" rel="noopener noreferrer" target="_blank">The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs, and solutions</a> (<a class="tc-tiddlylink-external" href="https://openreview.net/forum?id=HkpYwMZRb" rel="noopener noreferrer" target="_blank">openreview</a>)</p><ol><li>For some types of nonlinearities/layers, the gradients w.r.t. to parameters at different layers increase with the number of layers (it increases as we go from output to input). (<a class="tc-tiddlylink-external" href="https://youtu.be/5v41xjKauY0?t=21m51s" rel="noopener noreferrer" target="_blank">See visualization of decision surfaces on parameter space for fixed input</a></li><li>To avoid pseudorandom walk due to the decision surface from lower layers, we have to choose a smaller learning rate, as the we add more layers (see <a class="tc-tiddlylink-external" href="https://youtu.be/5v41xjKauY0?t=28m8s" rel="noopener noreferrer" target="_blank">here</a>)</li><li>For any network, we can do the <a class="tc-tiddlylink-external" href="https://youtu.be/5v41xjKauY0?t=31m59s" rel="noopener noreferrer" target="_blank">residual trick</a> to convert to a residual network. <a class="tc-tiddlylink-external" href="https://youtu.be/5v41xjKauY0?t=34m18s" rel="noopener noreferrer" target="_blank">The size of the residuals is then found to be small when the learning rate is small</a></li><li>By using the <a class="tc-tiddlylink-external" href="https://youtu.be/5v41xjKauY0?t=31m59s" rel="noopener noreferrer" target="_blank">residual trick</a>, and then doing <a class="tc-tiddlylink-external" href="https://youtu.be/5v41xjKauY0?t=37m42s" rel="noopener noreferrer" target="_blank">a Taylor expansion</a> (where we assume residual terms are small), we can reduce the number of layers, while still approximating well, giving a notion of <strong>effective depth</strong> (note: depending on for how many layers we do the Taylor expansion, we get different number of layers in the approximate network). <a class="tc-tiddlylink-external" href="https://youtu.be/5v41xjKauY0?t=38m42s" rel="noopener noreferrer" target="_blank">The effective depth decreases if the residuals are smaller</a>.</li><li><a class="tc-tiddlylink-external" href="https://youtu.be/5v41xjKauY0?t=39m5s" rel="noopener noreferrer" target="_blank">Therefore error is large</a>. Although for the nets with exploding gradients (and thus small residuals), the depth of the approximate nets is small, they have a lot of layers of nonliearities (with fixed parameters, that's why they are not counted for in the effective depth). These <u>needlessly randomize the features</u>, they argue</li></ol><p>See <a class="tc-tiddlylink-external" href="https://youtu.be/5v41xjKauY0?t=48m16s" rel="noopener noreferrer" target="_blank">summary</a></p><p><a class="tc-tiddlylink-external" href="https://gingkoapp.com/app#72f84d34b9dce3b61f00003d" rel="noopener noreferrer" target="_blank">gingkotree</a></p><p><u>Other problems discussed in the video</u></p><p>Apart from exploding gradients, networks can fail by </p><ul><li>pseudolinearity (which means that as the variance of activations decreases, for certain nonlinearities, the nonlinearity effectively looks more and more linear). This causes ineffective layers, and low effective depth (and then potentially high error). For some nonlinearities, the pseudolinearity effect only happens if domain bias also occurs. <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#ReLU">ReLU</a> is one example.</li></ul><h3 class="">–&gt; <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Residual%20neural%20network">Residual neural network</a>s seem to alliviate these problems! (see also <a class="tc-tiddlylink-external" href="https://youtu.be/78vq6kgsTa8?t=17m10s" rel="noopener noreferrer" target="_blank">this video</a>)</h3><h3 class=""><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1606.05340" rel="noopener noreferrer" target="_blank">Exponential expressivity in deep neural networks through transient chaos</a></h3><h3 class=""><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1611.01232" rel="noopener noreferrer" target="_blank">Deep Information Propagation</a></h3><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Information%20bottleneck">Information bottleneck</a></p><p><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Mean%20field%20theory%20of%20neural%20networks">Mean field theory of neural networks</a> – <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Neural%20network%20Gaussian%20process">Neural network Gaussian process</a>es!</p>