<p>A <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Probabilistic%20model">Probabilistic model</a> for <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Language">Language</a>: A <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Probability%20distribution">Probability distribution</a> over strings (sequence of tokens) in a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Language">Language</a></p><h2 class=""><u>Uses of conditional/joint probability models</u></h2><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=DVSvz2eaZns&amp;list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm&amp;index=9" rel="noopener noreferrer" target="_blank">Conditional language models</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Machine%20translation">Machine translation</a></p><p><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Question%20answering">Question answering</a></p><p><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Dialogue">Dialogue</a></p><hr><p>We use the <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Chain%20rule%20for%20joint%20probabilities">Chain rule for joint probabilities</a> (exact) to expand the joint prob dist of tokens.</p><p>Our objective function can be the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Cross%20entropy">Cross entropy</a> (a way of measuring how close two prob dists are) relative to the empirically observed frequency.</p><p><strong>Perplexity</strong>: <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mn>2</mn><mrow><mtext><mi mathvariant="normal">c</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">s</mi><mtext> </mtext><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">y</mi></mtext></mrow></msup></mrow><annotation encoding="application/x-tex">2^{\text{cross entropy}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathrm">2</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="text mord scriptstyle uncramped"><span class="mord mathrm">c</span><span class="mord mathrm">r</span><span class="mord mathrm">o</span><span class="mord mathrm">s</span><span class="mord mathrm">s</span><span class="mord mspace"> </span><span class="mord mathrm">e</span><span class="mord mathrm">n</span><span class="mord mathrm">t</span><span class="mord mathrm">r</span><span class="mord mathrm">o</span><span class="mord mathrm">p</span><span class="mord mathrm" style="margin-right:0.01389em;">y</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span></p><hr><p><u>Data</u></p><p>WikiText dataset.</p><hr><h1 class=""><u>Language models</u></h1><h2 class=""><u>n-gram models</u></h2><p>A nth-order <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Markov%20chain">Markov chain</a>, where each word's prob dist depends on the previous n words. </p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Maximum%20likelihood">Maximum likelihood</a> correspond to empirical counts of the form <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><msub><mi>w</mi><mn>3</mn></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo>)</mo><mo>=</mo><mfrac><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo>(</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>3</mn></msub><mo>)</mo></mrow><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo>(</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">p(w_3|w_1,w_2)=\frac{count(w_1,w_2,w_3)}{count(w_1,w_2)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.01em;"></span><span class="strut bottom" style="height:1.53em;vertical-align:-0.52em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">3</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.34500000000000003em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">u</span><span class="mord mathit">n</span><span class="mord mathit">t</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.485em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">u</span><span class="mord mathit">n</span><span class="mord mathit">t</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">3</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span> for 3-grams.</p><p>Maximum likelihood is a bad objective for language, apparently, need instead to choose a good prior and do <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Maximum%20a%20posteriori">MAP</a></p><p><u>Smoothing techniques</u></p><p>There are many cases which come out with prob 0. To improve on this, we can use bi-grams to resolve these cases. This is the idea of <strong>back-off</strong>, which is a kind of smoothing technique (c.f. <small><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Laplace%20smoothing">Laplace smoothing</a></small>)</p><p>One very simple way is linear interpolation: the probability of the next word is a convex combination of the probabilities assigned by the one-gram, bi-gram, three-gram etc, probs.</p><p><strong>Kneser-Ney</strong></p><p><em>An empirical study of smoothing techniques for language modeling</em></p><p>We want our posterior distribution to agree with the real distribution in the real world. For insance, we want to recover <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Zipf's%20law">Zipf's law</a> (<a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Power%20laws">Power laws</a>, long tails..) These long tails is what make good-old-AI rule systems to fail, as there's too much stuff to account for.</p><p>~ constant time algo.</p><p>Long n-grams: data is too sparse –&gt; can't really capture long-term dependencies.</p><p>– N-grams can't capture correlations and other patterns which it hasn't seen, so it's bad at generalizing</p><h2 class=""><u>Neural n-gram language models</u></h2><p>These will be able to better capture correlations, and <u>generalize</u>.., by better capturing semantics, stored in the hidden layers..</p><p>Use <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Artificial%20neural%20network">Artificial neural network</a> to model the n-gram prob dist.: input being n previous words, output prob dist. of next word.</p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Recurrent%20neural%20network">Recurrent neural network</a>s</u></h2><p>And also <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Augmented%20RNN">Augmented RNN</a>.</p><p>Want to have memory going all the way back..</p><p>The models are harder to paralelize than the neural n-gram models, because there's depenednce between the network acting at dfifferent points in the sequence.</p><p><u>Truncated <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Backpropagation">Backpropagation</a> through time</u></p><p>Add breaks in the backpropagation, maybe between sentences.</p><p>However, we do forward propagate!</p><p><u>Skip-thought vectors</u></p>