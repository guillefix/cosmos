<p>A class of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Reinforcement%20learning">Reinforcement learning</a> algorithms. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=9m" rel="noopener noreferrer" target="_blank">These are also known as direct search algorithms</a>, in contrast with algorithms where our aim is to find the optimal value function </p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=6m25s" rel="noopener noreferrer" target="_blank">intro vid</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=7m35s" rel="noopener noreferrer" target="_blank">General aim</a></p><p><strong>Stochastic policy</strong> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=11m23s" rel="noopener noreferrer" target="_blank">Definition</a></p><h3 class=""><u>Algorithm</u></h3><p>Sometimes called the <strong>reinforce algorithm</strong>, and is a form of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Stochastic%20gradient%20descent">Stochastic gradient descent</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=23m" rel="noopener noreferrer" target="_blank">Goal</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=27m" rel="noopener noreferrer" target="_blank">Algorithm</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=28m45s" rel="noopener noreferrer" target="_blank">explanation</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=33m" rel="noopener noreferrer" target="_blank">Derivation</a>, using the <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Product%20rule">Product rule</a></p><ol><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=36m05s" rel="noopener noreferrer" target="_blank">Differentiation</a></li><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=38m" rel="noopener noreferrer" target="_blank">Factor out joint probability from terms in sum</a> </li><li>Rewrite as expectation –&gt; <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=40m05s" rel="noopener noreferrer" target="_blank">On expectation, reinforce algorithm updates parameters in the direction of the gradient of the expected payout</a>. This shows the algorithm is an <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Stochastic%20gradient%20descent">Stochastic gradient descent</a> algorithm!</li></ol><p>With direct policy search, <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=kUiR0RLmGCo&amp;index=15&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=40m50s" rel="noopener noreferrer" target="_blank">rewards may be combined in other ways other than by summing them</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=kUiR0RLmGCo&amp;index=15&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=42m43s" rel="noopener noreferrer" target="_blank">Derivation by Nando</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=kUiR0RLmGCo&amp;index=15&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=46m50" rel="noopener noreferrer" target="_blank">comment on reward function not being really needed</a> –&gt; <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=kUiR0RLmGCo&amp;index=15&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=49m05s" rel="noopener noreferrer" target="_blank">result</a></p><p>What we use for the gradient descent is do a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Monte%20Carlo">Monte Carlo</a> estimate, which makes it stochastic.</p><h2 class=""><u>Pegasus</u></h2><p>-—&gt;<a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=yCqPMD6coO8&amp;index=20&amp;list=PLA89DCFA6ADACE599#t=48m" rel="noopener noreferrer" target="_blank">vid</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=kUiR0RLmGCo&amp;index=15&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw#t=52m" rel="noopener noreferrer" target="_blank">Nando's vid</a></p><h2 class=""><u>Policy gradient methods</u></h2><h3 class=""><u>Deterministic Policy Gradient Algorithms</u></h3><p><a class="tc-tiddlylink-external" href="http://jmlr.org/proceedings/papers/v32/silver14.pdf" rel="noopener noreferrer" target="_blank">paper</a></p><h3 class=""><u>Natural policy gradient</u></h3><h2 class=""><u>Other variations</u></h2><p>Can approach it as an <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Inference">Inference</a> problem, or in other ways. See <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=dV80NAlEins&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;index=16#t=1m47" rel="noopener noreferrer" target="_blank">comment</a></p><h3 class=""><u>pair-wise policy comparisons</u></h3><h3 class=""><u>probabilistic policy search approaches</u></h3><p><u>based on EM </u></p><p><u>based on probabilistic modeling </u></p><h3 class=""><u> Relative Entropy Policy Search </u></h3><p><a class="tc-tiddlylink-external" href="https://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/2012/AISTATS-2012-Daniel.pdf" rel="noopener noreferrer" target="_blank">Hierarchical Relative Entropy Policy Search</a> – <a class="tc-tiddlylink-external" href="http://jmlr.org/papers/volume17/15-188/15-188.pdf" rel="noopener noreferrer" target="_blank">extended version</a></p>