<p>An approach to <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Reinforcement%20learning">Reinforcement learning</a> (particularly <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Model-free%20reinforcement%20learning">Model-free reinforcement learning</a>) – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=34m10s" rel="noopener noreferrer" target="_blank">video</a></p><p>TD learning is a combination of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Monte%20Carlo">Monte Carlo</a> ideas and <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Dynamic%20programming">Dynamic programming</a> (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment's dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap).</p><p>Here we discuss the basic <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#On-policy%20learning">On-policy learning</a>s. See <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Off-policy%20learning">Off-policy learning</a> for their extensions to off-policy learning.</p><p><a class="tc-tiddlylink-external" href="http://videolectures.net/deeplearning2017_sutton_td_learning/" rel="noopener noreferrer" target="_blank">Sutton -- TD learning</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=EeMCEQa85tw" rel="noopener noreferrer" target="_blank">DeepMind's Richard Sutton - The Long-term of AI &amp; Temporal-Difference Learning</a> <small><a class="tc-tiddlylink-external" href="https://youtu.be/Qgd3OK5DZWI?t=23m30s" rel="noopener noreferrer" target="_blank">Demis Hassabis talks about it here</a></small></p><h1 class=""><u>TD <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Policy%20evaluation">Policy evaluation</a></u></h1><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h13m" rel="noopener noreferrer" target="_blank">intuition for why we update the current value function assuming the value function at the state after one step, instead of updating it the other way</a></p><h2 class=""><u>TD0</u></h2><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=36m10s" rel="noopener noreferrer" target="_blank">intro vid</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=dV80NAlEins&amp;list=PLjK8ddCbDMphIMSXn-w1IjyYpHU3DaUYw&amp;index=16#t=19m30s" rel="noopener noreferrer" target="_blank">vid</a></p><p>A kind of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Gradient%20descent">Gradient descent</a> to converge to solution to V(s) that satisfies <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Bellman%20equation">Bellman equation</a></p><p><a class="tc-tiddlylink-external" href="https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node60.html" rel="noopener noreferrer" target="_blank">https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node60.html</a></p><p>Proven to work (converge to true value function, in the case of table-lookup representation. But in the case of representing value function in some other ways (parametric function approximation), <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=53m10s" rel="noopener noreferrer" target="_blank">there are subtleties.</a></p><h3 class=""><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=59m" rel="noopener noreferrer" target="_blank">Simple example comparing monte carlo vs TD0</a>.</h3><p>If you let TD0 converge on a limited sample (a limited set of episodes from an MDP), it will converge to the <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Maximum%20likelihood">Maximum likelihood</a> estimate <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Markov%20reward%20process">MRP</a> for that data. <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h3m30s" rel="noopener noreferrer" target="_blank">TD makes use of the Markov property</a></p><h3 class="">Optimality of TD(0)</h3><p>See section 6.3 of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Sutton-Barto">Sutton-Barto</a></p><p><u>n-step look-ahead</u></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h12m5s" rel="noopener noreferrer" target="_blank">intro vid</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=PnHCvfgC_ZA&amp;index=4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT#t=1h16m" rel="noopener noreferrer" target="_blank">video</a></p><p>We can take <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span></span></span></span></span> steps of the (unknown) MDP, instead of 1. Monte Carlo <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Model-free%20reinforcement%20learning">Model-free reinforcement learning</a> is when <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">n \rightarrow \infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">n</span><span class="mrel">→</span><span class="mord mathrm">∞</span></span></span></span></span></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#TD(lambda)">TD(lambda)</a></u></h2><h1 class=""><u>TD control</u></h1><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=0g4j2k_Ggc4&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&amp;index=5#t=38m45s" rel="noopener noreferrer" target="_blank">introduction to TD learning for control</a></p><p>TD prediction + policy improvement (<a class="tc-tiddlylink tc-tiddlylink-missing" href="#Generalized%20policy%20iteration">GPI</a>)</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Sarsa">Sarsa</a></p><hr><p>Related with <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Actor-critic%20method">Actor-critic method</a>s</p>