<p><a class="tc-tiddlylink-external" href="https://www.robots.ox.ac.uk/~mebden/reports/GPtutorial.pdf" rel="noopener noreferrer" target="_blank">Good quick intro</a>. See <a class="tc-tiddlylink-external" href="http://www.gaussianprocess.org/gpml/chapters/" rel="noopener noreferrer" target="_blank">Gaussian Processes for Machine Learning</a></p><p>Basically assume a certain model <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mrow><mi mathvariant="bold">y</mi></mrow><mi mathvariant="normal">∣</mi><mrow><mi mathvariant="bold">x</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">p(\mathbf{y}|\mathbf{x})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord textstyle uncramped"><span class="mord mathbf" style="margin-right:0.01597em;">y</span></span><span class="mord mathrm">∣</span><span class="mord textstyle uncramped"><span class="mord mathbf">x</span></span><span class="mclose">)</span></span></span></span></span> where the <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span></span>s correspond to the <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span></span>s in these vectors. With this, then given some <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span></span>s for some <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span></span>s, we can have a marginal distribution for the <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span></span>s corresponding to unobserved <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span></span>s. A <strong>Gaussian process</strong> models <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mrow><mi mathvariant="bold">y</mi></mrow><mi mathvariant="normal">∣</mi><mrow><mi mathvariant="bold">x</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">p(\mathbf{y}|\mathbf{x})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord textstyle uncramped"><span class="mord mathbf" style="margin-right:0.01597em;">y</span></span><span class="mord mathrm">∣</span><span class="mord textstyle uncramped"><span class="mord mathbf">x</span></span><span class="mclose">)</span></span></span></span></span> as a <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Multivariate%20Gaussian%20distribution">Multivariate Gaussian distribution</a> with a covariance matrix that is given by a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Kernel">Kernel</a> function (ensuring consistency via what's called the marginaliation property).  This can be interpreted as a Gaussian prior on the space of functions.</p><blockquote><p>Formally it is defined as a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Stochastic%20process">Stochastic process</a> where any finite set of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Random%20variable">Random variable</a>s is <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Gaussian">Gaussian</a> distributed.</p></blockquote><p>Typically one chooses kernels that prefers smoothness, so that that <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span></span>s of nearby <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">x</span></span></span></span></span>s are more likely to be similar..</p><p>Efficient up to about 100,000 data points</p><p>They are equivalent to Bayesian <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Kernel%20ridge%20regression">Kernel ridge regression</a>! (what they call the &quot;weight-space view&quot; in <a class="tc-tiddlylink-external" href="http://www.gaussianprocess.org/gpml/chapters/RW2.pdf" rel="noopener noreferrer" target="_blank">here</a>)</p><p>See section 4.3 in Murphy's book (Machine learning - a probabilistic perspective) to see the derivation of the fact that the marginal distribution of a subset of variables from a larger set of random variables which have a <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Multivariate%20Gaussian">Gaussian joint distribution</a>. This is why the Gaussian process property (that the values at any set of points have joint Gaussian distribution) corresponds to a Gaussian prior over functions (<a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Gaussian%20random%20field">Gaussian random field</a>; <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Physical%20field">field</a> with quadratic energy functional..; see <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Path%20integral">Path integral</a> ).</p><hr><p><a class="tc-tiddlylink-external" href="https://pdfs.semanticscholar.org/3e38/092b962bcb430fdcebf1407d1299adb1a10b.pdf" rel="noopener noreferrer" target="_blank">Relationships between Gaussian processes, Support Vector machines and Smoothing Splines</a> – <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Support%20vector%20machine">Support vector machine</a> –<a class="tc-tiddlylink tc-tiddlylink-missing" href="#Spline">Spline</a>s</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1711.00165" rel="noopener noreferrer" target="_blank">Deep Neural Networks as Gaussian Processes</a> – Extensions for <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Convolutional%20neural%20network">CNN</a>s</p><p><a class="tc-tiddlylink-external" href="https://arxiv.org/abs/1804.11271" rel="noopener noreferrer" target="_blank">Gaussian Process Behaviour in Wide Deep Neural Networks</a></p><p><a class="tc-tiddlylink-external" href="https://en.wikipedia.org/wiki/Gaussian_process" rel="noopener noreferrer" target="_blank">https://en.wikipedia.org/wiki/Gaussian_process</a></p><hr><h2 class=""><u>Gaussian processes with non-Gaussian likelihood</u></h2><p>Usually the observed labels / <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span></span> are assumed to be either equal to the function modelled by the GP, or have a Gaussian distribution around it (what's called a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Gaussian">Gaussian</a> <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Likelihood%20function">likelihood</a> – note that here the function <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span></span> works like the parameters in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Bayesian%20inference">Bayesian inference</a>).</p><p>If one assumes a non-Gaussian likelihood, then the problem is not <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Analytically%20tractable">Analytically tractable</a> any more..</p><p>The most common case is in <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Gaussian%20process%20classification">Gaussian process classification</a>. See <a class="tc-tiddlylink-external" href="http://www.gaussianprocess.org/gpml/chapters/RW3.pdf" rel="noopener noreferrer" target="_blank">here</a></p>