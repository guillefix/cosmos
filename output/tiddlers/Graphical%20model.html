<p>A <strong>probabilistic graphical model</strong> is a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Model">Model</a> to represent a <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Joint%20probability%20distribution">Joint probability distribution</a> (joint PD) of a set of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Random%20variable">Random variable</a>s, which takes into account <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Causality">causal</a> relations, and dependencies. The models are called graphical, because these dependencies are represented using <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Graph">Graph</a>s, which allow for building the sparsely-parametrized representations of the joint PDs, and for many useful <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Algorithms">Algorithms</a> for inference and learning to be used.</p><p><u>Factors</u> are functions of the random variables, which are used to build the joint PD. One can do conditioning/reduction and marginalization on these factors. The reduction operation is like currying in <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Functional%20programming">Functional programming</a></p><p><a class="tc-tiddlylink-external" href="http://cs.brown.edu/courses/cs242/lectures/" rel="noopener noreferrer" target="_blank">http://cs.brown.edu/courses/cs242/lectures/</a></p><h1 class=""><u>Representation</u></h1><p><a class="tc-tiddlylink-external" href="https://www.coursera.org/learn/probabilistic-graphical-models/home" rel="noopener noreferrer" target="_blank">Coursera course</a> – <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=bPGr5kFQbaw" rel="noopener noreferrer" target="_blank">Knowledge engineering</a></p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Graph">Graph</a>s:</p><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Markov%20network">Undirected graphical model</a></li><li>Directed graphical models, or <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Bayesian%20network">Bayesian network</a>s –&gt; <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Hidden%20Markov%20model">Hidden Markov model</a></li></ul><p>See <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=ZYUnyyVgtyA&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=25#t=7m10s" rel="noopener noreferrer" target="_blank">here</a> for the distinction of directed vs undirected graphical models. The difference, is that a directed graphical model is an undirected one, but where the factors that correspond to the edges, are normalized, because they correspond to <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Conditional%20probability">Conditional probability</a>es</p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Bayesian%20network">Directed graphical models</a></u> (Bayesian nets)</h2><h3 class=""><u><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Template%20model">Template model</a>s</u></h3><p>Ways of representing graphical models that have a lot of internal shared structure (repeated variables and topologies), like events that occur over time, or relation types found over and over in a graph.. See <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=ogs4Oj8KahQ&amp;index=13&amp;list=PL50E6E80E8525B59C" rel="noopener noreferrer" target="_blank">vid</a></p><ul><li>Temporal models for temporal processes.<ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Dynamic%20Bayesian%20network">Dynamic Bayesian network</a>s <ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Hidden%20Markov%20model">Hidden Markov model</a></li></ul></li></ul></li><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Object-relational%20model">Object-relational model</a>s<ul><li>Directed: <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Plate%20model">Plate model</a>s</li><li>Undirected</li></ul></li></ul><p>An importance class are those that show <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=gkRBlXj8h-w&amp;index=24&amp;list=PL50E6E80E8525B59C" rel="noopener noreferrer" target="_blank">Structured CPD</a>s</p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Markov%20network">Undirected graphical models</a></u> (Markov nets)</h2><h3 class=""><u>Independencies</u></h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=obhBzPaESes&amp;list=PL50E6E80E8525B59C&amp;index=32" rel="noopener noreferrer" target="_blank">I-maps and perfect maps</a>.</p><p>An <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#I-map">I-map</a> (independence map) for a probability distribution <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span></span></span></span></span> is any graphical model <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">G</span></span></span></span></span> such that the set of independencies implied by the network (<span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi><mo>(</mo><mi>G</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">I(G)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord mathit">G</span><span class="mclose">)</span></span></span></span></span>) is a subset of the set of independences of <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span></span></span></span></span> (<span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi><mo>(</mo><mi>P</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">I(P)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mclose">)</span></span></span></span></span>) (see <a class="tc-tiddlylink-external" href="http://courses.cms.caltech.edu/cs155/slides/cs155-03-dseparation-marked.pdf" rel="noopener noreferrer" target="_blank">here</a>), i.e. <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi><mo>(</mo><mi>G</mi><mo>)</mo><mo>⊂</mo><mi>I</mi><mo>(</mo><mi>P</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">I(G) \subset I(P)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord mathit">G</span><span class="mclose">)</span><span class="mrel">⊂</span><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mclose">)</span></span></span></span></span></p><p>A perfect (independence) map is one such that <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi><mo>(</mo><mi>G</mi><mo>)</mo><mo>=</mo><mi>I</mi><mo>(</mo><mi>P</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">I(G) = I(P)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord mathit">G</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mclose">)</span></span></span></span></span></p><h2 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Sum-product%20network">Sum-product network</a>s</u></h2><h1 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Inference%20in%20graphical%20models">Inference</a></u></h1><h3 class=""><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Qa04kw1gKHk&amp;list=PL50E6E80E8525B59C&amp;index=36" rel="noopener noreferrer" target="_blank">Conditional Probability Queries</a> </h3><p>–  <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Qa04kw1gKHk&amp;list=PL50E6E80E8525B59C&amp;index=36#t=2m" rel="noopener noreferrer" target="_blank">Exact inference and even approximate inference are</a> <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#NP-hard">NP-hard</a>. <small>This comes about because the sum-product calculation over all possibilities when doing <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Marginalization">Marginalization</a> involves exponentially many terms</small>. However, this is for worst case, and for <a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Qa04kw1gKHk&amp;list=PL50E6E80E8525B59C&amp;index=36#t=4m40s" rel="noopener noreferrer" target="_blank">general/average cases, there are practical inference algorithms!</a> </p><ul><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Qa04kw1gKHk&amp;list=PL50E6E80E8525B59C&amp;index=36#t=6m30s" rel="noopener noreferrer" target="_blank">Basic inference</a></li><li><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Qa04kw1gKHk&amp;list=PL50E6E80E8525B59C&amp;index=36#t=8m50s" rel="noopener noreferrer" target="_blank">Inference with evidence</a>, use <a class="tc-tiddlylink tc-tiddlylink-missing" href="#Baye's%20theorem">Baye's theorem</a></li></ul><h3 class=""><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Maximum%20a%20posteriori">Maximum a posteriori</a> inference</h3><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=bL9Fvz3fx4c&amp;index=37&amp;list=PL50E6E80E8525B59C" rel="noopener noreferrer" target="_blank">video</a>. Hm, what about MAP, not {over all unobserved variables}?, i.e. with some marginalization... In any case this is also <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#NP-hard">NP-hard</a></p><h2 class=""><u>Algorithms</u></h2><h3 class=""><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=Qa04kw1gKHk&amp;list=PL50E6E80E8525B59C&amp;index=36#t=14m" rel="noopener noreferrer" target="_blank">Probability query algorithms</a></h3><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Variable%20elimination">Variable elimination</a>, using <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Dynamic%20programming">Dynamic programming</a></li><li>Message passing<ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Belief%20propagation">Belief propagation</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Variational%20inference">Variational inference</a></li></ul></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Monte%20Carlo">Monte Carlo</a> methods, random sampling..<ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Markov%20chain%20Monte%20Carlo">Markov chain Monte Carlo</a></li><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Importance%20sampling">Importance sampling</a></li></ul></li></ul><h3 class=""><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=bL9Fvz3fx4c&amp;index=37&amp;list=PL50E6E80E8525B59C#t=7m18s" rel="noopener noreferrer" target="_blank">Maximum a posteriori algorithms</a></h3><ul><li><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Variable%20elimination">Variable elimination</a></li><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Message%20passing">Message passing</a><ul><li>Max-product <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Belief%20propagation">Belief propagation</a></li></ul></li><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Integer%20programming">Integer programming</a> methods</li><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Graph-cut">Graph-cut</a> methods, and other <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Graph%20algorithms">Graph algorithms</a>s</li><li><a class="tc-tiddlylink tc-tiddlylink-missing" href="#Combinatorial%20search">Combinatorial search</a></li></ul><p>Other <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Optimization">Optimization</a> algorithms.</p><p><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Viterbi%20algorithm">Viterbi algorithm</a></p><h1 class=""><u><a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Graphical%20model%20learning">Learning</a></u></h1><hr><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=WPSQfOkb1M8&amp;list=PL50E6E80E8525B59C" rel="noopener noreferrer" target="_blank">1.0 - Welcome-Probabilistic Graphical Models - Professor Daphne Koller</a></p><p><a class="tc-tiddlylink-external" href="https://www.youtube.com/channel/UCvPnLF7oUh4p-m575fZcUxg/videos" rel="noopener noreferrer" target="_blank">Jeffrey A. Bilmes</a></p><p><u><a class="tc-tiddlylink-external" href="https://en.wikipedia.org/wiki/Graphical_model" rel="noopener noreferrer" target="_blank">Graphical models</a></u></p><p>They can often be represented as kinds of <a class="tc-tiddlylink tc-tiddlylink-resolves" href="#Artificial%20neural%20network">Artificial neural network</a>s</p><p><a class="tc-tiddlylink-external" href="http://mpawankumar.info/teaching/cdt-optimization/lecture2_2.pdf" rel="noopener noreferrer" target="_blank">Energy minimization</a></p><h3 class=""><a class="tc-tiddlylink-external" href="https://www.youtube.com/watch?v=btr1poCYIzw" rel="noopener noreferrer" target="_blank">Composing graphical models with neural networks</a></h3><p><a class="tc-tiddlylink-external" href="https://www.vicarious.com/2017/10/26/common-sense-cortex-and-captcha/" rel="noopener noreferrer" target="_blank">https://www.vicarious.com/2017/10/26/common-sense-cortex-and-captcha/</a></p>